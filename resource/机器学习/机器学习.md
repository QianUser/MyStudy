# 概述

## 机器学习的定义

**机器学习（Machine Learning，ML）**是计算机编程的科学与艺术，它可以从数据中学习到知识。

更通用的定义为：机器学习是可以使得计算机在没有显式编程的情况下进行学习的研究领域。

更加面向工程的定义如下：（机器学习）是一个计算机程序，它可以根据某个任务*T*与某个性能度量指标*P*从经验*E*中学习，并且*T*的性能（由*P*度量）能够随着*E*而得到改善。

例如，对于垃圾邮件过滤器（spam filter），*T*为辨认每个新邮件是否是垃圾邮件，*E*为训练集（training set），*P*表示定义的性能度量（performance measure），如准确率（accuracy）。

## 为什么使用机器学习

机器学习适用于以下情况：

- 对于某个问题，已有的解决方法需要大量的微调或很长的规则列表。机器学习算法经常可以简化代码且比传统方法表现更好。
- 对于某些问题，使用传统方法无法得到好的解决方法，而最好的机器学习技术可能发现一个解决方法。
- 环境波动时，机器学习系统可以适应新的数据。
- 从复杂问题与大量数据中获得一些见解。

要使用传统方法设计一个垃圾邮件过滤器的方法如下：

1. 识别垃圾邮件的特征，如主题行中某些单词或短语的出现频率过高，或发送人姓名、邮件主体或邮件其他部分出现一些其他的模式。
2. 编写算法去检测这些模式，如果某个邮件出现大量的这些模式，则将其标注为垃圾邮件。
3. 测试该程序，并迭代步骤1、2，直到结果足够好。

![传统方法识别垃圾邮件](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\传统方法识别垃圾邮件.png)

该方法可能需要设置大量复杂的规则，因此难以维护。

相比之下，基于机器学习技术的垃圾邮件过滤器通过检测垃圾邮件示例中模式出现频率异常的单词，自动学习哪些单词和短语有助于预测垃圾邮件。这样的程序更短、更容易维护，并且可能更准确。

![机器学习方法识别垃圾邮件](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\机器学习方法识别垃圾邮件.png)

垃圾邮件会演化。如果某种模式（如包含“4U”）的垃圾邮件都被拦截，则新的垃圾邮件可能将该模式重写为其他模式（如将“4U”写为“For U”）。传统的方法需要不停为此手动更新规则，而机器学习方法则可以自动注意到这一点。

<a name=(概述)(为什么使用机器学习)(1)>![机器学习方法适应变化](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\机器学习方法适应变化.png)</a>

对于某些问题，使用传统方法解决过于复杂或没有已知的算法，机器学习可以很好处理之。对于语音识别任务（speech recognition），使用传统方法难以识别出不同的人在嘈杂的环境中用几十种语言所说的大量的单词。迄今为止最好的方法是编写一个算法，让其从大量示例录音中自动学习语音特征。

机器学习可以帮助人类学习。可以探查机器学习算法，了解其学习到了什么。如果一个垃圾邮件过滤器通过足够的垃圾邮件得到有效训练，则可以了解到出现哪些单词或单词的组合的邮件最有助于预测垃圾邮件，这有助于揭示潜在的相关性或新趋势，从而加深对该问题的理解。应用机器学习技术挖掘大量数据有助于发现数据中难以察觉的模式，这被称为**数据挖掘（data mining）**。

![机器学习可以帮助人类学习](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\机器学习可以帮助人类学习.png)

## 机器学习系统的种类

### 监督/无监督学习

依据机器学习系统在训练过程中监督的量与类型，机器学习系统主要可以分为**监督学习（supervised learning）**、**无监督学习（unsupervised learning）**、**半监督学习（semisupervised learning）**与**强化学习（Reinforcement Learning）**。

#### 监督学习

在监督学习中，输入到算法中的训练集（training set）包含了期望解（desired solution），其被称为**标签（label）**。

典型的监督学习任务包括：

- **分类（classification）**。在分类任务中，标签即实例所属的**类别（class）**。垃圾邮件过滤器就是典型的分类任务，每个邮件的标签为“spam”或“ham”。
- **回归（regression）**，即（给定输入特征（一般有多个）的情况下）预测目标值（target numeric value）（可能有多个）。如给定车的特征（品牌、行驶里程数、年龄等）的情况下预测车的价格。训练时需要提供大量的车的样本（包括以上特征与标签，标签即车的价格）。使用多个特征进行预测的回归被称为**多元回归（multiple regression）**；对于每个实例只预测一个目标值的回归被称为**单变量回归（univariate regression）**；对于每个实例预测多个目标值的回归被称为**多变量回归（multivariate regression）**。

![垃圾邮件分类的带标签训练集](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\垃圾邮件分类的带标签训练集.png)

![回归问题。给定输入特征，预测值。通常输入特征有多个，有时输出值也有多个](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\回归问题.png)

有些分类算法可以用于回归任务中，反之亦然。如Logistic Regression通常用于分类，但也可以输出一个值，表示实例属于某个类别的概率。

##### 注：回归的名称由来

回归这个奇怪的名字是Francis Galton在研究高个子的孩子往往比他们的父母矮这一事实时引入的一个统计学术语。由于孩子们更矮，它将其称为**回归到均值（regression to the mean）**。这个名字后来被应用到他用来分析变量之间的相关性（correlations）的方法上。

#### 无监督学习

在无监督学习中，输入到算法中的训练集不包含标签（unlabeled）。

![用于无监督学习的无标签训练集](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\用于无监督学习的无标签训练集.png)

典型的无监督学习算法包括：

- **聚类（clustering）**。通过聚类算法可以将你的博客的访问者分为若干组，每组包含相似的访问者。算法可以自动发现有40%的访问者喜欢连环漫画并通常在晚上阅读你的博客等。如果使用**层级聚类（hierarchical clustering）**算法，则可以将组进一步细分为若干子组，形成组的层级结构。可以依据分组结果对不同的组投放不同的帖子。
- **异常检测（anomaly detection）**与**新奇检测（novelty detection）**。异常检测也被称为**离群值检测（outlier detection）**，它的任务是检测严重偏离正常值的实例。这些实例被称为**异常（anomalies）**或**离群值（outliers）**，而正常实例被称为**内点（inliers）**。异常检测应用广泛，例如欺诈检测、检测产品缺陷、训练另一个模型前移除数据集中的离群值（即清理数据集，可以显著提升最终模型的性能）。异常检测的训练集包括正常的实例（instance）（占据大多数）与异常的实例，而新奇检测的训练集全部为正常的实例，目标为检测出与训练集中所有实例不同的新实例。
- **可视化（visualization）**：对算法输入大量复杂的无标签数据，算法输出数据的2D或3D表示，可以容易地被绘制出来。可视化算法尽可能多得保持数据结构（例如尽可能防止输入空间中不同的簇在可视化显示中相互重叠），从而便于理解数据的组织方式，并发现潜在的模式。一个相关的任务为**降维（dimensionality reduction）**，它的目标是简化数据，同时不会丢失太多信息。一种方法是将若干相关属性合并为一个属性，例如车的行驶里程与年龄高度相关，所以合并其为一个特征表示车的磨损程度，这被称为特征提取（feature extraction）。
- **关联规则学习（association rule learning）**。关联规则学习的目标是通过挖掘大量数据以发现属性之间的有趣关系。例如对超市的销售日志运行关联规则，可能发现购买烧烤酱与薯片的人也倾向于购买牛排，因此超市可以将这些商品放置在相近的地方。

![聚类](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\聚类.png)



![异常检测](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\异常检测.png)



![突出语义集群的t-SNE可视化，其中相似的概念彼此接近，不相似的概念彼此远离。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\突出语义集群的t-SNE可视化.png)

[^突出语义集群的t-SNE可视化]: Figure reproduced with permission from Richard Socher et al., “Zero-Shot Learning Through Cross-Modal Transfer,” *Proceedings of the 26th International Conference on Neural Information Processing Systems* 1 (2013): 935–943.

#### 半监督学习

在半监督学习中，输入到算法中的训练集同时含有有标签与无标签（unlabeled）数据，通常有标签数据少而无标签数据多。

![含有两个类别的无监督学习（三角形与正方形）。其中无标签示例（圆）有助于将新实例（叉号）分类为三角形类，即使其更接近带标签的正方形](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\含有两个类别的无监督学习.png)

大部分的半监督算法是无监督算法与有监督算法的组合。

#### 强化学习

强化学习系统中包含一个**agent**，其可以观察环境、选择并执行动作并获得**奖励（reward）**或**惩罚（penalty）**。其必须自己找到最佳策略（称为**policy**）以随着时间推移获得最大奖励。一个策略定义了在特定的情形下agent应该选择什么动作。

![强化学习](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\强化学习.png)

例如，许多机器人采用强化学习算法来学习如何行走。DeepMind的AlphaGo程序也是强化学习的典型例子：2017年5月，它在围棋比赛中击败世界冠军柯洁，成为头条新闻（headlines）。它通过分析数以百万计的游戏，然后和自己玩很多游戏来学习自己的获胜策略。注意，在与冠军的比赛中，学习被关闭；AlphaGo只是在应用它学到的策略。

### 批量/在线学习

依据机器学习系统是否能够从传入的数据流中增量式学习，机器学习系统可以分为**批量学习（batch learning）**与**在线学习（online learning）**。

#### 批量学习

批量学习也被称为**离线学习（offline learning）**。批量学习系统无法增量式学习。首先利用所有可用数据训练系统，训练完成后系统投入生产并运行（launch），在此过程中不再学习，而是将系统在训练过程中学到的知识应用于新数据上。批量学习的训练过程通常消耗大量时间与计算资源，因此一般离线进行。

如果要让批量学习系统学习到新数据的知识，必须使用所有数据（包括老数据与新数据），从头重新训练新版本的系统，并停止并替代当前版本的系统。整个训练、评估（evaluate）、运行（launch）过程可以很容易地高度自动化（如图[机器学习方法适应变化](#(概述)(为什么使用机器学习)(1))所示），因此能适应数据的变化。

批量学习不适用于以下情况：

- 如果系统要适应快速变化的数据：使用所有数据训练系统的时间代价大，因此通常要求训练周期比较长。
- 如果数据量大且要求系统能短周期地自动从头训练或数据量过大导致无法使用批量学习：使用所有数据训练系统需要大量的计算资源（并带来大量金钱消耗）。
- 如果要求系统能够自动学习但是资源有限，使用大量数据、大量资源在短周期地花费过多时间训练不是好主意。

#### 在线学习

在线学习系统采用增量式训练：将数据实例成单地或成小组地（称为**小批量（mini-batch）**）依次（sequentially）输入到系统中训练。由于每个训练步都快速便宜，因此系统可以直接学习到达的新数据。

![在线学习：模型训练并投入生产，然后随着新数据的到来，模型不断学习](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\在线学习.png)

在线学习适用于以下情况：

- 当数据以连续流的方式进入系统中，且系统需要快速或自动适应数据变化时。
- 系统资源有限。因为当系统学习到某些数据的知识后，这些数据就可以被丢弃了（除非想要回滚到以前状态并“重放”这些数据），这将节省大量的空间资源。
- 数据量过大，不能一次性加载进一个机器的主存中时可以使用在线学习算法训练系统（这被称为**核心外学习（out-of core learning）**）：系统可以依次加载部分数据到主存中，在这些数据上执行训练步，重复该过程直到所有数据被训练。

![使用在线学习处理大量数据](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用在线学习处理大量数据.png)

在线学习的**学习率（learning rate）**表示系统对变化的数据的适应速度。如果设置一个高的学习率，则系统能很快适应新数据，但容易很快忘记旧数据的信息；如果设置一个低的学习率，则系统惰性（inertia）更大，即学习速度慢，但对新数据中的噪音（noise）或离群值（outliers）更不敏感。

在线学习容易受到坏数据（bad data）的影响，如果坏数据输入到在线学习系统中，系统性能将会逐渐下降。如果这发生在一个在线系统中，客户可能会注意到。为了降低这种风险，系统需要被密切监视，一旦系统性能下降就可以立即关闭学习过程甚至回滚系统。输入的数据也可能需要监视（使用异常检测算法等），一旦发现异常数据则作相应处理。

##### 注：核心外学习

核心外学习通常离线进行（即不在在线系统上进行），因此“在线学习”可能是一个具有误导性的（confusing）名字，**增量学习（incremental learning）**也许是个更好的名字。

### 基于实例/基于模型的学习

机器学习系统**泛化（generalize）**（系统训练后，在新的、以前未见过的样本上做出好的预测）的方式主要有两种：**基于实例的学习（instance-based learning）**与**基于模型的学习（model-based learning）**。

#### 基于实例的学习

基于实例的学习系统通过度量新数据与已学习数据（或其子集）的相似性（measure of similarity）来对新数据作预测。

![基于实例的学习](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\基于实例的学习.png)

对于垃圾邮件过滤器来说，最简单的基于实例的学习方式为：标记所有内容与与某个已知的垃圾邮件内容相同的邮件为垃圾邮件。一个更好的方式是将与已知的垃圾邮件相似度很高的邮件作为垃圾邮件，如：比较新邮件与已知的垃圾邮件的公共词数量，如果其与某个已知的垃圾邮件的公共词数量很多，则将新邮件标记为垃圾邮件。

#### 基于模型的学习

基于模型的学习通过为已知样本集建立模型来预测新样本，流程如下：

1. 学习数据，了解数据的特征。
2. **模型选择（model selection）**。
3. 使用训练集训练模型（即：学习算法搜索能够最小化代价函数的模型参数）。
4. 使用模型对新数据作预测（被称为**推断（inference）**）并希望模型能够很好地泛化。

![基于模型的学习](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\基于模型的学习.png)

模型一般含有若干**模型参数（model parameter）**（可以在训练中被调整）。模型的**效用函数（utility function）**或**适应度函数（fitness function）**度量了模型效果有多好，反之**代价函数（cost function）**或**损失函数（loss function）**度量了模型效果有多差。将训练样本输入模型后，算法找到一组参数使得模型最好地拟合数据的过程即**模型训练（training）**。

需要注意的是，模型在训练过程中的损失函数与测试过程中使用的性能度量可能不同（很常见）。例如分类器常常使用log loss等损失函数训练，但是使用精确率/召回率进行评估。通常有三个原因：

- 模型训练过程中需要将正则化项加到损失函数中（更一般地，需要在训练过程中约束模型），而训练完成后评估模型性能时需要移除正则化项。
- 好的损失函数应该要有便于优化的导数（optimization-friendly derivative）（更一般地，损失函数有性能度量缺少的有用的微分性质），而用于测试的性能度量应该尽可能接近最终目标。
- 某些损失函数更容易计算。

## 机器学习的主要挑战

因为机器学习主要任务是选择一个学习算法并在数据上训练，因此可能出错的两件事是：“坏的算法”（“bad algorithm”）与“坏的数据”（“bad data”）。

### 训练数据数量不足

即使对非常简单的问题使用机器学习算法，通常也需要数千样本，对于复杂的问题，例如图像识别或语音识别，则可能需要数百万的样本（除非可以重用现有模型的部分）。

#### 数据的高度有效性（The Unreasonable Effectiveness of Data）

论文[Scaling to very very large corpora for natural language disambiguation](https://homl.info/6)指出，只要提供足够的数据，不同的机器学习算法（包括很简单的机器学习学习算法）在一个复杂的自然语言消歧（natural language disambiguation）问题上表现得几乎一样好。作者指出：这些结果表明我们重新考虑在算法开发与语料开发上花费时间与金钱的权衡。

![数据的重要性 VS 算法的重要性](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\数据的重要性 VS 算法的重要性.png)

[^数据的重要性 VS 算法的重要性]: Figure reproduced with permission from Michele Banko and Eric Brill, “Scaling to Very Very Large Corpora for Natural Language Disambiguation,” *Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics* (2001): 26–33.

对于复杂问题，数据比算法更重要的思想在论文[The Unreasonable Effectiveness of Data](https://homl.info/7)中得到进一步推广（further popularized by）。但是，由于中小规模数据集（dataset）还很常见，获得额外的训练集并不总是很容易或便宜，因此算法仍旧很重要。

### 训练数据不具有代表性

为了很好地泛化，一个关键点是训练数据必须能够代表想要泛化得新实例。不管使用基于实例的学习还是基于模型的学习，这一点都成立。
> In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to.

![训练样本不具有代表性的例子，任务为根据人均国内生产总值预测国民生活满意度。如果训练数据只包含圆形，则拟合得到点线。如果加上正方形，则拟合得到实线，其显著改变了之前的线性模型，并且表明线性模型不适用于该任务，看起来非常富裕的国家并不比中等富裕的国家更开心，看起来甚至并不开心；相反，一些贫穷的国家看起来比许多富裕国家更开心。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\训练样本不具有代表性的例子.png)

如果样本很少，样本容易变成**采样噪音（sampling noise）**，即由于偶然性导致的不具有代表性的数据；如果采样方法有缺陷，那么即使样本很多也未必具有代表性，这被称为**采样偏差（sampling bias）**。

### 数据质量低下

如果训练数据包含很多错误（error）、离群值或噪音，则系统更难捕获数据的底层模式（underlying pattern），因此不太可能表现得好。一些解决措施如下：

- 如果一些实例明显离群，则丢弃之或尝试手动修复存在的错误。
- 如果一些实例缺失少量特征，则考虑是否忽略全部样本的该属性、忽略这些实例、使用特定策略填充这些缺失值（如使用其他实例在该属性上的平均值）或训练两个模型，其中一个模型使用该特征而另一模型不使用该特征。

### 特征不相关

设计好的用于训练的特征集是机器学习项目成功的关键之一。该过程被称为**特征工程（feature engineering）**，其包含以下步骤：

- **特征选择（feature selection）**：从已有特征中选择最有用的一些特征去训练。
- **特征提取（feature extraction）**：组合已有特征去产生更有用的一个特征，降维算法有助于这一过程。
- 收集新数据以创建新特征。

### 训练数据过拟合

过度泛化（overgeneralizing）在现实生活中经常发生（如出国被某个出租车司机坑骗（rip you off）从而认为该国的所有出租车司机都是小偷），在机器学习中这被称为**过拟合（overfitting）**：模型在训练数据中表现得很好，但是泛化能力不好。

![过拟合训练数据：使用高次多项式拟合国民生活满意度](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用高次多项式过拟合训练数据.png)

复杂的模型，如深度神经网络可以检测数据中的细节（subtle）模式，但是如果训练数据有噪音或过少（从而产生采样噪音），则模型容易检测到噪音本身的模式，这些模式不能很好地泛化。如根据人均国内生产总值预测国民生活满意度时，如果输入的属性包括不相关的（uninformative）属性例如国家名，则模型可能发现以名字包含“w”的国家国民生活满意度均大于7（包括New Zealand、Norway、Sweden与Switzerland），但是显然这一规则未必适用于Rwanda或Zimbabwe等国家。

当模型相比于训练数据的数量或噪音过于复杂时发生过拟合。

> Overfitting happens when the model is too complex relative to the amount and noisiness of the training data.

过拟合的解决方法包括：

- 简化模型：选择一个参数更少的模型、减少训练数据中的属性数量或对模型施加约束。
- 收集更多的训练数据。给模型提供更多的训练数据，直到其验证误差达到训练误差。
- 减少训练数据中的噪音（例如修复数据中的错误或移除离群值）。

约束模型以简化它并减少过拟合的风险被称为**正则化（regularization）**。

例如，根据人均国内生产总值预测国民生活满意度的线性模型含有两个参数：$\theta_0, \theta_1\rightarrow life\ satisfaction=\theta_0+\theta_1\times GDP\_per\_capita$，因此该算法有两个自由度（degrees of freedom）。如果强制$\theta_1=0$，则该算法只有一个自由度，只能上下移动至平均值处；如果限制$\theta_1$只能取较小的值，则模型的复杂度介于一个自由度与两个自由度之间。在完美拟合训练数据与保持模型简单之间要作平衡以保证其良好泛化。

![通过正则化减少过拟合的风险。点线代表在用圆形表示的国家上训练的原始模型；虚线代表使用所有国家训练得到的模型；实线代表在与第一个模型相同的数据上训练得到的模型，但使用了一个正则化约束，正则化使得该模型斜率更小，其拟合效果不如点线代表的模型，但（对训练过程中没看到的实例（即正方形））泛化得更好。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\通过正则化减少过拟合的风险.png)

学习过程中的正则化量由**超参数（hyperparameter）**控制，超参数为学习算法的参数（而不是模型的参数）。因此，超参数不受学习算法本身的影响，必须在训练前设置并在训练过程中保持不变。如果正则化超参数设置过大，则过拟合风险小但拟合难度大。超参数调整（tuning）是构建机器学习系统的重要部分。

### 训练数据欠拟合

**欠拟合（underfitting）**是过拟合的反面。当模型过于简单以至于无法学习到数据的底层结构时就会欠拟合。例如根据人均国内生产总值预测国民生活满意度的线性模型就容易欠拟合，因为现实情况要更复杂。

欠拟合的解决方法包括：

- 使用更强大、含有更多参数的模型。
- 通过特征工程为学习算法提供更好的特征。
- 减少模型约束，例如减少正则化超参数。

# <a name=(机器学习工程清单)>机器学习工程清单</a>

机器学习工程清单（checklist）可指导完成机器学习项目，包括以下步骤（当然，具体问题具体分析，根据需求调整清单）：

1. Frame the Problem and Look at the Big Picture

   1. Define the objective in business terms.
   2. How will your solution be used?
   3. What are the current solutions/workarounds (if any)?
   4. How should you frame this problem (supervised/unsupervised, online/offline, etc.)?
   5. How should performance be measured?
   6. Is the performance measure aligned with the business objective?
   7. What would be the minimum performance needed to reach the business objective?
   8. What are comparable problems? Can you reuse experience or tools?
   9. Is human expertise available?
   10. How would you solve the problem manually?
   11. List the assumptions you (or others) have made so far.
   12. Verify assumptions if possible.

2. Get the Data

   Note: automate as much as possible so you can easily get fresh data.

   1. List the data you need and how much you need.
   2. Find and document where you can get that data.
   3. Check how much space it will take.
   4. Check legal obligations, and get authorization if necessary.
   5. Get access authorizations.
   6. Create a workspace (with enough storage space).
   7. Get the data.
   8. Convert the data to a format you can easily manipulate (without changing the data itself).
   9. Ensure sensitive information is deleted or protected (e.g., anonymized).
   10. Check the size and type of data (time series, sample, geographical, etc.).
   11. Sample a test set, put it aside, and never look at it (no data snooping!).

3. Explore the Data

   Note: try to get insights from a field expert for these steps.

   1. Create a copy of the data for exploration (sampling it down to a manageable size if necessary).

   2. Create a Jupyter notebook to keep a record of your data exploration.
   3. Study each attribute and its characteristics:
      -  Name
      -  Type (categorical, int/float, bounded/unbounded, text, structured, etc.)
      -  % of missing values
      -  Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)
      -  Usefulness for the task
      -  Type of distribution (Gaussian, uniform, logarithmic, etc.)
   4. For supervised learning tasks, identify the target attribute(s).
   5. Visualize the data.
   6. Study the correlations between attributes.
   7. Study how you would solve the problem manually.
   8. Identify the promising transformations you may want to apply.
   9. Identify extra data that would be useful (go back to “Get the Data”).
   10. Document what you have learned.

4. Prepare the Data

   Notes:

   - Work on copies of the data (keep the original dataset intact).
   - Write functions for all data transformations you apply, for five reasons:
     - So you can easily prepare the data the next time you get a fresh dataset
     - So you can apply these transformations in future projects
     - To clean and prepare the test set
     - To clean and prepare new data instances once your solution is live
     - To make it easy to treat your preparation choices as hyperparameters

   1. Data cleaning:
      - Fix or remove outliers (optional).
      - Fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).

   2. Feature selection (optional):
      - Drop the attributes that provide no useful information for the task.
   3. Feature engineering, where appropriate:
      - Discretize continuous features.
      - Decompose features (e.g., categorical, date/time, etc.).
      - Add promising transformations of features (e.g., log(*x*), sqrt(*x*), *x*2 , etc.).
      - Aggregate features into promising new features.
   4. Feature scaling:
      - Standardize or normalize features.

5. Shortlist Promising Models

   Notes:

   - If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests).
   - Once again, try to automate these steps as much as possible.

   1. Train many quick-and-dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forest, neural net, etc.) using standard parameters.
   2. Measure and compare their performance.
      - For each model, use *N*-fold cross-validation and compute the mean and standard deviation of the performance measure on the *N* folds.
   3. Analyze the most significant variables for each algorithm.
   4. Analyze the types of errors the models make.
      - What data would a human have used to avoid these errors?
   5. Perform a quick round of feature selection and engineering.
   6. Perform one or two more quick iterations of the five previous steps.
   7. Shortlist the top three to five most promising models, preferring models that make different types of errors.

6. Fine-Tune the System

   Notes:

   - You will want to use as much data as possible for this step, especially as you move toward the end of fine-tuning.
   - As always, automate what you can.

   1. Fine-tune the hyperparameters using cross-validation:
      - Treat your data transformation choices as hyperparameters, especially when you are not sure about them (e.g., if you’re not sure whether to replace missing values with zeros or with the median value, or to just drop the rows).
      - Unless there are very few hyperparameter values to explore, prefer random search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, [as described by Jasper Snoek et al.](https://homl.info/134)).
   2. Try Ensemble methods. Combining your best models will often produce better performance than running them individually.
   3. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.

   Warning: Don’t tweak your model after measuring the generalization error: you would just start overfitting the test set.

7. Present Your Solution

   1. Document what you have done.
   2. Create a nice presentation.
      - Make sure you highlight the big picture first.
   3. Explain why your solution achieves the business objective.
   4. Don’t forget to present interesting points you noticed along the way.
      - Describe what worked and what did not.
      - List your assumptions and your system’s limitations.
   5. Ensure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one predictor of housing prices”).

8. Launch!

   1. Get your solution ready for production (plug into production data inputs, write unit tests, etc.).

   2. Write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops.
      - Beware of slow degradation: models tend to “rot” as data evolves.
      - Measuring performance may require a human pipeline (e.g., via a crowdsourcing service).
      - Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending random  values, or another team’s output becoming stale). This is particularly important for  online learning systems.

   3. Retrain your models on a regular basis on fresh data (automate as much as possible).

下面使用加州（California）人口的普查数据建立该州的房价模型。数据包括每个街区（block group）（本例称作区域（district））的人口、收入中位数、房价中位数等。通过学习这些数据，给定其他指标（metric），模型可以预测每个区域的房价中位数。

### 通观全局（Look at the Big Picture）

#### 形成问题框架（Frame the Problem）

假设这是一个实际的项目，第一步是要明确该任务的商业目标。建立模型未必是最终目的，公司要考虑怎样使用该模型以及如何从模型中受益。明确目标是形成问题、选择算法、选择模型的性能度量与确定调整该模型所花费的精力的决定因素。

假设该项目的目标是将模型的输出连同其他信号（signal）输入到另一个机器学习系统。下游系统决定了是否值得在某个地区投资。

![用于房地产投资的机器学习管道](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\用于房地产投资的机器学习管道.png)

接下来要明确是否有已有的解决方案，如果有，这些解决方案是怎样的。已有的解决方案可以作为性能参考并提供解决该问题的见解。假设当前的解决方案是：一个团队收集一个区域的最新信息，如果无法获得该区域的房价中位数，则其使用复杂的规则来估计（estimate）房价中位数（即由一些专家手动估计）。这通常代价很大并且很耗时，效果也不理想（预测很不准确）。所以这是公司提出训练模型预测区域房价中位数的原因，由于有充足的带标签数据，因此这具有可行性。

然后要形成问题框架：这是监督学习、无监督学习还是强化学习？这是分类问题、回归问题还是什么其他问题？是批量学习还是在线学习？等等。这是：监督学习任务；多元回归（multiple regression）、单变量回归（univariate regression）问题；批量学习问题（没有连续数据流到系统中，没有快速适应变化数据的特别需求，数据很少内存足以容纳。如果数据很多，则可以将批量学习任务拆分到多个服务器或使用在线学习技术）。

#### 选择性能指标（Select a Performance Measure）

本任务选择**均方根误差（root mean square error，RMSE）**作为性能指标，即：
$$
RMSE(\pmb{X}, h)=\sqrt{\frac{1}{m}\sum_{i=1}^m(h(\mathbf{x}^{(i)})-y^{(i)})^2}
$$
如果数据包含许多离群值，则可以考虑使用**平均绝对误差（mean absolute error，MAE）**（也被称为**平均绝对偏差（mean absolute deviation）**）：
$$
MAE(\pmb{X}, h)=\frac{1}{m}\sum_{i=1}^m{h(\mathbf{x}^{(i)}})-y^{(i)}|
$$

#### 检查假设（Check the Assumptions）

列出并确认到目前为止制定的假设，这有助于发现一些严重的问题。例如，如果下游任务需要将房价转化为多个类别（category），如“cheap”、“medium”、“expensive”，然后使用这些类别而不是房价本身。此时未必需要预测准确的房价，系统只需要输出类别即可，这就是一个分类任务了。如果在一个回归模型上工作了几个月才发现这个问题就很麻烦了。假设已确认这是一个如先前所示的回归问题。

### 获取数据（Get the Data）

#### 创建工作区（Create the Workspace）

首先需要配置运行环境、创建工作区。

#### 下载数据（Download the Data）

在典型环境中，数据被存储在关系数据库中或其他常见的数据仓库（data store）中，并分布在多个表/文档/文件中。要想获取数据，需要被授权并熟悉数据模式。

本项目比较简单，只需要下载一个单独的压缩文件，名为housing.tgz，解压后得到housing.csv文件。可以手动下载并解压文件，但是为了尽量自动化该过程，使用如下程序处理数据：

```python
# 下载地址不可用，这里仅作演示。
# 数据集可以从https://github.com/ageron/handson-ml2上获得。
# 这里使用jupyter notebook。在真实项目中，这些代码通常被保存在Python文件（而不是Jupyter notebook）中。

import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
```

```python
fetch_housing_data()
```

当数据定期更改时，自动化获取数据过程很有用。此时可以编写一个脚本，使用函数取获取最新数据或设置定时任务定期自动执行该操作。当要在多个机器上安装数据集时，自动化获取数据过程也很有用。

使用pandas加载数据：

```python
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
```

#### 快速浏览数据结构（Take a Quick Look at the Data Structure）

查看数据的前5行：

```python
housing = load_housing_data()
housing.head()
```

每行代表一个区域，一共10个属性：longitude、latitude、housing_median_age,、total_rooms、total_bedrooms、population、households、median_income、median_house_value、ocean_proximity。

查看数据的描述信息：

```python
housing.info()
```

一共20640个实例，在机器学习中是个比较小的数据集。其中total_bedrooms属性有20433个非空值，即有207个空值。除ocean_proximity外，所有属性均是数值类型；ocean_proximity为object类型，因为是从CSV文件加载进来的，其必定是文本属性，其值重复意味着它可能为一个类别属性（categorical attribute）。

通过`value_counts`方法查看有哪些类别以及每个类别包含多少区域：

```python
housing["ocean_proximity"].value_counts()
```

查看数值属性的摘要信息（包括实例总数、均值、方差、最小值、25百分位（percentiles）、50百分位、75百分位、最大值，其中$n$百分位表示有$n\%$的值低于该值）：

```python
housing.describe()
```

绘制直方图，以查看数值属性数值分布情况，这里一次性绘制所有数值属性的分布情况，当然也可以一次一个。

```python
import matplotlib.pyplot as plt

housing.hist(bins=50, figsize=(20,15))
plt.show()
```

从直方图中需要关注以下现象：

- 一些属性存在切顶（capped）现象，即将小于某个值m的值全部记为m，或大于某个值n的值全部记为n（可能有其一，也可能都有）。如果直方图的两端的长方形异常地高（不符合统计分布规律），则很可能出现切顶现象。这包括median_income、housing_median_age与median_house_value属性。从专业团队那里了解到，median_income的切顶值分别为0.49999与15.0001（之所以不是0.5与15，可能是为了与真实的0.5与15的值做区分），单位是万美元。median_house_value的切顶现象可能导致严重问题，因为机器学习算法可能误认为所有的房价中位数都不会超过切顶值。此时需要与客户团队仔细商讨，看他们是否接受这样算法处理。如果不接受，有两种选择：
  - 为切顶的标签值重新收集合适的标签。
  - 从数据集中移除这些区域。
- 属性值的分布尺度（scale）差别很大。
- 一些直方图的尾部很长（tail-heavy），即以中间长方形为标志，右边长方形的数量远远多于左边长方形的数量（并且长方形普遍很低）。

> 1. First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.
> 2. The housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:
>    1. Collect proper labels for the districts whose labels were capped.
>    2. Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).
> 3. These attributes have very different scales. We will discuss this later in this chapter, when we explore feature scaling.
> 4. Finally, many histograms are *tail-heavy*: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.

#### 创建测试集（Create a Test Set）

创建测试集，然后将其放到一边，在模型训练完成并测试前，不要查看其内容，以防止**数据窥探（data snooping）**偏差（bias）（查看测试集可能发现数据集中的某些模式，导致选择了特定的机器学习模型。当使用测试集估计模型的泛化误差时，得到的结果过于乐观，运行的系统表现得没有期望的好）。

> This is true, but your brain is an amazing pattern detection system, which means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called *data snooping* bias.

创建测试集的过程很简单：随机选择一些实例，典型情况下占20%（如果数据集很大，比例可以减小），然后将其放到一边：

```python
import numpy as np

np.random.seed(42)  # 确保每次运行时输出一致。
```

```python
def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
```

```python
train_set, test_set = split_train_test(housing, 0.2)
len(train_set)
```

```python
len(test_set)
```

该程序的问题在于每次运行时会产生不同的测试集，多次运行后整个数据集将可见（成为训练集的一部分被人或算法看到）。针对该问题至少有三种解决方案：

- 第一次运行该程序后保存测试集，在随后的运行过程中直接加载保存的测试集。如果数据集有可能被更新，该方法不可行。
- 在调用```np.random.permutation(len(data))```前为随机数生成器设置特定的种子（如```np.random.seed(42)```）。如果数据集有可能被更新，该方法不可行。
- 基于实例的标识符确定其是否属于测试集（假设实例的标识符唯一且不变）。例如：选择标识符的哈希值小于等于最大哈希值的20%的所有实例作为测试集。此时即使数据集被更新，测试集仍包含大约20%的实例且不会包含任何先前的训练集中的实例。一种实现如下：

```python
from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32  #? 可以省略“& 0xffffffff”吗？

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```

```python
# 以行的index作为标识符。
# 若数据集被更新，新数据必须被附加到行末尾，且不允许任何行被删除。
housing_with_id = housing.reset_index() # 增加'index'列，作为标识符。
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index")
```

```python
# 以行的最稳定特征建立唯一的标识符。
# 允许以任意方式添加或删除实例。
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")
```

```python
# 通过sklearn创建测试集。

from sklearn.model_selection import train_test_split

# random_state用于设置随机数生成器种子。
# train_test_split方法可以同时接受行数相同的多个数据集，在相同的（行）索引上划分数据集。这很有用，例如当标签位于另一个DataFrame中时（if you have a separate DataFrame for labels）。
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
```

如果数据集足够大，以上方法没有问题，否则容易产生采样偏差。此时考虑使用**分层抽样（stratified sampling）**，在该方法中，数据被分成多个同质组（homogeneous subgroup）称为**层（stratum）**，然后在每个层中按相同比例独立采样。

假设median_income属性是预测房价中位数非常重要的属性，因此要保证在测试集中median_income属性（各层）具有代表性，则可根据median_income分层抽样。因为median_income是连续数值属性，因此首先要将median_income分成多个层（即创建收入类别属性），且保证每个层有足够的实例，这意味着层不能过多，且每个层的值的范围必须足够大。

创建收入类别：

```python
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])
```

```python
housing["income_cat"].hist()
```

进行分层抽样：

```python
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)  # n_splits是将数据分成train / test对的组数。
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
```

查看分组情况：

```python
strat_test_set["income_cat"].value_counts() / len(strat_test_set)
```

```python
housing["income_cat"].value_counts() / len(housing)
```

比较随机方法与分层抽样方法划分数据集的（income_cat属性上的）类别比例：

```python
def income_cat_proportions(data):
    return data["income_cat"].value_counts() / len(data)

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

compare_props = pd.DataFrame({
    "Overall": income_cat_proportions(housing),
    "Stratified": income_cat_proportions(strat_test_set),
    "Random": income_cat_proportions(test_set),
}).sort_index()
compare_props["Rand. %error"] = 100 * compare_props["Random"] / compare_props["Overall"] - 100
compare_props["Strat. %error"] = 100 * compare_props["Stratified"] / compare_props["Overall"] - 100
```

```python
compare_props  # 使用分层抽样得到的类别比例与整个数据集中的类别比例几乎一致。
```

最后移除income_cat属性：

```python
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

### 了解数据（Discover and Visualize the Data to Gain Insights）

该过程只针对训练集，必须保证测试集不被窥探。

该过程不是一蹴而就的，而是迭代的：一旦构建好了模型原型并运行后，可以分析原型的输出以获得更多信息，然后再回到这一探索步骤中。

如果训练集过大，则应该对其采样得到探索集（explorartion set），以便使得该操作更容易且更快。该数据集比较小，可以直接在整个数据集上探索。

首先创建训练集的一个副本，以保证原始的训练集不被破坏：

```python
housing = strat_train_set.copy()
```

#### 可视化数据（Visualizing Geographical Data）

使用散点图可视化数据：

```python
housing.plot(kind="scatter", x="longitude", y="latitude")
```

该散点图的形状类似于California，但是很难从图中观察到特定模式。因此设置`alpha=0.1`以更容易可视化数据点高密度分布区域：

```python
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
```

可以清楚地看到高密度区域，即海湾地区以及洛杉矶和圣地亚哥周围，加上中央山谷的一长串高密度区域，特别是萨克拉门托和弗雷斯诺周围。

在可视化过程中要注意调整可视化参数以使得模式突出：

```python
# 可视化median_house_value的分布情况。
# 点的半径代表区域人口（由参数s指定），颜色代表价格（由参数c指定）。使用预定义的名为jet的颜色图（由选项cmap指定）表示，颜色从蓝（低值）到红（高值）变化。
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
             s=housing["population"]/100, label="population", figsize=(10,7),
             c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
             sharex=False)
plt.legend()
```

可以看出，房价与地理位置（例如是否靠近海洋）、人口密度紧密联系。

#### 寻找相关性（Looking for Correlations）

计算每对属性间的标准相关系数（standard correlation coefficient，又被称为Pearson's r）。

```python
corr_matrix = housing.corr()
```

查看每个属性与房价中位数间的相关性（注意相关系数用于揭示线性相关性，它可能完全忽略了非线性关系。以下用线性相关性代表属性间的相关性）：

```python
corr_matrix["median_house_value"].sort_values(ascending=False)
```

![各个数据集的标准相关系数（来源：Wikipedia；public domain image）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\各个数据集的标准相关系数.png)

使用pandas的`scatter_matrix`方法绘制所有数值属性对的分布，这里只关注与房价中位数最有可能相关的属性对的分布：

> Since there are now 11 numerical attributes, you would get 112 = 121 plots, which would not fit on a page—so let’s just focus on a few promising attributes that seem most correlated with the median housing value (Figure 2-15):

```python
from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms",
              "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))  # 由于主对角线上的图绘制出来必然是一条直线，因此在该位置pandas为每个属性绘制一个直方图（也可以设置其他选项）。
```

```python
# median_income是预测房价中位数最理想的属性，因此绘制它们之间的散点图。
housing.plot(kind="scatter", x="median_income", y="median_house_value",
             alpha=0.1)
plt.axis([0, 16, 0, 550000])
```

从图中可以看出：

- 两者之间的相关性非常强。
- 可以很清楚地观察到切顶现象（水平线\$50000处），除此以外，在\$45000与\$35000处也存在不明显地直线，在\$28000以及其他更低处也可能存在直线。为了避免算法学习到这样的不正常的知识，可以考虑移除对应的区域。

> This plot reveals a few things. First, the correlation is indeed very strong; you can clearly see the upward trend, and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at \$500,000. But this plot reveals other less obvious straight lines: a horizontal line around \$450,000, another around \$350,000, perhaps one around \$280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.

#### 属性组合（Experimenting with Attribute Combinations）

很多时候，我们可能更关心“平均”属性而不是“总数”属性：

```python
housing["rooms_per_household"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"] / housing["total_rooms"]
housing["population_per_household"] = housing["population"] / housing["households"]
```

再次查看每个属性与房价中位数间的相关性：

```python
corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
```

可以看到，相比于total_rooms或total_bedrooms属性，bedrooms_per_room属性与房价中位数的相关性更大。显然，$卧室/房间$比值较低的房子往往更贵。同理，相比于total_rooms，room_per_household也更重要。显然，房子越大越贵。

### 数据准备（Prepare the Data for Machine Learning Algorithms）

该过程为机器学习算法准备好数据。应尽可能编写函数来实现该目的，而不是手动处理数据，原因包括：

- 便于在任何数据集上重用这些转换。
- 可以逐渐构建一个转换函数库，可以在未来的项目中重用之。
- 可以将这些函数应用在工作的系统中，在新数据输入算法前对其转换。
- 可以很容易地尝试各种转换并找到最佳的转换组合。

> It’s time to prepare the data for your Machine Learning algorithms. Instead of doing this manually, you should write functions for this purpose, for several good reasons:
>
> - This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).
> - You will gradually build a library of transformation functions that you can reuse in future projects.
> - You can use these functions in your live system to transform the new data before feeding it to your algorithms.
首先分离出预测因子（predictor）与标签，因为该过程不应对预测因子与标签作相同的转换：

```python
housing = strat_train_set.drop("median_house_value", axis=1)  # 只是创建了副本，不会影响原始数据strat_train_set，下同。
housing_labels = strat_train_set["median_house_value"].copy()
```

#### 数据清洗（Data Cleaning）

大部分机器学习算法无法处理缺失的特征。本例中total_bedrooms属性存在缺失值，需要通过以下三种方式之一处理这种情况：

- 删除对应的区域。
- 删除整个属性。
- 设置缺失值为某值（如0、平均数、中位数等）。

使用DataFrame的三种方法处理缺失值的方式如下：

```python
# 删除对应的区域。
housing.dropna(subset=['total_bedrooms'])

# 删除整个属性。
housing.drop('total_bedrooms', axis=1)

# 设置缺失值为中位数。
median = housing['total_bedrooms'].median  # 注意保存该中位数，因为当评估系统时需要用它来替代测试集中的缺失值，一旦系统投入使用也需要它替代新数据中的缺失值。
housing['total_bedrooms'].fillna(median, inplace=True)
```

使用Scikit-Learn的`SimpleImputer`处理缺失值：

```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
```

```python
housing_num = housing.drop("ocean_proximity", axis=1)  # 去除非数值属性，保存副本。
# alternatively: housing_num = housing.select_dtypes(include=[np.number])
```

```python
imputer.fit(housing_num)  # 拟合实例。
```

```python
imputer.statistics_  # imputer将每个属性的中位数保存在该实例变量中。
```

```python
housing_num.median().values  # 获取每个属性的中位数，作为对比。
```

```python
X = imputer.transform(housing_num)  # 处理缺失值，返回NumPy数组，不会影响到原来的数据。注意这里将imputer作用到所有数值属性上，因为不能确保系统运行后新数据中（其他属性上）没有缺失值。
```

```python
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                          index=housing.index)  # 将结果转换为pandas DataFrame。
```

#### 处理文本与类别属性（Handling Text and Categorical Attributes）

本例存在唯一的文本属性：ocean_proximity。

查看前10个实例：

```python
housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10)
```

文本的可能取值是有限的，每种文本代表一个类别，因此这个属性是个类别属性（categorical attribute）。

大部分的机器学习算法更擅长处理数字，因此需要将类别从文本转换为数字：

```python
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
housing_cat_encoded[:10]
```

```python
ordinal_encoder.categories_  # 类别存放在该实例变量中，这是一个列表，为每个类别属性保存一个1D类别数组（It is a list containing a 1D array of categories for each categorical attribute），本例中只有一个类别属性。
```

机器学习算法通常假定相近的值之间的相似度比较远的值之间的相似度更高。本例不符合该假设（如类别0表示“<1H OCEAN”，类别4表示“NEAR OCEAN”，两者相似但值不相近）。为解决这个问题，通常使用**独热编码（one-hot encoding）**（为每个类别创建一个二元属性，如果实例对应的原始属性值为该类别，则二元属性取1，否则取0）。这些属性有时被称为**虚拟（dummy）**属性。

下面使用独热编码处理该文本属性：

```python
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()  # OneHotEncoder可将类别值转换为独热向量。
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot  # 输出Scipy稀疏矩阵（sparse matrix）（存储非零值的索引），而非Numpy数组（You can use it mostly like a normal 2D array）。如果类别很多，这有助于节约存储空间。
```

```python
housing_cat_1hot.toarray()  # 转换为NumPy数组。
```

```python
cat_encoder.categories_  # 类别存放在该实例变量中。
```

如果类别属性包含大量可能的类别，则独热编码会产生大量的输入特征，这可能会降低训练速度并降低降低性能（performance）。此时可以使用与类别相关的有用的数值特征替代类别输入，如将ocean_proximity特征替换为到海洋的距离，或将国家代码替换为国家的人口与人均国内生产总值（GDP per capita）；也可以将每个类别替换为**嵌入向量（embedding）**，即低维的、可学习的向量，这是**表示学习（representation learning）**的一个例子。

> Alternatively, you could replace each category with a learnable, low-dimensional vector called an *embedding*. Each category’s representation would be learned during training.

#### 自定义转换器（Custom Transformers）

通过实现```fit```（返回```self```）、```transform```与```fit_transform```方法自定义转换器（transformer）（用于自定义数据清洗操作或特定属性组合，并可无缝地与Scikit-Learn的功能（如管道）配合使用）。

> Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need to do is create a class and implement three methods: ```fit()``` (returning ```self```), ```transform()```, and ```fit_transform()```.

如果将```TransformerMixin```作为基类，则类可自动实现```fit_transform```。如果将```BaseEstimator```作为基类，并避免在构造器中使用```*args```与```**kargs```参数，则可额外获得方法```get_params```与```set_params```，这对于自动超参数调整很有用。

```python
from sklearn.base import BaseEstimator, TransformerMixin

rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room=True):
        self.add_bedrooms_per_room = add_bedrooms_per_room  # 增加一个超参数，用于确定应不应该增加bedrooms_per_room属性（More generally, you can add a hyperparameter to gate any data preparation step that you are not 100% sure about）。
    def fit(self, X, y=None):
        return self  # 什么也不做。
    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)
```

以上的数据准备步骤的自动化程度越高，则可以自动化的（超参数）组合数目就会越多，因此就越有可能发现一个好的组合并节省大量时间。

#### 特征缩放（Feature Scaling）

大部分情况下，当输入的数值属性有非常不同的尺度（scale）时，机器学习算法表现得不好。该例中，total_rooms的范围为6到39320，而median_income的范围仅为0到15。因此需要进行特征缩放。

目标值一般不需要特征缩放，本例中，median_house_value不需要特征缩放。

通常有两种特征缩放方法，使得所有属性有相同的尺度：**min-max scaling**（很多人称之为**normalization**）与**standardization**。

min-max scaling将某个属性的所有值移位并缩放到0\~1的范围内（一般而言）。方法是将某个值减去最小值，然后除以最大值与最小值之差。Scikit-Learn为此提供转换器```MinMaxScaler```，它有超参数```feature_range```以调整缩放范围。min-max scaling容易受离群值的影响。

standardization将某个值减去平均值，然后除以标准差，以使得最终分布的方差为1。standardization不会将值缩放到某个范围内，对于某个算法这会产生问题（如神经网络经常期望输入值的范围为0到1），但是其受离群值的影响比较小。Scikit-Learn为此提供转换器```StandardScaler```。

和其他转换一样，缩放器（scaler）只能作用于训练集，而不能作用于整个数据集（包括测试集）。只有这样才能对训练集、测试集与新数据作转换。

#### 转换管道（Transformation Pipelines）

使用Scikit-Learn的`Pipeline`构建转换管道：

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler()),
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)
```

```Pipeline```的构造器接受名字/估计器（name/estimator）对列表，用于定义一个顺序步骤。除了最后一个估计器，其他估计器必须是转换器（即必须有```fit_transform```方法）。名字可以是任意合法字符串（必须唯一且不能包含```__```），可以用于超参数调整。

当调用管道的```fit```方法后，其顺序调用各个转换器的```fit_transform```方法，每次调用的输出作为下一次调用的输入，直到最后一个估计器，并调用它的```fit()```方法。由于最后一个估计器```StandardScaler```为转换器，因此可以对```num_pipeline```调用```transform```与```fit_transform```方法。

> The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a ```StandardScaler```, which is a transformer, so the pipeline has a ```transform()``` method that applies all the transforms to the data in sequence (and of course also a ```fit_transform()``` method, which is the one we used).

使用`ColumnTransformer`处理所有列，针对不同的列应用适当的转换器：

```python
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)  # 获取属性
cat_attribs = ["ocean_proximity"]
```

```python
full_pipeline = ColumnTransformer([
        ("num", num_pipeline, num_attribs),
        ("cat", OneHotEncoder(), cat_attribs),
    ])  # 可选的remainder属性（'drop'、'passthrough'等）指定对剩余列的处理方式，默认丢弃

housing_prepared = full_pipeline.fit_transform(housing)
```

```ColumnTransformer```的构造器接受元组列表，每个元组包括一个名字（和`Pipeline`一样，可以是任意合法字符串，只要不包含`__`）、一个转换器（transformer）与该转换器作用的列的名字列表或索引列表。```ColumnTransformer```对象会将转换器作用到对应列上，并将输入沿第2维度进行拼接，因此转换器必须返回相同的行数。

```OneHotEncoder```返回一个稀疏矩阵，而```num_pipeline```返回一个稠密矩阵（dense matrix）。当稀疏矩阵与稠密矩阵被混合使用，```ColumnTransformer```会估计最终矩阵的稠密度（density），如果最终矩阵的稠密度小于某个阈值（默认为```sparse_threshold = 0.3```），则返回稀疏矩阵。此例最终返回稠密矩阵。

### 选择并训练模型（Select and Train a Model）

#### 在训练集上训练并评估（Training and Evaluating on the Training Set）

采用线性回归模型在训练集上训练：

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)
```

查看（训练集中的）部分实例结果：

```python
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)  # 不要调用fit_transform。
print("Predictions:", lin_reg.predict(some_data_prepared))
```

```python
print("Labels:", list(some_labels))
```

可以直观上看到预测结果不太准确。

查看RMSE：

```python
from sklearn.metrics import mean_squared_error

housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse
```

RMSE比较大，因此模型欠拟合，原因可能是特征选择不合适，或模型不够强大。在前面提到的解决模型欠拟合问题的三种方案中，方案3不适用（因为模型没有约束），在尝试方案2前，可以先尝试方案1。

尝试另一种模型：

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(housing_prepared, housing_labels)
```

查看结果：

```python
housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse
```

可以看到RMSE为0，这意味着模型很可能过拟合。

#### 交叉验证评估（Better Evaluation Using Cross-Validation）

可以使用```train_test_split```方法将训练集分为一个小的训练集与一个验证集，然后在小的训练集上训练模型并在验证集上评估模型。更好的方法是采用交叉验证：

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                         scoring="neg_mean_squared_error", cv=10)  # 使用10折交叉验证，结果是包含10次评估得分的数组。
tree_rmse_scores = np.sqrt(-scores)  # Scikit-Learn的交叉验证功能期望效用函数而不是损失函数。
```

查看结果：

```python
def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())

display_scores(tree_rmse_scores)
```

模型的表现看起来甚至比线性模型还要差。

交叉验证不仅返回模型性能的估计结果，还返回估计的精度（即它的标准差），如果仅使用一个验证集是无法做到的。只是交叉验证要对模型训练若干次，这并不总是能够做到的。

接下来查看线性模型的交叉验证结果：

```python
lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)
```

可以认定决策树模型严重过拟合，以至于表现得比线性模型还要差。

最后再使用随机森林回归模型：

```python
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg.fit(housing_prepared, housing_labels)
```

```python
housing_predictions = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_predictions)
forest_rmse = np.sqrt(forest_mse)
forest_rmse
```

```python
forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
                                scoring="neg_mean_squared_error", cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)
display_scores(forest_rmse_scores)
```

可以看到模型效果要好得多，但是仍然过拟合。

解决过拟合问题的方法包括简化模型、约束模型（对其正则化）或获取更多训练数据等。同时还需要尝试不同类别的机器学习算法中的许多其他模型，甚至是神经网络。这样做的目的是选出2~5个效果比较好的模型。

> Before you dive much deeper into Random Forests, however, you should try out many other models from various categories of Machine Learning algorithms (e.g., several Support Vector Machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models.

在该过程中需要将实验的模型保存下来（包括超参数、训练参数、交叉验证得分，甚至是实际的预测）以便轻松回滚到任何想要的模型。这样便于比较不同模型的得分、模型犯错的类型。

可以使用Python的```pickle```模块（module）或```joblib```库（library）保存Scikit-learn模型，后者能更好地序列化（serialize）大Numpy数组：

`joblib`的使用演示：

```python
import joblib

joblib.dump(forest_reg, model_path)
# and later...
my_model_loaded = joblib.load(model_path)
```

### 微调模型（Fine-Tune Your Model）

假设已经找到了几个比较好的模型，下面对其微调。

#### 网格搜索（Grid Search）

一种微调方法是手动微调超参数，直到找到好的超参数值的组合。这非常乏味且且需要花费大量时间探索多种组合。更好的方法是自动化微调：

```python
from sklearn.model_selection import GridSearchCV

# 提供的超参数及其取值。
param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]

forest_reg = RandomForestRegressor(random_state=42)

# 只需要提供超参数及其取值，GridSearch使用交叉验证去评估超参数值的所有可能组合。
# GridSearchCV初始化时默认refit=True，即当其使用交叉验证找到最佳估计器后，会重新在整个训练集上训练。这通常是个好主意，因为训练数据更多可能改善模型性能。
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)  # 依次针对每个字典中的所有取值进行组合（先尝试n_estimators与max_features的12个组合，再尝试bootstrap、n_estimators与max_features的6个组合），总组合数为每个字典的组合数的和。本例中使用5折交叉验证，因此一共训练90轮。
grid_search.fit(housing_prepared, housing_labels)
```

查看最佳参数组合：

```python
grid_search.best_params_
```

结果：

`{'max_features': 8, 'n_estimators': 30}`。

由于`max_features`与`n_estimators`均取到最大值，因此应该尝试更大的值，此时模型得分可能更高。这里省略。

查看最佳估计器：

```python
grid_search.best_estimator_
```

查看评估得分：

```python
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)
```

要注意数据准备阶段的一些步骤也可以被看作超参数。网格搜索可以用来确定是否应该添加某个特征（如在```CombinedAttributesAdder```转换器中使用```add_bedrooms_per_room```），以及找到处理离群值、缺失特征、特征选择的最好方法等。

#### 随机搜索（Randomized Search）

如果超参数搜索空间很大，则可使用```RandomizedSearchCV```。它的使用方法与```GridSearchCV```差不多，但是它不会尝试所有可能组合，而是评估给定数量的随机组合：每个迭代时，为每个超参数选择一个随机值。该方法有两个主要好处：

- 如果随机搜索迭代次数足够多，那么每个超参数就能取到足够多的取值。如果使用网格搜索，则每个超参数只能有很少的取值。
- 只需设置迭代次数，就可以更好地控制要分配给超参数搜索的计算预算。

> This approach has two main benefits:
>
> - If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).
#### 集成方法（Ensemble Methods）

另一种微调系统的方法是组合表现最好的若干模型。这些集成的模型经常比最好的单一模型表现更好，尤其当单一模型犯不同类型错误时。

#### 分析最佳模型（Analyze the Best Models and Their Errors）

通过探查模型常常可以获得新的见解。

> You will often gain good insights on the problem by inspecting the best models.

`RandomForestRegressor`可以指示每个属性对于做出准确预测的相对重要性：

```python
feature_importances = grid_search.best_estimator_.feature_importances_
feature_importances
```

在属性名称前显示重要性得分，根据这些信息，可以丢弃一些不太有用的特征：

```python
extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
cat_encoder = full_pipeline.named_transformers_["cat"]
cat_one_hot_attribs = list(cat_encoder.categories_[0])
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
sorted(zip(feature_importances, attributes), reverse=True)
```

还要注意系统犯了哪些特定（specific）错误、为什么犯这些错误以及如何修复这些问题（增加特征、去除不太有用的（uninformative ）特征或清除离群值等）。

#### 在测试集上评估系统（Evaluate Your System on the Test Set）

```python
final_model = grid_search.best_estimator_

X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)
final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
```

```python
final_rmse
```

计算泛化误差的95%置信区间（confidence interval），以便了解估计的精度。

```python
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                         loc=squared_errors.mean(),
                         scale=stats.sem(squared_errors)))
```

如果进行了大量的超参数调整，模型的性能通常会比使用交叉验证测量得到的性能略差，此时不要再调整超参数使得数字在测试集上更好看。因为这些调整不太可能很好地泛化。

#### 运行、监视并维护系统（Launch, Monitor, and Maintain Your System）

# 数据

## 要素

在机器学习中，**属性（attribute）**表示数据类型（如车的行驶里程：mileage），一个数据实例包含若干属性；**特征（feature）**根据上下文有不同的含义，但一般表示属性加上它的值（如汽车行驶里程“mileage = 15000”）。很多情况下两者被交替使用。

## 数据划分

### 训练与测试

如果想要验证模型的泛化能力的唯一方法是将模型应用于新用例中。一种方法是将模型投入生产并监测其性能表现，但是如果模型表现很差，则用户会抱怨，因此这不是最好的解决办法。

更好的方法是将数据集分成两部分：**训练集（training set）**与**测试集（test set）**，在训练集上训练模型，在测试集上测试模型。模型在新用例上的误差称为**泛化误差（generalization error）**或**样本外误差（out-of-sample error）**，在测试集上评估模型可以得到泛化误差的估计值。

通常将80%的数据作为训练集，将20%的数据作为测试集。当然，具体取决于实际情况。如果数据集很大，也许只需要将很小比例的数据作为测试集已足够很好地估计泛化误差了。

如果模型的训练误差很小但是泛化误差很大，则意味着模型过拟合。

### 超参数调整与模型选择

机器学习中常常遇到在多个模型中选择最好的模型或针对特定的模型选择一组最好的超参数的问题。如果采用测试集去评估模型的泛化误差，并选择使得泛化误差最小的一个模型或一组超参数，则该模型的实际泛化误差会被低估。

通常的解决方式为**留出验证（holdout validation）**，即从训练集中留出部分数据作为**验证集（validation set）**（有时被称为**开发集（development set，dev set）**）。在剩下的训练集（不包含验证集的训练集）中训练多个模型，然后选取在验证集上表现最好的模型。然后在整个训练集（包括验证集）上训练该模型得到最终的模型，并在测试集上评估模型得到其泛化误差的估计值。

如果模型在训练集上表现很好但是泛化性能很差，则模型过拟合；如果模型在训练集与验证集上表现都很差，则模型欠拟合。因此留出验证是查看模型复杂度的一种方法。

##### 交叉验证

如果验证集过小则模型评估不够精确，导致错误地选择了次优（suboptimal）的模型，反之如果验证集过大，则剩下的训练集远小于整个训练集，导致留出验证过程中的训练集过小而最终训练过程中的训练集过大，产生不匹配（类似于选择最好的短跑运动员参加马拉松）。

> Why is this bad? Well, since the final model will be trained on the full training set, it is not ideal to compare candidate models trained on a much smaller training set. It would be like selecting the fastest sprinter to participate in a marathon.

**交叉验证（cross-validation）**使用多个小的验证集进行验证，每个模型都在对应的剩下的训练集上训练，并在该验证集上进行模型评估。最后针对每个模型或每组超参数，取所有评估结果的均值作为最终结果。这样得到的结果更准确，但训练时间随着验证集数量成倍增长。

#### 数据不一致

有时很容易获得大量的训练数据，但是这些数据可能与实际生产中使用的数据不一致。

例如要创建一个移动应用（mobile app）拍摄花并识别其种类。为了构建验证集，可以从web上爬取大量的花的图片并用于训练，但是这些图片与使用该移动应用拍摄的花的图片不一致。但是验证集或测试集必须是代表性图片，即尽可能是使用该移动应用拍摄的花的图片。此时在训练集上训练该数据后发现模型在验证集上表现不好，原因通常有两种：

- 模型过拟合。
- 训练集（web图片）与验证集（移动应用图片）数据不一致。

为了找出具体原因，一种方法是留出部分（不包括验证集的）训练集作为**训练-开发集（train-dev set）**，当模型在训练集（不包括训练-开发集与验证集）上训练后，在训练-开发集上评估模型。如果表现好，则模型没有过拟合，此时如果模型在验证集上表现差，则数据不一致。如果表现差，则模型过拟合（不代表没有数据不一致的问题，但首先要解决过拟合的问题），此时可以简化或约束模型、获得更多训练数据并清洗训练数据。

# <a name="模型训练">模型训练</a>

## 泛化误差

统计学和机器学习的一个重要理论成果是：模型的泛化误差可以表示为三种不同误差的和：

- **偏差（bias）**：该部分的泛化误差是由错误的假设导致的。例如假定数据是线性的但实际上是二次的。一个高偏差的模型最有可能欠拟合训练数据。
- **方差（variance）**：该部分的泛化误差是由模型对训练数据中的微小变化过于敏感导致的。如果模型的自由度很高（比如高次多项式模型）可能有很高的方差，从而过拟合训练数据。
- **不可约误差（irreducible error）**：该部分的泛化误差是由数据中的噪音（noisiness）导致的。减少这部分误差的唯一方法是清洗（clean up）数据（例如修复数据源，如损坏的传感器，或检测并移除离群值）。

> A high-bias model is most likely to underfit the training data

通常，增加模型的复杂度会增加它的方差但是减少它的偏差；反之，减少模型的复杂度会增加它的偏差并减少它的方差。这被称为偏差/方差折中（the bias/variance trade-off）。

### 学习曲线

除了使用交叉验证评估模型的泛化性能外，也可以使用**学习曲线（learning curve）**查看模型复杂度与泛化性能。学习曲线是模型在训练集与验证集上的性能关于训练集大小或训练迭代次数的函数。要想产生该图像（plot），需要在训练集不同大小的子集上训练模型若干次。

下面继续[多项式回归](#(回归)(多项式回归))的例子来演示学习曲线的使用。

```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

for style, width, degree in (("g-", 1, 300), ("b--", 2, 2), ("r-+", 2, 1)):
    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)
    std_scaler = StandardScaler()
    lin_reg = LinearRegression()
    polynomial_regression = Pipeline([
            ("poly_features", polybig_features),
            ("std_scaler", std_scaler),
            ("lin_reg", lin_reg),
        ])
    polynomial_regression.fit(X, y)
    y_newbig = polynomial_regression.predict(X_new)
    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)

plt.plot(X, y, "b.", linewidth=3)
plt.legend(loc="upper left")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-3, 3, 0, 10])
plt.show()
```

采用多个多项式拟合数据，可以看到高次模型对训练集的拟合效果很好，但是严重过拟合；线性模型欠拟合。在这种情况下二次模型泛化性能最好。

下面查看`LinearRegression`模型的学习曲线：

```python
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

def plot_learning_curves(model, X, y):
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)
    train_errors, val_errors = [], []
    for m in range(1, len(X_train)):
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))
        val_errors.append(mean_squared_error(y_val, y_val_predict))

    plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train")
    plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val")
    plt.legend(loc="upper right", fontsize=14)
    plt.xlabel("Training set size", fontsize=14)
    plt.ylabel("RMSE", fontsize=14)
```

```python
lin_reg = LinearRegression()
plot_learning_curves(lin_reg, X, y)
plt.axis([0, 80, 0, 3])
plt.show()    
```

当训练集中的实例数量非常少时，模型能够完美拟合训练集（曲线开始于0的原因）；随着新的实例不断加入训练集，模型不能完美拟合训练集，因为数据中有噪音且不是线性的。因此训练集的误差不断上升直到到达高原，此时加入向训练集中加入新的实例不会使平均误差明显更好或更差。

当模型在很少的训练实例上训练时，模型无法很好泛化（初始验证误差大）；当模型在更多训练集上训练时，模型验证误差逐渐减小。但是由于直线无法很好拟合数据，因此误差最终到达高原，与训练集的曲线很近。

因此模型欠拟合：两个曲线都到达高原，它们接近且相当高。此时添加更多训练示例没有用，需要使用更复杂的模型或设计更好的特征。

下面查看10次多项式模型在相同数据上的学习曲线：

```python
from sklearn.pipeline import Pipeline

polynomial_regression = Pipeline([
        ("poly_features", PolynomialFeatures(degree=10, include_bias=False)),
        ("lin_reg", LinearRegression()),
    ])

plot_learning_curves(polynomial_regression, X, y)
plt.axis([0, 80, 0, 3])
plt.show()
```

与前一个曲线相似，但是有两个非常重要的特征：训练集上的误差比线性回归模型小得多；两条曲线间存在间隙（gap），这意味模型在训练集上的表现比在验证集上的表现明显要好。这是过拟合的特征。如果使用更大的训练集，两条曲线将继续接近。

### 早停法

**早停法（early stopping）**是一种正则化迭代学习算法（例如梯度下降）的方法，它在验证误差达到最小值时停止训练过程。

对于随机梯度下降与小批量梯度下降，学习曲线（以迭代次数为自变量）不会太平滑，因此可能很难知道有没有到达最小值。一种方法是当验证误差超过最小值一定时间后停止训练，然后回滚模型参数到验证误差最小的地方。

#### 例

下面是一个早停法示例：

```python
import numpy as np
from sklearn.model_selection import train_test_split

np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)

X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)
```

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_squared_error
from copy import deepcopy

poly_scaler = Pipeline([
        ("poly_features", PolynomialFeatures(degree=90, include_bias=False)),
        ("std_scaler", StandardScaler())
    ])

X_train_poly_scaled = poly_scaler.fit_transform(X_train)
X_val_poly_scaled = poly_scaler.transform(X_val)

sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
                       penalty=None, learning_rate="constant", eta0=0.0005, random_state=42)  # 因为设置了warm_start=True，当fit方法被调用时，它会在停止的地方继续训练，而不是从头开始。

minimum_val_error = float("inf")
best_epoch = None
best_model = None
for epoch in range(1000):
    sgd_reg.fit(X_train_poly_scaled, y_train)
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    val_error = mean_squared_error(y_val, y_val_predict)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch = epoch
        best_model = deepcopy(sgd_reg)
```

```python
import matplotlib.pyplot as plt

sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
                       penalty=None, learning_rate="constant", eta0=0.0005, random_state=42)

n_epochs = 500
train_errors, val_errors = [], []
for epoch in range(n_epochs):
    sgd_reg.fit(X_train_poly_scaled, y_train)
    y_train_predict = sgd_reg.predict(X_train_poly_scaled)
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    train_errors.append(mean_squared_error(y_train, y_train_predict))
    val_errors.append(mean_squared_error(y_val, y_val_predict))

best_epoch = np.argmin(val_errors)
best_val_rmse = np.sqrt(val_errors[best_epoch])

plt.annotate('Best model',
             xy=(best_epoch, best_val_rmse),
             xytext=(best_epoch, best_val_rmse + 1),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.05),
             fontsize=16,
            )

best_val_rmse -= 0.03  # 只是为了让图更好看。
plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], "k:", linewidth=2)
plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="Validation set")
plt.plot(np.sqrt(train_errors), "r--", linewidth=2, label="Training set")
plt.legend(loc="upper right", fontsize=14)
plt.xlabel("Epoch", fontsize=14)
plt.ylabel("RMSE", fontsize=14)
plt.show()
```

```python
best_epoch, best_model
```

## 梯度下降

**梯度下降（Gradient Descent）**是一种通用的优化算法，能够找到各种问题的最优解。梯度下降的核心思想（general idea）是迭代调整参数以最小化损失函数。梯度下降度量误差函数相对于参数向量$\pmb{θ}$的局部梯度，并沿梯度下降的方向前进，一旦梯度为零，就达到了最小值（类似于如果你迷失在山中，你只能感觉到脚下地面的坡度时，沿着最陡的斜坡方向下坡，这是快速到达谷底的一个好策略）。具体来说，它首先**随机初始化（random initialization）**参数$\pmb{\theta}$，然后令其每次沿梯度的反方向前进一小步，试图降低损失函数，直到算法收敛（converges）到最小值。

![梯度下降。模型参数被随机初始化，并被反复调整以最小化成本函数。在这里，学习步长与代价函数的斜率成正比（proportional to），因此当参数接近最小值时，学习步长逐渐变小。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\梯度下降.png)

梯度下降的步长由超参数**学习率（learning rate）**决定。如果学习率过小，则算法需要很多次迭代才能收敛，需要花费很多时间；如果学习率过大，则损失函数可能会跳到山谷的另一边，甚至可能比以前更高。这可能导致算法发散（diverge），值越来越大，找不到好的解。

要想找到好的学习率，可以使用网格搜索。如果在搜索过程中需要限制迭代次数以让网格搜索排除需要花费很长时间才收敛的模型。如果迭代次数过少，则算法停止时仍然远离最优解；如果迭代次数过多，则模型参数不再改变时仍然没有停止。可以将迭代次数设置为一个很大的值，然后当梯度向量非常小（当它的范数（norm）小于一个很小的值$\epsilon$（被称作**tolerance**））时，中断算法，因为这意味着梯度下降几乎已经到达了最小值。

学习率可以在训练中被动态调整。决定每次迭代时的学习率的函数被称为**学习调度（learning schedule）**。

![学习率过小](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\学习率过小.png)

![学习率过大](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\学习率过大.png)

另外，有些损失函数的形状不是碗状的，而是存在洞穴、山脊、高原等各种不规则的地形，导致梯度下降难以收敛到最小值。梯度下降有可能令参数$\pmb{\theta}$收敛到一个局部最小值（local minimum）而不是全局最小值（global minimum）；梯度下降也可能令$\pmb{\theta}$花费过多时间通过高原，如果此时过早停止梯度下降过程，则永远也不能到达全局最小值。

![梯度下降的缺陷](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\梯度下降的缺陷.png)

线性回归的MSE损失函数是凸函数（convex function），因此没有局部最小值，只有一个全局最小值；并且它是一个连续函数（它的导数是Lipschitz连续的），斜率不会突然变化。这两个因素保证梯度下降保证能够任意接近全局最小值（只要时间足够且学习率不过大）。

实际上，该损失函数是碗状的（如果特征的尺度差别很大，就是一个拉长的碗）。

<a name="(模型训练)(梯度下降)(1)">![有（左）和没有（右）特征缩放的梯度下降。前者可以直接向最小值移动，到达最小值的速度快；后者首先沿着一个几乎与全局最小值正交的方向前进，最后沿着一个几乎平坦的前进（march down），它最终将到达最小值，但需要很长时间。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\有（左）和没有（右）特征缩放的梯度下降.png)</a>

当使用梯度下降时，应该要确保所有特征的尺度相似（例如使用Scikit-Learn的```StandardScaler```类），否则它的收敛时间会长得多。

上图还说明了一个事实：训练模型意味着搜索模型参数的组合，使其最小化（在训练集上的）损失函数。该搜索是在模型的**参数空间（parameter space）**中进行的，参数越多则模型维度越多，搜索就越困难。

### 批量梯度下降

**批量梯度下降（Batch Gradient Descent）**在每个梯度下降步中使用整个训练集（这也是为什么该算法叫批量梯度下降的原因，“Full Gradient Descent”也许是一个更好的名字）。

MSE对每个参数$\theta_j$的偏导（注意包括$\theta_0$，$\pmb{x}$包括$x_0$）：
$$
\frac{\partial}{\partial(\theta_j)}MSE(\pmb{\theta})=\frac{2}{m}\sum_{i=1}^m(\pmb{\theta}^T\pmb{x}^{(i)}-y^{(i)})x_j^{(i)}
$$
也可以一次性计算MSE对所有参数的偏导：
$$
\nabla_{\pmb{\theta}} MSE(\pmb{\theta})=\left(
\begin{matrix}
{\frac{\partial}{\partial\theta_0}MSE(\pmb{\theta})}\\
{\frac{\partial}{\partial\theta_1}MSE(\pmb{\theta})}\\
{.}\\
{.}\\
{.}\\
{\frac{\partial}{\partial\theta_n}MSE(\pmb{\theta})}
\end{matrix}
\right)
=\frac{2}{m}\pmb{X}^T(\pmb{X}\pmb{\theta}-\pmb{y})
$$
当计算完梯度向量后，就可以执行梯度下降步了：
$$
\pmb{\theta}^{(next\ step)}=\pmb{\theta}-\eta\nabla_{\pmb{\theta}}MSE(\pmb{\theta})
$$
其中$\eta$表示学习率。

当训练集很大时，批量梯度下降的速度非常慢；但是它对特征数量的扩展性良好，当有非常多的特征时，使用梯度下降训练一个线性回归模型的速度要比使用正规方程或SVD分解快得多。

当损失函数是凸函数，且斜率不会突然变化，则使用固定学习率的批量梯度下降最终能收敛到最优解，且需要花费$O(1/\epsilon)$次迭代。

> Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but you may have to wait a while: it can take $O(1/\epsilon)$ iterations to reach the optimum within a range of $\epsilon$, depending on the shape of the cost function.

#### 例

下面继续[线性回归](#(回归)(线性回归))的例子来演示批量梯度下降的实现：

```python
eta = 0.1
n_iterations = 1000
m = 100

theta = np.random.randn(2,1)

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
```

查看theta：

```python
theta
```

可以看出与正规方程得到的解一致。

进行预测：

```python
X_new_b.dot(theta)
```

结果同样一致。

下面绘制出不同的学习率下梯度下降的前10步：

```python
theta_path_bgd = []

def plot_gradient_descent(theta, eta, theta_path=None):
    m = len(X_b)
    plt.plot(X, y, "b.")
    n_iterations = 1000
    for iteration in range(n_iterations):
        if iteration < 10:
            y_predict = X_new_b.dot(theta)
            style = "b-" if iteration > 0 else "r--"
            plt.plot(X_new, y_predict, style)
        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
        theta = theta - eta * gradients
        if theta_path is not None:
            theta_path.append(theta)
    plt.xlabel("$x_1$", fontsize=18)
    plt.axis([0, 2, 0, 15])
    plt.title(r"$\eta = {}$".format(eta), fontsize=16)
```

```python
np.random.seed(42)
theta = np.random.randn(2,1)

plt.figure(figsize=(10,4))
plt.subplot(131); plot_gradient_descent(theta, eta=0.02)  # 过低的学习率：需要花费很长时间才能得到解。
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)  # 合适的学习率：花费少许迭代就能收敛到解。
plt.subplot(133); plot_gradient_descent(theta, eta=0.5)  # 过高的学习率：算法发散，到处跳跃，实际上每一步都离解决方案越来越远。

plt.show()
```

### 随机梯度下降

**随机梯度下降（Stochastic Gradient Descent，SGD）**每步选择训练集中的一个随机实例，并基于该实例计算梯度。

一次处理一个实例使得算法快得多，因为它在每次迭代中操作的数据非常少；由于每次迭代内存中只需要一个实例，因此这使得使用随机梯度下降训练大规模训练集成为可能（随机梯度下降可以实现为核心外算法（out-of-core algorithm））。

另一方面，由于随机梯度下降的随机（stochastic，即random ）特性，该算法远不如批量梯度下降规则（regular）：损失函数不会缓慢下降直到最小值，而是会上下波动，只是总体上（on average）会下降。随着时间的推移，它最终将非常接近最小值，但一旦达到这里，它将继续反弹，永远不会稳定下来。因此，一旦算法停止，最终的参数值足够好，但不是最优的。

> Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down (see Figure 4-9). So once the algorithm stops, the final parameter values are good, but not optimal.

![使用随机梯度下降，每个训练步会快得多，但是比使用批量梯度下降也随机得多](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\随机梯度下降.png)

如果损失函数非常不规则（例如存在洞穴、山脊、高原等各种不规则的地形），这种训练方式实际上可以帮助算法跳出局部最小值，因此相比于批量梯度下降，随机梯度下降更有可能找到全局最小值。因此，随机性（randomness）好在有利于避开局部最优解，但坏在算法不能稳定在最小值。

对此，一种解决方法是逐渐减少学习率：开始时学习率被设置为较大值（有助于快速取得进展并跳出局部最小值），然后越来越小，使得算法在全局最小值处稳定。该过程类似于模拟退火（simulated annealing）。如果学习率递减得过快，则可能陷入局部最小值，甚至在到达最小值的半路上就停止了；如果学习率递减得很慢，则会长时间在最小值附近，并最终得到次优解（如果训练停止过早的话）。

> If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early.

按照惯例，每轮迭代次数记为$m$，每轮迭代被称作一**代（epoch）**。

如果要使用梯度下降，必须要保证训练实例独立同分布，以确保参数向全局最优解行进，否则（例如根据标签对实例进行排序）不会在全局最优解处稳定。一个简单的实现方法是在训练时混洗所有数据：

- 随机选取每个实例。
- 如果要使算法在每代使用所有的实例，则可以（在每代的开始）将训练集混洗（shuffle）（要确保输入特征与标签被同时混洗），然后迭代时依次使用每个实例，然后再混洗，如此反复。然后该方法通常收敛得更慢。

下面演示随机梯度下降的实现：

```python
theta_path_sgd = []
m = len(X_b)
np.random.seed(42)
```

```python
n_epochs = 50
t0, t1 = 5, 50

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    for i in range(m):
        if epoch == 0 and i < 20:   # 显示训练的前20步。
            y_predict = X_new_b.dot(theta)
            style = "b-" if i > 0 else "r--"
            plt.plot(X_new, y_predict, style)
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
        theta_path_sgd.append(theta)

plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()                                  
```

```python
theta
```

结果相当好。

使用Scikit-Learn的`SGDRegressor`类通过随机梯度下降执行线性回归（`SGDRegressor`默认最小化平方误差）：

```python
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)  # 最多运行1000代（max_iter=1000）；如果在某代损失下降小于0.001则停止（tol=1e-3）；初始学习率为0.1（eta0=0.1）；使用默认的学习调度；不使用任何正则化（penalty=None）。
sgd_reg.fit(X, y.ravel())
```

```python
sgd_reg.intercept_, sgd_reg.coef_
```

结果相当好。

### 小批量梯度下降

**小批量梯度下降（Mini-batch Gradient Descent）**每步选择训练集中的一个**小批量（mini-batch）**，并基于该小批量计算梯度。小批量就是小的随机实例集（small random sets of instances）。

相比于随机梯度下降，小批量的主要优点是可以从矩阵运算的硬件优化中获得性能提升，尤其是在使用GPU时。

小批量梯度下降在参数空间中的进展要规整，尤其当小批量比较大时。因此，小批量梯度下降最终将比随机梯度下降更接近（a bit closer to）最小值，但它可能更难摆脱局部最小值。

> The algorithm’s progress in parameter space is less erratic than with Stochastic GD, especially with fairly large mini-batches. 

<a name=(模型训练)(梯度下降)(小批量梯度下降)(1)>![三种梯度下降算法训练过程中在参数空间中的经过的路径，它们最终都接近最小值。批量梯度下降的路径实际上停在了最小值处，而随机梯度下降与小批量梯度下降会波动。但是批量梯度下降每步需要花费很多时间，且如果使用好的学习调度，随机梯度下降与小批量梯度下降也能到达最小值。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\参数空间中的梯度下降路径.png)</a>

以下是对作用于线性回归的算法的比较（$m$表示训练实例数量，$n$表示特征数量）：

<table>
    <tr>
        <th>算法</th>
        <th>对大m</th>
        <th>是否支持核心外学习</th>
        <th>对大n</th>
        <th>超参数</th>
        <th>是否需要缩放数据</th>
        <th>Scikit-Learn</th>
    </tr>
    <tr>
        <td>正规方程</td>
        <td>快</td>
        <td>否</td>
        <td>慢</td>
        <td>0</td>
        <td>否</td>
        <td>未提供</td>
    </tr>
    <tr>
        <td>SVD</td>
        <td>快</td>
        <td>否</td>
        <td>慢</td>
        <td>0</td>
        <td>否</td>
        <td>LinearRegression</td>
    </tr>
    <tr>
        <td>批量梯度下降</td>
        <td>慢</td>
        <td>否</td>
        <td>快</td>
        <td>2</td>
        <td>是</td>
        <td>SGDRegressor</td>
    </tr>
    <tr>
        <td>随机梯度下降</td>
        <td>快</td>
        <td>是</td>
        <td>快</td>
        <td>&gt;=2</td>
        <td>是</td>
        <td>SGDRegressor</td>
    </tr>
    <tr>
        <td>小批量梯度下降</td>
        <td>快</td>
        <td>是</td>
        <td>快</td>
        <td>&gt;2</td>
        <td>是</td>
        <td>SGDRegressor</td>
    </tr>
</table>
训练完成后，这些算法都会产生非常相似的模型，并且按照同样方式进行预测。

下面演示小批量梯度下降的实现。

```python
theta_path_mgd = []

n_iterations = 50
minibatch_size = 20

np.random.seed(42)
theta = np.random.randn(2,1)

t0, t1 = 200, 1000
def learning_schedule(t):
    return t0 / (t + t1)

t = 0
for epoch in range(n_iterations):
    shuffled_indices = np.random.permutation(m)
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0, m, minibatch_size):
        t += 1
        xi = X_b_shuffled[i:i+minibatch_size]
        yi = y_shuffled[i:i+minibatch_size]
        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(t)
        theta = theta - eta * gradients
        theta_path_mgd.append(theta)
```

```python
# 查看theta。
theta
```

最后，绘制图[参数空间中的梯度下降路径](#(模型训练)(梯度下降)(小批量梯度下降)(1))。

```python
theta_path_bgd = np.array(theta_path_bgd)
theta_path_sgd = np.array(theta_path_sgd)
theta_path_mgd = np.array(theta_path_mgd)
```

```python
plt.figure(figsize=(7,4))
plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], "r-s", linewidth=1, label="Stochastic")
plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], "g-+", linewidth=2, label="Mini-batch")
plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], "b-o", linewidth=3, label="Batch")
plt.legend(loc="upper left", fontsize=16)
plt.xlabel(r"$\theta_0$", fontsize=20)
plt.ylabel(r"$\theta_1$   ", fontsize=20, rotation=0)
plt.axis([2.5, 4.5, 2.3, 3.9])
plt.show()
```

## 自动微分

求函数的偏导主要有四种方法：手动求微分（manual differentiation）、**有限差分近似（finite difference approximation）**、前向自动微分（forward-mode autodiff）与反向自动微分（reverse-mode autodiff）。TensorFlow实现的是反向自动微分。

自动计算导数被称为**自动微分（automatic differentiation/Autodiff）**。有多种自动微分技术，它们各有利弊。

> There are various autodiff techniques, with different pros and cons. 

### 手动求微分

手动求微分就是人利用微积分知识手动推导出导数公式。如果函数很复杂（complex），则该方法十分冗长乏味（tedious），并且可能犯错。

假设定义了一个函数$f(x,y)=x^2y+y+2$，现在要求其偏导$\partial f/\partial x$与$\partial f/\partial x$，通常用于执行梯度下降（或其他优化算法）。

手动求微分可以得到：$\partial f/\partial x=2xy$，$\partial f/\partial y=x^2+1$。该函数比较简单，因此手动求微分并不复杂。

### 有限差分近似

有限差分近似基于导数定义计算导数：
$$
h'(x_0)=\underset{x\rightarrow x_0}{\lim}\frac{h(x)-h(x_0)}{x-x_0}=\underset{\epsilon\rightarrow0}{\lim}\frac{h(x_0+\epsilon)-h(x_0)}{\epsilon}
$$

有限差分近似将$(h(x_0+\epsilon)-h(x_0))/\epsilon$作为$h(x)$在$x=x_0$处的近似导数值，其中$\epsilon$为一个很小的数。

以下代码利用有限差分近似计算$f(x,y)$在$x=3$与$y=4$处的导数（$\partial f/\partial x\ (3,4)$）。在该例中，$f(x,y)$被调用了4次，但可以优化使其只被调用3次。

```python
def f(x, y):
    return x**2*y + y + 2

def derivative(f, x, y, x_eps, y_eps):
    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)

df_dx = derivative(f, 3, 4, 0.00001, 0)
df_dy = derivative(f, 3, 4, 0, 0.00001)
```

```python
print(df_dx)
```

```python
print(df_dy)
```

有限差分近似的缺点是结果不精确（如果函数很复杂（complicated）则精度会受很大影响）；并且如果有$n$个参数，则至少需要调用$h(x)$ $n+1$次，对于大型神经网络，这导致有限差分近似效率很低。

有限差分近似比较简单，可以作为检验其他方法是否正确实现的工具。例如，如果有限差分近似得出的结果与手动求导得出的结果不一致，则后者可能出错。

### 前向自动微分

前向自动微分构建函数的计算图，然后自底向上（从输入到输出）利用求导规则计算函数的各个子表达式的导数，直到到达根节点计算出整个函数的导数。在该过程中，前向自动微分会构建出另一个表示求导过程的计算图，被称为**符号微分（symbolic differentiation）**，它有两个好特征：一旦导数的计算图生成，我们就可以多次使用它来计算给定函数对参数在任意值处的导数；可以在结果图中运行前向自动微分得到二阶导数，如此下去得到多阶导数。

函数$g(x,y)=5+xy$的前向自动微分过程如下图，它计算的是$\partial f/\partial x\ (3,4)$：

![前向自动微分](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\前向自动微分.png)

首先计算叶结点的偏导：$\partial 5/\partial x=0$，$\partial x/\partial x=1$，$\partial y/\partial x=0$；然后计算$\partial(xy)/\partial x$：由于$\partial(u\times v)=\partial v/\partial x\times u+v\times \partial u/\partial v$，因此在右边构建一个图表示$0\times x+y\times1$；最后，由于求和函数的导数等于被求和函数的导数之和，因此创建一个额外结点并将其连接到计算的图的其他部分，得到正确的偏导：$\partial g/\partial x=0+(0\times x+y\times1)$。

> We can therefore construct a large part of the graph on the right, representing 0 × *x* + *y* × 1.

> So we just need to create an addition node and connect it to the parts of the graph we have already computed.

该等式可以被简化。对该计算图执行一些修剪步骤以消除所有不必要操作，可以得到一个小得多的图，它只有一个结点：$\partial g/\partial x=y$。在该例中简化十分容易，但是对于更复杂（complex）的函数，前向自动微分会产生一个非常难以简化的很大的图，导致性能次优（suboptimal）。

也可以在不构建新图的情况下运行前向自动微分，即直接计算中检结果。一种方法使用了**对偶数（dual number）**，其形如$a+b\epsilon$，其中$a$与$b$是实数（real number）而$\epsilon$为无穷小数（infinitesimal number）使得$\epsilon^2=0$但是$\epsilon\ne0$。对偶数在内存中被表示为一对浮点数，例如$42+24\epsilon$被表示为$(42.0,24.0)$。

> But it is also possible to run forward-mode autodiff without constructing a graph (i.e., numerically, not symbolically), just by computing intermediate results on the fly.

对偶数支持的部分运算如下：

- $\lambda(a+b\epsilon)=\lambda a+\lambda b\epsilon$
- $(a+b\epsilon)+(c+d\epsilon)=(a+c)+(b+d)\epsilon$
- $(a+b\epsilon)\times(c+d\epsilon)=ac+(ad+bc)\epsilon+(bd)\epsilon^2=ac+(ad+bc)\epsilon$

特别地，$h(a+b\epsilon)=h(a)+b\times h'(a)\epsilon$，因此计算$h(a+\epsilon)$后可以直接得到$h(a)$与$h'(a)$。


下图展示的是使用对偶数运行前向自动微分，它计算的是$\partial f/\partial x\ (3,4)$：

![使用对偶数的前向自动微分](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用对偶数的前向自动微分.png)

它计算的是$f(3+\epsilon,4)$，并输出对偶数，其第一个分量等于$f(3,4)$，第二个分量等于$\partial f/\partial x\ (3,4)$。

前向自动微分比有限差分近似更准确（accurate），但是它也有同样的主要缺陷（至少当输入很多输出很少的时候，处理神经网络时会遇到这种情况）：如果有$n$个参数，则它需要遍历（pass through）整个图$n$次去计算所有的偏导。

### 反向自动微分

反向自动微分首先正向（forward direction，从输入到输出）遍历整个图去计算每个结点的值，然后再反向（reverse direction，从输出到输入）遍历整个图去计算所有的偏导。

反向自动微分的思想是逐渐沿图下降，计算函数对每个连续结点的偏导，直到到达变量结点。因此，反向自动微分严重依赖**链式法则（chain rule）**：

$$
\frac{\partial f}{\partial x}=\frac{f}{n_i}\times\frac{n_i}{\partial x}
$$

函数$f(x,y)$的反向自动微分过程如下图（正向过程计算所有的结点值，这里只展示反向过程。结点值在每个结点的右下角，为清晰起见，每个结点都加了标签$n_1$\~$n_7$）：

![反向自动微分](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\反向自动微分.png)

输出结点为$n_7$：$f(3,4)=n_7=42$。

因为$n_7$为输出结点，所以$\partial f/\partial n_7=1$；$\partial f/\partial n_5=\partial f/\partial n_7\times\partial n_7 /\partial n_5$，而$n_7=n_5+n_6$，所以$\partial n_7/\partial n_5=1$，所以$\partial f/\partial n_5=1\times1=1$；$\partial f/\partial n_4=\partial f/\partial n_5\times\partial n_5/\partial n_4$，而$n_5=n_4\times n_2$，所以$\partial n_5/\partial n_4=n_2$，所以$\partial f/\partial n_4=1\times n_2=4$。该过程持续下去，直到到达图的底部，此时我们已经计算了$f(x,y)$在$x=3$与$y=4$处的所有偏导。

反向自动微分是强大且准确的技术，尤其是输入很多输出很少的时候，因为对于每个输出，它只要一次正向与反向过程就能计算输出对所有输入的导数。当训练神经网络时，我们通常需要最小化损失函数，因此只有一个输出（即损失），所以两次遍历（pass through）图就能计算所需梯度。反向自动微分也能处理不完全可微的函数，只要要求它去计算在可微点的偏导即可。

在上例中，数值结果（numerical result）在每个结点被直接计算，TensorFlow并不完全是这么做的。TensorFlow在执行反向自动微分时会创建一个新的计算图。换言之，它实现的是**符号（symbolic）**反向自动微分。通过该方法，计算损失函数相对于神经网络的所有参数的梯度的计算图只需要产生一次，然后可以在任何优化器（optimizer）需要计算梯度的时候被反复执行。同时，这也使得计算高阶导数成为可能。

> If you ever want to implement a new type of low-level TensorFlow operation in C++, and you want to make it compatible with autodiff, then you will need to provide a function that returns the partial derivatives of the function’s outputs with regard to its inputs. For example, suppose you implement a function that computes the square of its input: $f(x)=x^2$ . In that case you would need to provide the corresponding derivative function: $f′(x) = 2x$.

# 分类

本章引入MNIST数据集。该数据集包含70000张包含数字的小图片，每个数字由高中生和美国人口普查局员工手写。每个数字的标签为该图片所代表的数字。

> In this chapter we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.

MNIST数据集常被称为机器学习中的“hello world”：每当人们提出一种新的分类算法时，他们想知道该算法在MNIST上的表现如何；任何学习机器学习的人迟早都会与该数据集打交道。

Scikit-Learn提供许多帮助函数（helper function）来下载流行（popular）的数据集，下面使用它下载MNIST数据集：

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1, as_frame=False)  # 从Scikit-Learn 0.24开始，fetch_openml()默认返回一个Pandas DataFrame，使用as_frame=False避免之。
mnist.keys()
```

Scikit-Learn加载的数据集通常有相似的字典结构，包括：`DESCR`键：描述数据集；`data`键：包含一个数组，其中每个实例占一行，每个特征占一列；`target`键：包含一个数组及其标签（labels）。

查看这些数组：

```python
X, y = mnist["data"], mnist["target"]
X.shape
```

```python
y.shape
```

一共70000图片，每个图片像素为28 * 28（一维表示），故一共784个特征，每个特征表示一个像素的灰度（intensity），从0（白）到255（黑）。

看一下数据集中的一个数字：

```python
import matplotlib as mpl
import matplotlib.pyplot as plt

some_digit = X[0]
some_digit_image = some_digit.reshape(28, 28)
plt.imshow(some_digit_image, cmap=mpl.cm.binary)
plt.axis("off")

plt.show()
```

查看该数字的标签：

```python
y[0]
```

 将标签从string转换为整型（most ML algorithms expect numbers）。

```python
import numpy as np

y = y.astype(np.uint8)
```

这里定义一个用于绘制单个数字的函数：

```python
def plot_digit(data):
    image = data.reshape(28, 28)
    plt.imshow(image, cmap = mpl.cm.binary,
               interpolation="nearest")
    plt.axis("off")
```

这里定义一个用于绘制多个数字的函数：

```python
def plot_digits(instances, images_per_row=10, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size,size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row : (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)
    plt.imshow(image, cmap = mpl.cm.binary, **options)
    plt.axis("off")
```

绘制多个数字：

```python
plt.figure(figsize=(9,9))
example_images = X[:100]
plot_digits(example_images, images_per_row=10)
plt.show()
```

注意：在探查数据之前，应始终创建测试集并将其放在一边。

MNIST数据集实际上已经分为一个训练集（前60000个图像）和一个测试集（后10000个图像）（所以刚才才能绘制数字）。并且训练集已经混洗（shuffle）过了，这是一件好事：这样保证了交叉验证的每折（all cross-validation folds）都相似；同时一些学习算法对训练实例的顺序敏感，如果这些算法连续得到许多相似的属性，它的表现将很差，混洗则保证这种情况不会发生。

```python
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
```

## 二分类

可以区分两个类别的分类器被称为**二分类器（binary classifier）**。

下面训练一个二分类器，识别一个数字（这里是数字5）。

为分类任务创建目标向量：

```python
y_train_5 = (y_train == 5)
y_test_5 = (y_test == 5)
```

使用随机梯度下降分类器：

```python
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)  # SGDClassifier训练过程中依赖于随机性（hence the name “stochastic”），如果要产生可再现（reproducible）的结果，要设置random_state参数。
sgd_clf.fit(X_train, y_train_5)
```

预测：

```python
sgd_clf.predict([some_digit])
```

结果正确。

### 二分类任务的性能度量

#### 准确率

**准确率（accuracy）**即类别预测正确的实例占所有实例的比例。

准确率的概念同样适用于多分类任务。

使用交叉验证度量准确率：

```python
from sklearn.model_selection import cross_val_score

cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
```

在每折上都有超过93%的准确率，看上去效果很好。

也可以自己实现交叉验证，以对交叉验证过程有更多的控制权。以下代码与Scikit-Learn的cross_val_score函数作用大体相同：

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 分层抽样。

for train_index, test_index in skfolds.split(X_train, y_train_5):
    clone_clf = clone(sgd_clf)  # 深度复制模型与参数
    X_train_folds = X_train[train_index]
    y_train_folds = y_train_5[train_index]
    X_test_fold = X_train[test_index]
    y_test_fold = y_train_5[test_index]

    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_test_fold)
    n_correct = sum(y_pred == y_test_fold)
    print(n_correct / len(y_pred))
```

准确率通常不是分类器首选（preferred）的性能度量，尤其是当数据集有偏斜（一些类别出现频率远高于其他类别出现频率）时。当数据集有偏斜时，一个分类器只要将所有实例预测为偏斜的类别即可获得很高的准确率。

一个dumb分类器：

```python
from sklearn.base import BaseEstimator

class Never5Classifier(BaseEstimator):
    def fit(self, X, y=None):
        pass
    def predict(self, X):
        return np.zeros((len(X), 1), dtype=bool)
```

```python
never_5_clf = Never5Classifier()
cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring="accuracy")  # cross_val_score接受估计器。
```

可以看到，即使dumb分类器同样可以得到很高的准确率（超过90%）。

#### 混淆矩阵

为了更好地评估分类器的性能，需要查看**混淆矩阵（confusion matrix）**。混淆矩阵的每行表示**实际类别（actual class）**，每列表示**预测类别（predicted class）**。

混淆矩阵的概念同样适用于多分类任务。

对于二分类任务，混淆矩阵的第一行表示**负类（negative class）**，第二行表示**正类（positive class）**。对于二分类任务，混淆矩阵的4个位置分别为**真阴性（true negative）**（正确分类为负类的实例，其数量记为$TN$）、**假阳性（false positive）**（错误分类为正类的实例，其数量记为$FP$）、**假阴性（false negative）**（错误分类为负类的实例，其数量记为$FN$）与**真阳性（true positive）**（正确分类为正类的实例，其数量记为$TP$）的数量。一个完美的分类器只有真阳性与真阴性。

查看混淆矩阵：

```python
from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)  # 和cross_val_score函数一样，cross_val_predict函数同样执行K折交叉验证，但是它返回对每个测试折所做的预测，而不是评估得分。
```

> This means that you get a clean prediction for each instance in the training set (“clean” meaning that the prediction is made by a model that never saw the data during training).

```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_train_5, y_train_pred)
```

完美的分类器的混淆矩阵：

```python
y_train_perfect_predictions = y_train_5
confusion_matrix(y_train_5, y_train_perfect_predictions)
```

##### 精确率、召回率与$F_1$得分

给定混淆矩阵，分类器的**精确率（precision）**表示正预测（positive prediction）的准确率，定义为：
$$
precision=\frac{TP}{TP+FP}
$$
要想获得完美的精确度，只需要做出一个正预测并保证它是正确的即可。但是这样分类器将忽略其他的正实例（positive instance），因此这不是很有用。因此精确度通常与另一个名为**召回率（recall）**的指标一起使用，它也被称为**灵敏度（sensitivity）**或**真阳性率（true positive rate，TPR）**，表示被分类器正确分类的正实例的比例，被定义为：
$$
recall=\frac{TP}{TP+FN}
$$
![一个混淆矩阵，包含真阴性（左上）、假阳性（右上）、假阴性（左下）和真阳性（右下）的示例](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个混淆矩阵.png)

精确率与召回率的调和平均数（harmonic mean）被称为**$F_1$得分（$F_1$ score）**。
$$
F_1=\frac{2}{\frac{1}{precision}+\frac{1}{recall}}=2\times\frac{precision\times recall}{precision+recall}=\frac{TP}{TP+\frac{FN+FP}{2}}
$$
Scikit-Learn提供一些函数计算分类器指标（metric，包括精确率与召回率），下面使用它计算精确率：

```python
from sklearn.metrics import precision_score, recall_score

precision_score(y_train_5, y_train_pred)
```

可以看到没有准确率显示得那么好。

计算召回率：

```python
recall_score(y_train_5, y_train_pred)
```

可以看到没有准确率显示得那么好。

```python
# 计算F1得分。

from sklearn.metrics import f1_score

f1_score(y_train_5, y_train_pred)
```

$F_1$得分对精确率与召回率相似的分类器有利。但是有时候我们更关心精确率或召回率之一（而不是同样关心两者）。例如，如果要训练一个分类器检测出适宜儿童观看的视频，则可能要求分类器具有高精确率但可以容忍低召回率（如果召回率很高但精确率相对较低，则可能需要添加人工管道去检查分类器的视频选择）；如果想要训练一个分类器来检测监控图像中的商店扒手，则分类器的精确率可以较低，只要其召回率很高。

##### 精确率/召回率折中

以``` SGDClassifier```为例，查看它是如何做出分类决策的。对于每个实例，``` SGDClassifier```根据一个**决策函数（decision function）**计算一个得分。如果该得分超过某个阈值，它就将该实例作为正类，否则将该实例作为负类。阈值越高，则召回率越低（非严格的单调递减），且通常精确率越高。增加精确率（通常）会降低召回率，反之亦然，这被称为**精确率/召回率折中（precision/recall trade-of）**。

![精确率与召回率折中的例子。图像根据分类器得分进行排序，得分超过所选阈值的图像被认为是正类。三个箭头处分别表示不同的阈值，并列出相应的精确率与召回率。注意当中间阈值向右移动一个数字后，精确率会下降为75%。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\精确率与召回率折中的例子.png)

使用分类器的decision_function方法（而不是predict方法）获取用于预测的决策分（注意Scikit-Learn不允许直接设置阈值）：

```python
y_scores = sgd_clf.decision_function([some_digit])
y_scores
```

```python
threshold = 0
y_some_digit_pred = (y_scores > threshold)  # SGDClassifier使用的阈值等于0，因此其等价于：y_some_digit_pred = sgd_clf.predict([some_digit])
```

```python
y_some_digit_pred
```

提升阈值：

```python
threshold = 8000
y_some_digit_pred = (y_scores > threshold)
y_some_digit_pred
```

此时分类器不能检测出该图片表示5，这证实了提高阈值会降低召回率。

如果要确定阈值，可以绘制精确率与召回率关于阈值的函数（横轴为阈值，纵轴为精确率或召回率）。

返回所有实例的决策分（需要指定返回决策分而不是预测结果）：

```python
y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,
                             method="decision_function")
```

计算在所有可能阈值下的精确率与召回率：

```python
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
```

绘制精确率与召回率作为阈值的函数，可以根据曲线选择阈值：

```python
def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision", linewidth=2)
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall", linewidth=2)
    plt.legend(loc="center right", fontsize=16)
    plt.xlabel("Threshold", fontsize=16)
    plt.grid(True)
    plt.axis([-50000, 50000, 0, 1])

recall_90_precision = recalls[np.argmax(precisions >= 0.90)]
threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]

plt.figure(figsize=(8, 4))
plot_precision_recall_vs_threshold(precisions, recalls, thresholds)
plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], "r:")
plt.plot([-50000, threshold_90_precision], [0.9, 0.9], "r:")
plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], "r:")
plt.plot([threshold_90_precision], [0.9], "ro")
plt.plot([threshold_90_precision], [recall_90_precision], "ro")
plt.show()
```

##### PR曲线（curve）

另一种确定阈值的方法是直接绘制精确率相对于召回率的曲线（横轴为召回率，纵轴为精确率，设定不同的阈值，曲线上的点表示在某个阈值下，精确率与召回率的取值）。这就是**PR（precision/recall）**曲线。

绘制PR曲线：

```python
def plot_precision_vs_recall(precisions, recalls):
    plt.plot(recalls, precisions, "b-", linewidth=2)
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.grid(True)

plt.figure(figsize=(8, 6))
plot_precision_vs_recall(precisions, recalls)
plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], "r:")
plt.plot([0.0, recall_90_precision], [0.9, 0.9], "r:")
plt.plot([recall_90_precision], [0.9], "ro")
plt.show()
```

同样可以根据曲线选择阈值。比如发现当召回率80%左右，精确率大幅下降，故选择使得召回率60%的阈值（在精确率大幅下降前的一个值）。当然，具体情况要具体分析。

假设目标是要达到90%的精确率：

```python
threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]
```

```python
threshold_90_precision
```

```python
# 使用以下代码预测（而不是分类器的predict方法）。
y_train_pred_90 = (y_scores >= threshold_90_precision)
```

```python
# 该预测的精确率。
precision_score(y_train_5, y_train_pred_90)
```

```python
# 该预测的召回率。
# 可以看出精确率但召回率低。
recall_score(y_train_5, y_train_pred_90)
```

##### ROC曲线

**receiver operating characteristic（ROC）**曲线也是常被二分类器使用的工具，它与PR相似，但是绘制的是**真阳性率（true positive rate，TPR）**（召回率的另一个名字）相对于**假阳性率（false positive rate，FPR）**的曲线。假阳性率表示被错误分类为正类的负实例的比例，而**真阴性率（true negative rate，TNR）**则表示被正确分类为负类的负实例的比例。因此$FRP=1-TNR$。TNR也被称为**特异度（specificity）**，因此ROC曲线绘制的是灵敏度相对于$1-特异度$的曲线。

$$
FPR=\frac{FP}{TN + FP}
$$

$$
TNR=\frac{TN}{TN+FR}
$$

##### PR曲线与ROC曲线的选择

根据经验，如果正类很少或相比于假阴性更关心假阳性，则使用PR曲线，否则使用ROC曲线。

> As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve.

例如，如果正类很少但使用了ROC曲线，分类器的实际性能可能被高估（一个分类器只要将全部实例预测为负类就能获得很高的召回率与真阴性率）。

要想绘制ROC曲线，首先使用`roc_curve`函数计算在不同阈值下的TPR与FPR：

```python
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)
```

绘制ROC曲线：

```python
def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)
    plt.ylabel('True Positive Rate (Recall)', fontsize=16)
    plt.grid(True)

plt.figure(figsize=(8, 6))
plot_roc_curve(fpr, tpr)
fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]
plt.plot([fpr_90, fpr_90], [0., recall_90_precision], "r:")
plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], "r:")
plt.plot([fpr_90], [recall_90_precision], "ro")
plt.show()
```

点线表示完全随机的分类器的ROC曲线，一个好的分类器会尽可能远离这条线，即朝左上角。

可以发现分类器的ROC曲线（与ROC AUC得分）表现相当好，这很大程度上是因为正类数量很少。而从PR曲线可以看出分类器的还有改善的空间（曲线还可以更接近左上角）。

一个比较各个分类器表现的方法是计算它们的（ROC曲线的）**曲线下面积（area under the curve，AUC）**：

```python
from sklearn.metrics import roc_auc_score

roc_auc_score(y_train_5, y_scores)
```

一个完美的分类器的ROC AUC等于1，一个完全随机的分类器的ROC AUC等于0.5。

下面训练一个```RandomForestClassifier```，并与```SGDClassifier```比较两者的ROC曲线与ROC AUC得分：

```python
from sklearn.ensemble import RandomForestClassifier

forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)
y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,
                                    method="predict_proba")  # RandomForestClassifier没有decision_function方法，而是有一个predict_proba方法，其返回一个数组，其中每个实例占一行，每个类别占一列，每个位置包含给定实例属于给定类的概率。
```

```python
y_scores_forest = y_probas_forest[:, 1]
fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)  # roc_curve接受标签与得分，这里用类别概率代替得分。
```

```python
recall_for_forest = tpr_forest[np.argmax(fpr_forest >= fpr_90)]

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, "b:", linewidth=2, label="SGD")
plot_roc_curve(fpr_forest, tpr_forest, "Random Forest")
plt.plot([fpr_90, fpr_90], [0., recall_90_precision], "r:")
plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], "r:")
plt.plot([fpr_90], [recall_90_precision], "ro")
plt.plot([fpr_90, fpr_90], [0., recall_for_forest], "r:")
plt.plot([fpr_90], [recall_for_forest], "ro")
plt.grid(True)
plt.legend(loc="lower right", fontsize=16)
plt.show()
```

看起来比`SGDClassifier`的更好（更接近左上角）。

```python
roc_auc_score(y_train_5, y_scores_forest)
```

`RandomForestClassifier`的ROC AUC得分比`SGDClassifier`的更高。

查看`RandomForestClassifier`精确率与召回率。

```python
y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)
precision_score(y_train_5, y_train_pred_forest)
```

```python
recall_score(y_train_5, y_train_pred_90)
```

## 多类分类（Multiclass Classification）

可以区分多个（$>2$）类别的分类器被称为**多类分类器（multiclass classifier）**或**多项式分类器（multinomial classifier）**。

一些算法（如SGD分类器、随机森林分类器、朴素贝叶斯分类器）天然可以处理多个类别，其他算法（如Logistic回归或SVM分类器）则是严格的二分类器。

有多种策略可以使用多个二进制分类器执行多类分类，例如：

- 为每个类别训练一个二分类器。要想对某个实例进行分类，则获取每个二分类器对该实例的决策分，然后选择一个类别，其对应的分类器输出的得分最高。这被称为**一对剩余（one-versus-the-rest，OvR）**（也被称为**一对全部（one-versus-all）**）策略。
- 为每对类别训练一个二分类器。要想对某个实例进行分类，则获取每个二分类器的分类结果，然后选择最多的类别。这被称为**一对一（one-versus-one，OvO）**策略。如果有$n$个类别，则需要训练$n\times (n-1)/2$个分类器。该方法的主要优点是：每个分类器只需要在其必须区分的两个类别的实例上进行训练。

> The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.

一些算法（如支持向量机分类器）随训练集大小的扩展能力很差，这些算法OvO更适用，因为在小的训练集上训练多个分类器的速度要快于在大的训练集上训练少量的分类器。对于大多数二分类算法，OvR更适用。

下面训练多类分类器，识别所有数字。

Scikit-Learn可以检测到尝试将二分类器算法用于多类分类的情况，此时它会自动运行OvR或OvO策略（取决于算法）。

使用支持向量机分类器（使用OvO策略）：

```python
from sklearn.svm import SVC

svm_clf = SVC(gamma="auto", random_state=42)
svm_clf.fit(X_train[:1000], y_train[:1000])
svm_clf.predict([some_digit])
```

查看决策分，为每个实例返回10个得分（而不是1个），每个类别一个得分：

```python
#? 得分代表什么？
some_digit_scores = svm_clf.decision_function([some_digit])
some_digit_scores
```

接下来查看得分最高的类别（是不是5）：

```python
np.argmax(some_digit_scores)  # 输出得分最高的类别的索引（注意：未必是数字本身）。
```

```python
svm_clf.classes_  # 当分类器被训练后，它将目标类别列表存放在classes_属性中。
```

```python
svm_clf.classes_[5]  # 5。
```

如果要强制使用OvO或OvR策略，则可以使用类```OneVsOneClassifier```或```OneVsRestClassifier```：创建类的实例并将分类器（甚至可以不是二分类器）传递给它的构造方法。

```python
from sklearn.multiclass import OneVsRestClassifier

ovr_clf = OneVsRestClassifier(SVC(gamma="auto", random_state=42))
ovr_clf.fit(X_train[:1000], y_train[:1000])
ovr_clf.predict([some_digit])
```

```python
len(ovr_clf.estimators_)
```

下面在不使用OvR或OvO策略的情况下训练一个多类分类器：

```python
sgd_clf.fit(X_train, y_train)
sgd_clf.predict([some_digit])
```

获得每个类别的得分：

```python
sgd_clf.decision_function([some_digit])
```

可以看出，类别5的得分最高，但是模型对类别3仍然有所怀疑。

交叉验证评估分类器：

```python
cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring="accuracy")
```

可以看到分类器表现不算差（在所有测试折上都大于84%）（随机的分类器只能有10%的准确率）。

缩放输入可以提高准确率（>89%）：

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))
cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring="accuracy")
```

## 错误分析

假设这是一个实际的项目，则要根据机器学习清单中的步骤来指导该项目。假设现在已经发现了一个较好的（promising）模型并想要改善它，一种方法是分析模型所犯的错误类型。

预测并获取混淆矩阵：

```python
y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)
conf_mx = confusion_matrix(y_train, y_train_pred)
conf_mx
```

得到混淆矩阵的图像表示：

```python
plt.matshow(conf_mx, cmap=plt.cm.gray)
plt.show()
```

绝大部分图像分布在主对角线上，意味着它们被正确分类。

“5”（5s）看起来比其他数字更黑，意味着数据集中表示“5”的图像很少或分类器对“5”的分类能力不如对其他数字的分类。实际上两者都存在。

> The 5s look slightly darker than the other digits, which could mean that there are fewer images of 5s in the dataset or that the classifier does not perform as well on 5s as on other digits. In fact, you can verify that both are the case.

将混淆矩阵除以相应类别中的图像数量，以便比较错误率（而不是错误的数量）：

```python
row_sums = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums
```

用0覆盖对角线上的值使得只保留错误，并绘制结果：

```python
np.fill_diagonal(norm_conf_mx, 0)
plt.matshow(norm_conf_mx, cmap=plt.cm.gray)
plt.show()
```

可以清晰看出分类器所犯的错误类型。

表示类“8”的列比较明亮，表明许多图像被错误分类为“8”（8s）；表示类“8”的行则没有这么糟糕，表明实际为“8”的图像总体上被正确分类为“8”。还可以看到“3”与“5”经常被混淆（“3”经常被错分类为“5”，“5”经常被错分类为“3”）。可以看出，混淆矩阵不一定是对称的。

分析混淆矩阵通常可以让我们深入了解改进分类器的方法。对于本例，可以看出应该花费精力减少假“8”（false 8s，即将图片错误分类为“8”的情况）。例如，可以为看起来像“8”（但是实际不是“8”）的数字收集更多的训练数据，使得分类器可以学习区分它们与实际的“8”；或设计新的特征来帮助分类器，例如编写一个算法来计算封闭圈（closed loop）的数量（例如”8“有2个，”6“有1个，“5”没有）；或预处理图像使得某些模式（比如封闭圈）更加突出。

分析单独错误也是深入了解分类器行为与失败原因的好方法，但是通常更困难且更耗时。

下面分析单独错误。

绘制“3”与“5”的例子：

```python
cl_a, cl_b = 3, 5
X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]
X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]
X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]
X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]

plt.figure(figsize=(8,8))
plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)
plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)
plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)
plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)
plt.show()
```

位于左边的两个5 * 5块表示分类为“3”的数字，位于右边的两个5 * 5块表示分类为“5”的数字。一些分类错误的数字（即位于左下角与右上角的块）写得十分糟糕，以至于人类也很难对其分类，但是大多数错误对我们而言都是很明显的。分类器犯（对人而言）明显的错误的原因是我们使用的分类器（`SGDClassifier`）过于简单（一个线性模型，为每个像素计算一个权值，当看到一个新图片，它单纯地将像素灰度加权求值，为每个类别生成一个得分。因此3与5只有一小部分的像素有差别，该模型也很容易混淆它们）。

“3”与“5”的主要区别在于连接顶部线段与底部弧线的小线段的位置。如果该小线段稍微往左移，则分类器可能将其分类为“5”，反之亦然。换言之，该分类器对图像移动与旋转比较敏感，因此一个减少“3/5”混淆的方法是预处理图片，以确保它们居中显式并且不会过于旋转，这也可能降低其他的错误率。

## 多标签分类

需要区分多个二元类别的分类系统被称为**多标签分类（multilabel classification）**系统。

例如：要想识别一个图像中的多个人，需要为每个要识别的人附上一个标签。假设分类器经过训练可以识别三种人脸：Alice、Bob与Charlie。当将一个包含Alice与Charlie的图片展示给分类器时，分类器应该输出$[1,0,1]$（表示Alice在图片中，Bob不在，Charlie在）。

下面训练一个多标签分类器（数字是不是大于等于7；数字是不是奇数）：

```python
from sklearn.neighbors import KNeighborsClassifier

y_train_large = (y_train >= 7)
y_train_odd = (y_train % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)
```

预测，为每个实例输出两个标签：

```python
knn_clf.predict([some_digit])
```

有多种方法去评估多标签分类器，选择正确的指标取决于具体项目。一种方法是为每个独立的标签计算$F_1$得分（或其他任何二分类器的指标），然后计算平均分：

```python
y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)
f1_score(y_multilabel, y_train_knn_pred, average="macro")
```

这假设所有标签同样重要，但是实际未必如此。如果想要给予不同的标签不同的权重，一个简单的方法是给每个标签一个等于它的**支持（support）**（即属于目标标签的实例数量）的权重。要想做到这点，在以上代码中设置`average=weighted`。

> In particular, if you have many more pictures of Alice than of Bob or Charlie, you may want to give more weight to the classifier’s score on pictures of Alice. One simple option is to give each label a weight equal to its *support* (i.e., the number of instances with that target label). To do this, simply set average="weighted" in the preceding code.

## 多输出分类

需要区分多个类别（可以是多类）的分类系统被称为**多输出分类（multioutput-multiclass classification，multioutput classification）**。

需要注意的是，有时候分类与回归的界限是模糊的。例如，预测像素灰度可以看做分类，但它更类似于回归。此外，多输出分类系统并不局限于分类任务，我们甚至可以构建一个系统为每个实例输出多个标签，包括类别标签与值标签。

下面训练一个多输出分类器。建立一个系统，可以移除图片中的噪音。它的输入是一个有噪音的图片，并希望它能输出一个干净的图片。这个分类器的输入是多标签的（每个像素一个标签），每个标签可以有多个值（图像灰度，从0到255）。

为图像增加噪音：

```python
noise = np.random.randint(0, 100, (len(X_train), 784))
X_train_mod = X_train + noise
noise = np.random.randint(0, 100, (len(X_test), 784))
X_test_mod = X_test + noise
y_train_mod = X_train
y_test_mod = X_test
```

查看测试集中的一张图片（注意，这窥探了测试集，在实际项目中不要这么做）：

```python
some_index = 0
plt.subplot(121); plot_digit(X_test_mod[some_index])  # 有噪音的图片。
plt.subplot(122); plot_digit(y_test_mod[some_index])  # 干净的图片。
plt.show()
```

查看预测结果：

```python
knn_clf.fit(X_train_mod, y_train_mod)
clean_digit = knn_clf.predict([X_test_mod[some_index]])
plot_digit(clean_digit)
```

# 回归

## <a name="(回归)(线性回归)">线性回归</a>

给定输入特征，**线性回归（Linear Regression）**模型通过计算输入特征的加权平均数并加上一个**偏置项（bias term）**（也被称为**截距项（intercept term）**）来作出预测：

> More generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term* (also called the *intercept term*), as shown in Equation 4-1.

$$
\widehat{y}=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
$$

其中，$\widehat{y}$表示预测值，$n$表示特征数；$x_i,0\le i\le n$表示第$i$个特征值；$\theta_j,0\le j\le n$表示模型参数（包括偏置项$\theta_0$与特征权重$\theta_1$，$\theta_2$，……，$\theta_n$）。

更简洁的写法如下：
$$
\widehat{y}=h_{\pmb{\theta}}(\pmb{x})=\pmb{\theta}\cdot\pmb{x}
$$
其中，$\pmb{\theta}$表示模型的参数向量，包括$\theta_j,0\le j\le n$；$\pmb{x}$表示实例的特征向量，包括$x_i,0\le i\le n$，其中$x_0=1$。$h_{\pmb{\theta}}$为假设函数（hypothesis function）。

### 模型训练

线性回归的最常用的性能指标为均方根误差，为了训练一个线性回归模型，需要找到$\pmb{\theta}$的取值，使得均方根误差最小。实际上，最小化均方误差（mean squared error，MSE）要更简单，并且结果相同：
$$
MSE(\pmb{X}, h_{\pmb{\theta}})=\frac{1}{m}\sum_{i=1}^m(\pmb{\theta}^T\pmb{x}^{(i)}-y^{(i)})^2
$$
为了简化表达，下面将$MSE(\pmb{X}, h_{\pmb{\theta}})$记为$MSE(\pmb{\theta})$。

当线性模型被训练完成，预测是一个非常快速的过程，预测的计算复杂度与实例数量与特征数量成线性关系。

#### 正规方程

线性模型有使得损失函数最小的闭式解（closed-form solution）（一个数学方程（mathematical equation）直接给出结果），这被称为**正规方程（Normal Equation）**：
$$
\widehat{\pmb{\theta}}=(\pmb{X}^T\pmb{X})^{-1}{\pmb{X}^T}\pmb{y}
$$
其中$\widehat{\pmb{\theta}}$表示最小化损失函数的$\pmb{\theta}$取值，$\pmb{y}$表示包含$y^{(1)}$到$y^{(n)}$的向量。

其中，对$\pmb{X}^T\pmb{X}$求逆的计算复杂度（computational complexity）通常为$O(n^{2.4})$到$O(n^3)$（取决于具体实现）。

下面生成线性数据（$y=4+3x_1+高斯噪音$）来演示正规方程的使用。

生成线性数据（linear-looking data）：

```python
import numpy as np

X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
```

绘制图像：

```python
import matplotlib.pyplot as plt

plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()
```

使用正规方程求解：

```python
X_b = np.c_[np.ones((100, 1)), X]
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```

```python
theta_best
```

期望得到4与3，因为有高斯噪音，实际得到比较接近的值。

进行预测：

```python
X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]
y_predict = X_new_b.dot(theta_best)
y_predict
```

 绘制模型预测结果：

```python
plt.plot(X_new, y_predict, "r-", linewidth=2, label="Predictions")
plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.legend(loc="upper left", fontsize=14)
plt.axis([0, 2, 0, 15])
plt.show()
```

使用Scikit-Learn执行线性回归：

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.intercept_, lin_reg.coef_
```

```python
lin_reg.predict(X_new)
```

```LinearRegression```类基于```scipy.linalg.lstsq```方法，可以直接调用之。该方法计算$\widehat{\pmb{\theta}}=\pmb{X}^+\pmb{y}$，其中$\pmb{X}^+$表示$\pmb{X}$的伪逆（pseudoinverse）（specifically, the Moore-Penrose inverse）。

```python
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
```

直接计算伪逆：

```python
np.linalg.pinv(X_b).dot(y)
```

Scikit-Learn的```LinearRegression```类SVD方法求解的时间复杂度大约为$O(n^2)$。

当特征数量非常多的时候，正规方程与SVD方法的求解速度会很慢；这两种方法的求解速度与训练集中的实例数量成线性关系，因此如果大规模的数据集可以容纳在内存中，它们可以高效地处理之。

## <a name="(回归)(多项式回归)">多项式回归</a>

可以使用线性模型拟合非线性数据，一个简单的方法是将每个特征的幂作为新特征，并在扩展的特征集上训练线性模型。该技术被称为**多项式回归（Polynomial Regression）**。

下面展示多项式回归的实现。

```python
import numpy as np
import numpy.random as rnd

np.random.seed(42)
```

生成非线性数据：

```python
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)
```

绘制图像：

```python
import matplotlib.pyplot as plt

plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-3, 3, 0, 10])
plt.show()
```

使用`PolynomialFeatures`转换数据：加上训练集中每个特征的平方项作为新特征：

```python
from sklearn.preprocessing import PolynomialFeatures

poly_features = PolynomialFeatures(degree=2, include_bias=False)  # interaction_only：是否只有交叉项， include_bias：是否有0次幂项（全1）。
X_poly = poly_features.fit_transform(X)
X[0]
```

```python
X_poly[0]
```

对转换后的数据执行线性回归：

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_
```

与原始结果相近。

查看拟合图像：

```python
X_new=np.linspace(-3, 3, 100).reshape(100, 1)
X_new_poly = poly_features.transform(X_new)
y_new = lin_reg.predict(X_new_poly)
plt.plot(X, y, "b.")
plt.plot(X_new, y_new, "r-", linewidth=2, label="Predictions")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.legend(loc="upper left", fontsize=14)
plt.axis([-3, 3, 0, 10])
plt.show()
```

多项式回归可以发现特征之间的关系（普通的线性回归无法做到），因为Scikit-Learn的```PolynomialFeatures```可以添加特征之间的（最多不超过给定次数的）组合。

Scikit-Learn的```PolynomialFeatures(degree=d)```将一个包含$n$个特征的数组转换为一个包含$(n+d)!/d!n!$个特征的数组。要当心特征数量的组合爆炸（combinatorial explosion）。

## 模型约束

正则化模型（即约束模型）使得模型的自由度越少就越难过拟合数据。对于多项式模型，一个简单的正则化方法是减少多项式的次数；对于线性模型，正则化通常通过约束模型的权重实现，以下是三种实现方式。

### 岭回归

**岭回归（Ridge Regression）**（也被称作**Tikhonov regularization**）将等于$\alpha\sum_{i=1}^n\theta_i^2$的**正则化项（regularization term）**加到损失函数上。这迫使模型不仅要拟合数据，还要保持权重尽可能小。需要注意，正则化项只应该在训练时加到损失函数上，模型训练完成后，要使用没有正则化的性能度量去评估模型的性能。

岭回归损失函数：
$$
J(\pmb{\theta})=MSE(\pmb\theta)+\alpha\frac{1}{2}\sum_{i=1}^{n}\theta_i^2
$$
需要注意$\theta_0$未被正则化。如果定义$\pmb{w}$为特征向量（包含$\theta_1$到$\theta_n$），则正则化项等于$\frac{1}{2}(||\pmb{w}||)_2^2$，其中$||\pmb{w}||$表示权重矩阵的$ℓ_2$范数（norm）。对于梯度下降，只要将$\alpha\pmb{w}$加到MSE梯度向量上。

岭回归的闭式解：
$$
\widehat{\pmb{\theta}}=(\pmb{X}^T\pmb{X}+\alpha\pmb{A})^{-1}{\pmb{X}^T}\pmb{y}
$$
其中$\pmb{A}$是$(n+1)\times(n+1)$维的单位矩阵，只是其最左上角的值为0，其对应偏置项。

我们可以通过计算闭式方程（closed-form equation）或通过执行梯度下降来执行岭回归。它们的利弊与通过它们执行线性回归的利弊一样。

超参数$\alpha$控制对模型正则化的程度。如果$\alpha=0$，则岭回归就是线性回归；如果$\alpha$非常大，则所有权重都接近0，结果（大致）是一条穿过数据平均值的平线。

岭回归对输入特征的尺度很敏感，因此在执行岭回归前要先缩放数据（例如使用```StandardScaler```）。这对大多数正则化模型都适用。

> It is important to scale the data (e.g., using a `StandardScaler`) before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.

通过Scikit-Learn使用闭式解执行岭回归（采用的闭式解是上述岭回归的闭式解的一种变体，使用André-Louis Cholesky提出的矩阵分解技术）：

> Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solution (a variant of Equation 4-9 that uses a matrix factorization technique by André-Louis Cholesky):

```python
import numpy as np

np.random.seed(42)
m = 20
X = 3 * np.random.rand(m, 1)
y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5
X_new = np.linspace(0, 3, 100).reshape(100, 1)
```

```python
from sklearn.linear_model import Ridge

ridge_reg = Ridge(alpha=1, solver="cholesky", random_state=42)
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
```

使用`Ridge`类并使用`sag` solver（Stochastic Average GD是Stochastic GD的一种变体，见[Minimizing Finite Sums with the Stochastic Average Gradient Algorithm](https://homl.info/12)）：

```python
ridge_reg = Ridge(alpha=1, solver="sag", random_state=42)
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
```

下面展示在一些线性数据上使用不同的$\alpha$值训练的岭模型（Ridge model）。

```python
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge

def plot_model(model_class, polynomial, alphas, **model_kargs):
    for alpha, style in zip(alphas, ("b-", "g--", "r:")):
        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()
        if polynomial:
            model = Pipeline([
                    ("poly_features", PolynomialFeatures(degree=10, include_bias=False)),
                    ("std_scaler", StandardScaler()),
                    ("regul_reg", model),
                ])
        model.fit(X, y)
        y_new_regul = model.predict(X_new)
        lw = 2 if alpha > 0 else 1
        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r"$\alpha = {}$".format(alpha))
    plt.plot(X, y, "b.", linewidth=3)
    plt.legend(loc="upper left", fontsize=15)
    plt.xlabel("$x_1$", fontsize=18)
    plt.axis([0, 3, 0, 4])

plt.figure(figsize=(8,4))
plt.subplot(121)
plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(122)
plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)

plt.show()
```

注意右图，增加`alpha`导致更平坦（也就是不太极端、更合理）的预测，因此减少模型的方差，但是增加模型的偏差。

也可以使用随机梯度下降：

```python
sgd_reg = SGDRegressor(penalty="l2", max_iter=1000, tol=1e-3, random_state=42)
sgd_reg.fit(X, y.ravel())
sgd_reg.predict([[1.5]])
```

### 套索回归

**套索回归（Least Absolute Shrinkage and Selection Operator Regression，Lasso Regression）**类似于岭回归，但是它的正则化项是权重向量的$ℓ_1$范数。其损失函数为：
$$
J(\pmb{\theta})=MSE(\pmb\theta)+\alpha\sum_{i=1}^{n}|\theta_i|
$$
套索回归的一个重要特征是它倾向于消除最不重要的特征的权重（设置其为0），换言之，套索回归自动执行特征选择并输出**稀疏模型（sparse model）**（只有很少的非零特征权重）。

套索损失函数在$\theta_i=0,1\le i\le n$处不可微，但是如果在$\theta_i=0$处使用一个**次梯度向量（subgradient vector）**则梯度下降仍然能很好工作。

以下是一个可以对套索回归执行梯度下降的次梯度向量等式（equation）。

$$
g(\pmb{\theta},J)=\nabla_{\pmb{\theta}}MSE(\pmb{\theta})+\alpha\left(
\begin{matrix}
{sign(\theta_1)}\\
{sign(\theta_1)}\\
{.}\\
{.}\\
{.}
{sign(\theta_n)}
\end{matrix}
\right)\ where\ sign(\theta_i)=
\left\{
\begin{aligned}
-1\ if\ \theta_i<0\\
0\ if\ \theta_i=0\\
+1\ if\ \theta_i>0
\end{aligned}
\right.
$$

下面展示在一些线性数据上使用不同的$\alpha$值训练的套索模型（相比于岭模型，这里使用了较小的$\alpha$值）：

```python
from sklearn.linear_model import Lasso

plt.figure(figsize=(8,4))
plt.subplot(121)
plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(122)
plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42) 

plt.show()
```

右图中的虚线（$\alpha=10^{-7}$）看起来像二次曲线，几乎线性：高次多项式特征的所有权重均等于零。可以看出索回归自动执行特征选择。

下面是套索回归的简短示例（也可以使用`SGDRegressor`，并设置`penalty=l1`。）：

```python
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])
```

### 弹性网络

**弹性网络（Elastic Net）**的正则化项是岭回归与套索回归正则化项的混合，并且混合率（mix ratio）$r$可被控制。当$r=0$，弹性网络等价于岭回归；当$r=1$，弹性网络等价于套索回归。

弹性网络的损失函数为：
$$
J(\pmb{\theta})=MSE(\pmb\theta)+r\alpha\sum_{i=1}^{n}|\theta_i|+\frac{1-r}{2}\sum_{i=1}^n\theta_i^2
$$
对于线性回归，正则化几乎总是最好至少要有一点的，因此通常不应该使用普通的线性回归。岭回归是很好的默认选项，但是如果怀疑只有少数特征时有用的，则最好使用套索回归或弹性网络，因为它们倾向于将无用的特征的权重减为0。通常，弹性网络优于套索回归，因为当特征的数量大于训练实例的数量或当若干特征之间强相关时，套索回归可能表现不稳定。

下面是弹性网络的简短示例：

```python
from sklearn.linear_model import ElasticNet

elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)  # l1_ratio对应混合率r。
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
```

## Logistic回归

前面指出，有些分类算法可以用于回归任务中，反之亦然。**Logistic回归（Logistic Regression）**（也称**Logit回归（Logit Regression）**）通常用于估计实例属于特定类的概率。如果估计的概率大于50%，模型预测该实例属于该类（被称作**正类（positive class）**，标签为“1”），否则模型预测该实例不属于该类（即属于**反类（negative class）**，标签为0）。这使它成为一个二分类器。

和线性回归模型一样，Logistic回归模型为输入特征计算一个加权和（包括偏置项）；和线性回归模型不同，它不会直接输出该结果，而是输出该结果的**logistic**：
$$
\widehat{p}=h_{\pmb{\theta}}(\pmb{x})=\sigma(\pmb{x}^T\pmb{\theta})
$$
Logistic（记为$\sigma(·)$）是一个**sigmoid 函数（sigmoid function）**（S形函数），它输出一个介于0到1之间的数字，其定义如下：
$$
\sigma(t)=\frac{1}{1+exp(-t)}
$$
下面绘制出Logistic函数。

```python
t = np.linspace(-10, 10, 100)
sig = 1 / (1 + np.exp(-t))
plt.figure(figsize=(9, 3))
plt.plot([-10, 10], [0, 0], "k-")
plt.plot([-10, 10], [0.5, 0.5], "k:")
plt.plot([-10, 10], [1, 1], "k:")
plt.plot([0, 0], [-1.1, 1.1], "k-")
plt.plot(t, sig, "b-", linewidth=2, label=r"$\sigma(t) = \frac{1}{1 + e^{-t}}$")
plt.xlabel("t")
plt.legend(loc="upper left", fontsize=20)
plt.axis([-10, 10, -0.1, 1.1])
plt.show()
```

一旦Logistic回归模型预测了一个实例$\pmb{x}$属于正类的概率（$\widehat{p}=h_{\pmb{\theta}}(\pmb{x})$），Logistic预测$\widehat{y}$就很容易了：
$$
\widehat{y}=\left\{
\begin{aligned}
0\ if\ \widehat{p}<0.5\\
1\ if\ \widehat{p}\ge0.5
\end{aligned}
\right.
$$
换言之，如果$\pmb{x}^T\pmb{\theta}\ge0$，则模型预测1，否则模型预测0。

Logistic回归模型的训练目标是设置参数$\pmb{\theta}$，使得模型对正实例（$y=1$）的预测概率高，对于反实例（$y=0$）的预测概率低。因此，对于单个实例$\pmb{x}$，Logistic回归模型采取的损失函数如下：
$$
c(\pmb{\theta})=\left\{
\begin{aligned}
-\log(\widehat{p})\ if\ y=1\\
-log(1-\widehat{p})\ if\ y=0
\end{aligned}
\right.
$$

模型在整个训练集上的损失函数是其在所有训练实例上的损失的平均，它可以被写成单个表达式，称为**log损失（log loss）**：
$$
J(\pmb{\theta})=-\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(\widehat{p}^{(i)})+(1-y^{(i)})\log(1-\widehat{p}^{(i)})]
$$
没有已知的闭式方程（closed-form equation）可以计算出最小化损失函数的$\pmb{\theta}$，但是该损失函数是凸函数（convex），因此可以使用梯度下降或其他任何优化算法找到全局最小值（只要学习率不过高并且等待足够长的时间）。损失函数对第$j$个模型参数$\theta_j$的偏导为：
$$
\frac{\partial}{\partial\theta_j}J(\pmb{\theta})=\frac{1}{m}\sum_{i=1}^m(\sigma(\pmb{\theta}^T\pmb{x}^{(i)})-y^{(i)})x_j^{(i)}
$$
该等式与MSE对每个参数$\theta_j$的偏导的等式很相似：对于每个实例，计算其预测误差并将其乘以第$j$个特征值，然后计算所有训练实例的平均值。

下面使用iris数据集去展示Logistic回归的使用。这是一个著名的数据集，包含三个不同物种（Iris setosa, Iris versicolor与Iris virginica）的150朵鸢尾花（iris）的萼片（sepal）和花瓣（petal）的长度和宽度。

> This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: *Iris setosa*, *Iris versicolor*, and *Iris virginica* (see Figure 4-22).

![三种鸢尾属植物的花（Flowers of three iris plant species）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\三种鸢尾属植物的花.png)

[^三种鸢尾属植物的花]: Photos reproduced from the corresponding Wikipedia pages. *Iris virginica* photo by Frank Mayfield ([Creative Commons BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/)), *Iris versicolor* photo by D. Gordon E. Robertson ([Creative Commons BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)), *Irissetosa* photo public domain.

加载并预处理数据：

```python
from sklearn import datasets
iris = datasets.load_iris()
list(iris.keys())
```

```python
print(iris.DESCR)
```

```python
X = iris["data"][:, 3:]  # 花瓣宽度。
y = (iris["target"] == 2).astype(np.int)  # 如果是Iris virginica,则y为1，否则y为0。
```

训练一个Logistic回归模型：

```python
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(solver="lbfgs", random_state=42)
log_reg.fit(X, y)
```

查看模型对花瓣宽度从0厘米到3厘米不等的花的估计概率：

> Let’s look at the model’s estimated probabilities for flowers with petal widths varying from 0 cm to 3 cm (Figure 4-23):

```python
X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]

plt.figure(figsize=(8, 3))
plt.plot(X[y==0], y[y==0], "bs")
plt.plot(X[y==1], y[y==1], "g^")
plt.plot([decision_boundary, decision_boundary], [-1, 2], "k:", linewidth=2)
plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris virginica")
plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2, label="Not Iris virginica")
plt.text(decision_boundary+0.02, 0.15, "Decision  boundary", fontsize=14, color="k", ha="center")
plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')
plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')
plt.xlabel("Petal width (cm)", fontsize=14)
plt.ylabel("Probability", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 3, -0.02, 1.02])
plt.show()
```

Iris virginica（用三角形表示）的花瓣长度为1.4\~2.5厘米，而其他鸢尾花的花瓣长度为0.1\~1.8厘米，注意到长度有一些重叠。当花瓣长度超过2厘米，分类器很确定（highly confident）花是Iris virginica，当花瓣长度小于1厘米，分类器很确定它不是Iris virginica。在这两个极端间，分类器不太确定。但是如果分类器使用```predict```（而不是```predict_proba```）方法去预测类别，它会输出最有可能的类别。**决策边界（decision boundary）**大约为1.6厘米，此时两类的概率都等于50%。如果花瓣宽度大于1.6厘米，分类器预测花为Iris virginica，否则预测花不是（即使不太确信（confident））。

```python
decision_boundary
```

```python
log_reg.predict([[1.7], [1.5]])
```

下面使用两个特征（花瓣宽度与长度）去预测（展示决策边界）：

```python
from sklearn.linear_model import LogisticRegression

X = iris["data"][:, (2, 3)]  # 花瓣长度与宽度。
y = (iris["target"] == 2).astype(np.int)

log_reg = LogisticRegression(solver="lbfgs", C=10**10, random_state=42)
log_reg.fit(X, y)

x0, x1 = np.meshgrid(
        np.linspace(2.9, 7, 500).reshape(-1, 1),
        np.linspace(0.8, 2.7, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]

y_proba = log_reg.predict_proba(X_new)

plt.figure(figsize=(10, 4))
plt.plot(X[y==0, 0], X[y==0, 1], "bs")
plt.plot(X[y==1, 0], X[y==1, 1], "g^")

zz = y_proba[:, 1].reshape(x0.shape)
contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)

left_right = np.array([2.9, 7])
boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]

plt.clabel(contour, inline=1, fontsize=12)
plt.plot(left_right, boundary, "k--", linewidth=3)
plt.text(3.5, 1.5, "Not Iris virginica", fontsize=14, color="b", ha="center")
plt.text(6.5, 2.3, "Iris virginica", fontsize=14, color="g", ha="center")
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.axis([2.9, 7, 0.8, 2.7])
plt.show()
```

虚线表示模型预测正类为50%概率的点集，这是模型的决策边界，它是线性的。

每个平行线表示模型输出给定概率的点集（点$\pmb{x}$的集合，满足：$\theta_0+\theta_1x_1+\theta_2x_2=0$）。所有分布在最右上角的直线上方的花都有90%以上的可能是Iris virginica。

Logistic回归模型同样可以使用$ℓ_1$或$ℓ_2$惩罚进行正则化（Scikit-Learn实际会默认加上$ℓ_2$惩罚）。

需要注意的是，控制Scikit-Learn的```LogisticRegression```模型的正则化强度的超参数不是```alpha```，而是与其相反的```C```。```C```越大，模型正则化强度越小。

### Softmax回归

**Softmax回归（Softmax Regression）**（也被称为**多项式Logistic回归（Multinomial Logistic Regression）**）是Logistic回归的推广形式，可以直接支持多类分类，而不需要训练并组合多个二分类器。方法如下：给定一个实例$\pmb{x}$，Softmax回归模型为每个类别$k$计算一个得分$s_k(\pmb{x})$，然后对得分应用**softmax方程（softmax function）**（也被称为**归一化指数（normalized exponential）**）来估计每个类别的概率。

注意Softmax回归分类器是多类分类器，不是多输出分类器，因此只能用于互斥的类（例如不能识别一张图片中的多个人）。

类别$k$的Softmax得分如下：
$$
s_k(\pmb{x})=\pmb{x}^T\pmb{\theta}^{(k)}
$$
注意，每个类别都有自己的专用参数向量${\theta}^{(k)}$。所有这些向量通常作为行存储在**参数矩阵（parameter matrix）**$\pmb{\Theta}$中。

一旦$\pmb{x}$的每个类别的得分计算完毕，就可以通过softmax函数去估计$\pmb{x}$属于每个类别的概率$\widehat{p}_k$：
$$
\widehat{p}_k=\sigma(\pmb{s}(\pmb{x}))=\frac{\exp(s_k(\pmb{x}))}{\sum_{j=1}^K\exp(s_j(\pmb{x}))}
$$
其中$K$是类别数量，$\pmb{s}(\pmb{x})$保存了$\pmb{x}$的每个类别的得分，$\sigma(\pmb{s}(\pmb{x}))$为给定$\pmb{x}$每个类别得分的情况下，$\pmb{x}$属于类别$k$的估计概率。

类似于Logistic回归分类器，Softmax回归分类器将估计概率最高的类别（即得分最高的类别）作为预测的类别：

$$
\widehat{y}=\underset{k}{argmax}\ \sigma(\pmb{s}(\pmb{x}))_k=\underset{k}{argmax}\ s_k(\pmb{x})=\underset{k}{argmax}((\pmb{\theta}^{(k)})^Tx)
$$

其中，$argmax$返回最大化函数的变量取值。在该例中，它返回一个最大化估计概率$\sigma(\pmb{s}(\pmb{x}))_k$的$k$值。

Softmax回归分类器的目标是使得模型为目标类别估计一个高概率（因此其他类别预测一个低概率）。通过**交叉熵（cross entropy）**来实现该目标：

> The objective is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). 

$$
J(\pmb{\Theta})=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}\log(\widehat{p}_k^{(i)})
$$

其中$y_k^{(i)}$为第$i$个实例属于类别$k$的目标概率，通常，它等于1或0，取决于该实例是否属于该类别。

如果只有两个类别（$K=2$），则该损失函数等价于Logistic回归的损失函数。

交叉熵经常被用来度量一组估计的类别概率与目标类别的匹配程度。

该损失函数对$\pmb{\theta}^{(k)}$的梯度向量为：
$$
\nabla_{\pmb{\theta}^{(k)}}J(\pmb{\Theta})=\frac{1}{m}\sum_{(i=1)}^m(\widehat{p}_k^{(i)}-y_k^{(i)})\pmb{x}^{(i)}
$$
下面使用Softmax回归将鸢尾花分为三类：

```python
X = iris["data"][:, (2, 3)]
y = iris["target"]

softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10, random_state=42)
softmax_reg.fit(X, y)
```

当在多于两个类别上训练Scikit-Learn的`LogisticRegression`时，它默认使用OvR。但是可以设置超参数`mult_class`为`multinomial`令其使用Softmax回归，此时必须指定一个支持Softmax回归的solver，例如lbfgs solver。它默认也应用ℓ2正则化（通过超参数`C`控制）。

展示决策边界：

```python
x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1, 1),
        np.linspace(0, 3.5, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]


y_proba = softmax_reg.predict_proba(X_new)
y_predict = softmax_reg.predict(X_new)

zz1 = y_proba[:, 1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)

plt.figure(figsize=(10, 4))
plt.plot(X[y==2, 0], X[y==2, 1], "g^", label="Iris virginica")
plt.plot(X[y==1, 0], X[y==1, 1], "bs", label="Iris versicolor")
plt.plot(X[y==0, 0], X[y==0, 1], "yo", label="Iris setosa")

from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
plt.show()
```

预测类别：

```python
softmax_reg.predict([[5, 2]])
```

预测概率：

```python
softmax_reg.predict_proba([[5, 2]])
```

# 支持向量机

**支持向量机（Support Vector Machine，SVM）**是强大且通用（versatile）的机器学习模型，它能够执行线性或非线性分类、回归甚至是离群值检测。它是机器学习中最流行的模型之一。SVM特别适合于复杂的中小规模数据集的分类。

## SVM分类

### 线性SVM分类

#### 硬间距分类

<a name="(支持向量机)(SVM分类)(线性SVM分类)(硬间距分类)(1)">下面</a>展示了iris数据集的一部分。很明显，两个类别能够用一条直线分开（它们**线性可分（linearly separable）**）。左图展示了三个可能的线性分类器的决策边界。用决策边界由虚线表示的模型很糟糕，它甚至不能将两个类别正确（properly）分开。其他两个模型在训练集上的表现完美，但是它们的决策边界太接近实例了，因此这些模型在新实例上的表现可能没有这么好。相反，右图实线表示一个SVM分类器的决策边界。这条线不仅将两个类别分开，而且尽可能远离最近的训练实例。可以认为SVM分类器拟合两个类别间的尽可能宽的街道（由两条平行虚线表示）。这被称为**大间距分类（large margin classification）**。

> You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes.

```python
from sklearn.svm import SVC
from sklearn import datasets

iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = iris["target"]

setosa_or_versicolor = (y == 0) | (y == 1)
X = X[setosa_or_versicolor]
y = y[setosa_or_versicolor]

# SVM Classifier model
svm_clf = SVC(kernel="linear", C=float("inf"))
svm_clf.fit(X, y)
```

```python
import matplotlib as mpl
import numpy as np
import matplotlib.pyplot as plt

# Bad models
x0 = np.linspace(0, 5.5, 200)
pred_1 = 5*x0 - 20
pred_2 = x0 - 1.8
pred_3 = 0.1 * x0 + 0.5

def plot_svc_decision_boundary(svm_clf, xmin, xmax):
    w = svm_clf.coef_[0]
    b = svm_clf.intercept_[0]

    # At the decision boundary, w0*x0 + w1*x1 + b = 0
    # => x1 = -w0/w1 * x0 - b/w1
    x0 = np.linspace(xmin, xmax, 200)
    decision_boundary = -w[0]/w[1] * x0 - b/w[1]

    margin = 1/w[1]
    gutter_up = decision_boundary + margin
    gutter_down = decision_boundary - margin

    svs = svm_clf.support_vectors_
    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')
    plt.plot(x0, decision_boundary, "k-", linewidth=2)
    plt.plot(x0, gutter_up, "k--", linewidth=2)
    plt.plot(x0, gutter_down, "k--", linewidth=2)

fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)

plt.sca(axes[0])
plt.plot(x0, pred_1, "g--", linewidth=2)
plt.plot(x0, pred_2, "m-", linewidth=2)
plt.plot(x0, pred_3, "r-", linewidth=2)
plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", label="Iris versicolor")
plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", label="Iris setosa")
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=14)
plt.axis([0, 5.5, 0, 2])

plt.sca(axes[1])
plot_svc_decision_boundary(svm_clf, 0, 5.5)
plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs")
plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo")
plt.xlabel("Petal length", fontsize=14)
plt.axis([0, 5.5, 0, 2])

plt.show()
```

因为这里严格要求所有实例必须远离街道且位于正确一侧，所以这被称为**硬间距分类（hard margin classification）**

注意，增加更多的“街道外（off the street）”训练实例不会影响决策边界，它们完全由位于街道边缘的实例决定（determined（或“supported”））。这些实例被称为**支持向量（support vectors）**。它们在上图中被圈出。

SVM对特征缩放（feature scales）敏感，<a name="(支持向量机)(SVM分类)(线性SVM分类)(硬间距分类)(2)">如下所示</a>。在左图中，垂直尺度远大于水平尺度，所以最宽的可能街道（the widest possible street）接近水平。特征缩放后（例如，使用Scikit-Learn的`StandardScaler`），右图所示的决策边界看起来好得多。

```python
Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)
ys = np.array([0, 0, 1, 1])
svm_clf = SVC(kernel="linear", C=100)
svm_clf.fit(Xs, ys)

plt.figure(figsize=(9,2.7))
plt.subplot(121)
plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], "bo")
plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], "ms")
plot_svc_decision_boundary(svm_clf, 0, 6)
plt.xlabel("$x_0$", fontsize=20)
plt.ylabel("$x_1$    ", fontsize=20, rotation=0)
plt.title("Unscaled", fontsize=16)
plt.axis([0, 6, 0, 90])

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(Xs)
svm_clf.fit(X_scaled, ys)

plt.subplot(122)
plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], "bo")
plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], "ms")
plot_svc_decision_boundary(svm_clf, -2, 2)
plt.xlabel("$x'_0$", fontsize=20)
plt.ylabel("$x'_1$  ", fontsize=20, rotation=0)
plt.title("Scaled", fontsize=16)
plt.axis([-2, 2, -2, 2])
```

### 软间距分类

硬间距分类有两个问题：第一，仅当数据线性可分时它才能工作；第二，它对离群值非常敏感。下面展示了有一个额外离群值的iris数据集：在左边，无法找到硬间距；在右边，决策边界与[图1](#(支持向量机)(SVM分类)(线性SVM分类)(硬间距分类)(1))看到的没有异常值的决策边界非常不同，它可能无法很好地泛化。

```python
X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])
y_outliers = np.array([0, 0])
Xo1 = np.concatenate([X, X_outliers[:1]], axis=0)
yo1 = np.concatenate([y, y_outliers[:1]], axis=0)
Xo2 = np.concatenate([X, X_outliers[1:]], axis=0)
yo2 = np.concatenate([y, y_outliers[1:]], axis=0)

svm_clf2 = SVC(kernel="linear", C=10**9)
svm_clf2.fit(Xo2, yo2)

fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)

plt.sca(axes[0])
plt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], "bs")
plt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], "yo")
plt.text(0.3, 1.0, "Impossible!", fontsize=24, color="red")
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.annotate("Outlier",
             xy=(X_outliers[0][0], X_outliers[0][1]),
             xytext=(2.5, 1.7),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.1),
             fontsize=16,
            )
plt.axis([0, 5.5, 0, 2])

plt.sca(axes[1])
plt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], "bs")
plt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], "yo")
plot_svc_decision_boundary(svm_clf2, 0, 5.5)
plt.xlabel("Petal length", fontsize=14)
plt.annotate("Outlier",
             xy=(X_outliers[1][0], X_outliers[1][1]),
             xytext=(3.2, 0.08),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.1),
             fontsize=16,
            )
plt.axis([0, 5.5, 0, 2])

plt.show()
```

为了避免这个问题，需要使用更灵活的模型。目标是在保持街道尽可能宽和限制**边界破坏（margin violations）**（最终在街道中间甚至错误一侧的实例）之间找到一个良好的平衡，这被称为**软间距分类（soft margin classification）**。

以下代码加载iris数据集，缩放特征，然后训练一个线性SVM模型（使用`C=1`与hinge损失函数的`LinearSVC`类去检测Iris virginica花。

```python
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC

iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris virginica

svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("linear_svc", LinearSVC(C=1, loss="hinge", random_state=42)),
    ])

svm_clf.fit(X, y)
```

作出预测：

```python
svm_clf.predict([[5.5, 1.7]])
```

与Logistic回归分类器不同，SVM分类器不会为每个类别输出概率。

得到的模型如图左边：

```python
scaler = StandardScaler()
svm_clf1 = LinearSVC(C=1, loss="hinge", random_state=42)
svm_clf2 = LinearSVC(C=100, loss="hinge", random_state=42)

scaled_svm_clf1 = Pipeline([
        ("scaler", scaler),
        ("linear_svc", svm_clf1),
    ])
scaled_svm_clf2 = Pipeline([
        ("scaler", scaler),
        ("linear_svc", svm_clf2),
    ])

scaled_svm_clf1.fit(X, y)
scaled_svm_clf2.fit(X, y)
```

```python
# Convert to unscaled parameters
b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])
b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])
w1 = svm_clf1.coef_[0] / scaler.scale_
w2 = svm_clf2.coef_[0] / scaler.scale_
svm_clf1.intercept_ = np.array([b1])
svm_clf2.intercept_ = np.array([b2])
svm_clf1.coef_ = np.array([w1])
svm_clf2.coef_ = np.array([w2])

# Find support vectors (LinearSVC does not do this automatically)
t = y * 2 - 1
support_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()
support_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()
svm_clf1.support_vectors_ = X[support_vectors_idx1]
svm_clf2.support_vectors_ = X[support_vectors_idx2]
```

```python
fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)

plt.sca(axes[0])
plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^", label="Iris virginica")
plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs", label="Iris versicolor")
plot_svc_decision_boundary(svm_clf1, 4, 5.9)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=14)
plt.title("$C = {}$".format(svm_clf1.C), fontsize=16)
plt.axis([4, 5.9, 0.8, 2.8])

plt.sca(axes[1])
plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")
plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")
plot_svc_decision_boundary(svm_clf2, 4, 5.99)
plt.xlabel("Petal length", fontsize=14)
plt.title("$C = {}$".format(svm_clf2.C), fontsize=16)
plt.axis([4, 5.9, 0.8, 2.8])
```

当使用Scikit-Learn创建一个SVM模型时，可以指定一些超参数。`C`是其中一个超参数，如果设置它为一个小值，则得到上图左边的模型；如果设置它为一个大值，则得到上图右边的模型。如果SVM模型过拟合，则可以通过减小`C`来正则化它。边界破坏很不好，通常最好少一些超参数。然而，在该情况中，左边的模型有很多边界破坏，但是可能泛化得更好。

除了使用`LinearSVC`类外，还可以使用带有一个线性核的`SVC`。当创建该SVC模型时，可以写成`SVC(kernel="linear", C=1)`。也可以使用`SGDClassifier`类：`SGDClassifier(loss="hinge", alpha=1/(m*C))`，这会应用普通的随机梯度下降去训练一个线性SVM分类器。它的收敛速度不如`LinearSVC`类，但它对处理在线分类任务或不能容纳进内存的大型数据集（核心外训练（out-of-core training））很有用。

`LinearSVC`类会正则化偏置项，所以应该首先通过减去它的均值来居中数据集。如果使用`StandardScaler`缩放数据，这会自动实现。还要确保超参数`loss`被设置为`"hinge"`，因为它不是默认值。最后，为了得到更好的性能，应该设置超参数`dual`为`False`，除非特征数量比训练实例数量更多。

## 非线性SVM分类

虽然线性SVM分类器高效且在很多情况下工作得非常好，但是许多数据集甚至不是近似线性可分的。一种处理非线性数据集的方法是添加更多的特征，例如多项式特征。某些情况下这可以导致线性可分的数据集。例如，下面左图表示一个简单的数据集，它只有一个特征$x_1$。该数据集不是线性可分的。但是如果添加第二个特征$x_2=(x_1)^2$后，得到的2维数据集完全线性可分。

> Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable.

```python
X1D = np.linspace(-4, 4, 9).reshape(-1, 1)
X2D = np.c_[X1D, X1D**2]
y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])

plt.figure(figsize=(10, 3))

plt.subplot(121)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.plot(X1D[:, 0][y==0], np.zeros(4), "bs")
plt.plot(X1D[:, 0][y==1], np.zeros(5), "g^")
plt.gca().get_yaxis().set_ticks([])
plt.xlabel(r"$x_1$", fontsize=20)
plt.axis([-4.5, 4.5, -0.2, 0.2])

plt.subplot(122)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.axvline(x=0, color='k')
plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], "bs")
plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], "g^")
plt.xlabel(r"$x_1$", fontsize=20)
plt.ylabel(r"$x_2$  ", fontsize=20, rotation=0)
plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])
plt.plot([-4.5, 4.5], [6.5, 6.5], "r--", linewidth=3)
plt.axis([-4.5, 4.5, -1, 17])

plt.subplots_adjust(right=1)

plt.show()
```

为了使用Scikit-Learn实现该思想，创建一个`Pipeline`，它包含一个`PolynomialFeatures`转换器，并跟随一个`StandardScaler`与一个`LinearSVC`。下面在moons数据集上进行测试。moons数据集是一个用于二分类（binary classification）的玩具数据集（toy dataset），其中数据点的形状为两个交错的半圆，可以使用`make_moons`函数生成该数据集。

> Let’s test this on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles (see Figure 5-6).

```python
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

def plot_dataset(X, y, axes):
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")
    plt.axis(axes)
    plt.grid(True, which='both')
    plt.xlabel(r"$x_1$", fontsize=20)
    plt.ylabel(r"$x_2$", fontsize=20, rotation=0)

plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.show()
```

```python
from sklearn.datasets import make_moons
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

polynomial_svm_clf = Pipeline([
        ("poly_features", PolynomialFeatures(degree=3)),
        ("scaler", StandardScaler()),
        ("svm_clf", LinearSVC(C=10, loss="hinge", random_state=42))
    ])

polynomial_svm_clf.fit(X, y)
```

```python
def plot_predictions(clf, axes):
    x0s = np.linspace(axes[0], axes[1], 100)
    x1s = np.linspace(axes[2], axes[3], 100)
    x0, x1 = np.meshgrid(x0s, x1s)
    X = np.c_[x0.ravel(), x1.ravel()]
    y_pred = clf.predict(X).reshape(x0.shape)
    y_decision = clf.decision_function(X).reshape(x0.shape)
    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)
    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)

plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])

plt.show()
```

### 多项式核

添加多项式特征实现起来很简单，并且可以用于各种（all sorts of）机器学习算法（不只是SVM）。在低多项式次数下，该方法无法处理非常复杂的数据集；而在高多项式次数下，它创建了大量的特征，导致模型十分慢。

幸运的是，当使用SVM时，可以应用被称为**核技巧（kernel trick）**的数学技术。核技巧使得在不实际添加多项式特征的情况下，得到与添加许多多项式特征（即使是非常高次的多项式）后相同的结果。因为没有实际添加任何特征，所以也就不会有特征数量的组合爆炸。该技巧由`SVC`类实现，下面在moons数据集上测试它：

```python
from sklearn.svm import SVC

poly_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))
    ])
poly_kernel_svm_clf.fit(X, y)
```

```python
poly100_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("svm_clf", SVC(kernel="poly", degree=10, coef0=100, C=5))
    ])
poly100_kernel_svm_clf.fit(X, y)
```

```python
fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)

plt.sca(axes[0])
plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])
plt.title(r"$d=3, r=1, C=5$", fontsize=18)

plt.sca(axes[1])
plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])
plt.title(r"$d=10, r=100, C=5$", fontsize=18)
plt.ylabel("")

plt.show()
```

以上代码分别使用3次多项式核、10次多项式核训练了一个SVM分类器，并分别在左、右图中展示。显然，如果模型过拟合，则可能需要降低多项式次数；反之，如果模型欠拟合，则可以尝试增加多项式次数。超参数`coef0`控制相对于低次多项式，模型受高次多项式影响的程度。

### 相似特征

另一种处理非线性问题的技术是添加使用**相似函数（similarity function）**计算得到的特征。相似函数度量每个实例与特定**地标（landmark）**的相似程度。例如，下面以前面讨论的1维数据集为例，并在$x_1=-2$与$x_1=1$处增加了两个地标（见左图），然后定义相似函数为$\gamma=0.3$的高斯**径向基函数（Radial Basis Function，RBF）**：
$$
\phi_\gamma(\pmb{x}, \ell)=\exp(-\gamma||\pmb{x}-\ell||)
$$


> Another technique to tackle nonlinear problems is to add features computed using a *similarity function*, which measures how much each instance resembles a particular *landmark*. 

```python
def gaussian_rbf(x, landmark, gamma):
    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)

gamma = 0.3

x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)
x2s = gaussian_rbf(x1s, -2, gamma)
x3s = gaussian_rbf(x1s, 1, gamma)

XK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]
yk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])

plt.figure(figsize=(10.5, 4))

plt.subplot(121)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c="red")
plt.plot(X1D[:, 0][yk==0], np.zeros(4), "bs")
plt.plot(X1D[:, 0][yk==1], np.zeros(5), "g^")
plt.plot(x1s, x2s, "g--")
plt.plot(x1s, x3s, "b:")
plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])
plt.xlabel(r"$x_1$", fontsize=20)
plt.ylabel(r"Similarity", fontsize=14)
plt.annotate(r'$\mathbf{x}$',
             xy=(X1D[3, 0], 0),
             xytext=(-0.5, 0.20),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.1),
             fontsize=18,
            )
plt.text(-2, 0.9, "$x_2$", ha="center", fontsize=20)
plt.text(1, 0.9, "$x_3$", ha="center", fontsize=20)
plt.axis([-4.5, 4.5, -0.1, 1.1])

plt.subplot(122)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.axvline(x=0, color='k')
plt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], "bs")
plt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], "g^")
plt.xlabel(r"$x_2$", fontsize=20)
plt.ylabel(r"$x_3$  ", fontsize=20, rotation=0)
plt.annotate(r'$\phi\left(\mathbf{x}\right)$',
             xy=(XK[3, 0], XK[3, 1]),
             xytext=(0.65, 0.50),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.1),
             fontsize=18,
            )
plt.plot([-0.1, 1.1], [0.57, -0.1], "r--", linewidth=3)
plt.axis([-0.1, 1.1, -0.1, 1.1])
    
plt.subplots_adjust(right=1)

plt.show()
```

这是一个钟形函数，取值从0（离地标非常远）到1（在地标处）（例如，下面代码计算$x_1=-1$的实例的新特征）。右图显示的是转换后的数据集（丢弃原来的特征）。可以看到，现在它线性可分。

```python
x1_example = X1D[3, 0]
for landmark in (-2, 1):
    k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)
    print("Phi({}, {}) = {}".format(x1_example, landmark, k))
```

一个问题是如何选择地标。最简单的方法是在数据集中每个实例的位置创建一个地标。这样将创建很多维度，因此增加转换后的训练集线性可分的可能性。这样做的缺点在于一个有$m$个实例、$n$个特征的训练集会被转换为一个有$m$个实例、$m$个特征的训练集（假设丢弃原来的特征）。如果训练集很大，则特征数量同样会很多。

> If your training set is very large, you end up with an equally large number of features.

#### 高斯RBF核

就像多项式特征方法一样，相似特征方法同样可以用于任意机器学习算法，但是计算所有额外特征的计算成本可能很大，尤其在大型训练集上。同样地，核技巧使得在不添加任何特征的情况下，可以获得与添加许多特征后相似的结果。下面使用`SVC`类，并指定高斯RBF核：

```python
rbf_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))
    ])
rbf_kernel_svm_clf.fit(X, y)
```

下面绘制出该模型，如图左下角。其他图展示使用不同的超参数`gamma`（$\gamma$）与`C`训练得到的模型。增加`gamma`使得钟形曲线更窄（narrower）（见左边的两个图）。因此，每个实例的影响范围较小：决策边界最终变得更加不规整（irregular），在单个实例周围扭动。反之，一个小的`gamma`值使得钟形曲线更宽：实例的影响范围更大，决策边界最终更平滑。所以，$\gamma$充当一个正则化超参数：如果模型过拟合，则应该减小它；如果模型欠拟合，则应该增加它（与超参数`C`类似）。

```python
from sklearn.svm import SVC

gamma1, gamma2 = 0.1, 5
C1, C2 = 0.001, 1000
hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)

svm_clfs = []
for gamma, C in hyperparams:
    rbf_kernel_svm_clf = Pipeline([
            ("scaler", StandardScaler()),
            ("svm_clf", SVC(kernel="rbf", gamma=gamma, C=C))
        ])
    rbf_kernel_svm_clf.fit(X, y)
    svm_clfs.append(rbf_kernel_svm_clf)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)

for i, svm_clf in enumerate(svm_clfs):
    plt.sca(axes[i // 2, i % 2])
    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])
    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])
    gamma, C = hyperparams[i]
    plt.title(r"$\gamma = {}, C = {}$".format(gamma, C), fontsize=16)
    if i in (0, 1):
        plt.xlabel("")
    if i in (1, 3):
        plt.ylabel("")

plt.show()
```

其他核存在，但很少被使用。一些核专门用于特定的数据结构。当对文本文档或DNA序列进行分类时，有时会使用**字符串核（string kernels）**（例如，使用**字符串子串（ string subsequence kernel）**核或基于**编辑距离（Levenshtein distance）**的核）。

> *String kernels* are sometimes used when classifying text documents or DNA sequences (e.g., using the *string subsequence kernel* or kernels based on the *Levenshtein distance*).

有很多核可以选择，一个问题是如何决定使用哪个核。根据经验，应该总是首先尝试线性核（注意`LinearSVC`比`SVC(kernel="linear"`快得多），尤其当训练集很大或它有很多特征时。如果训练集不是特别大，则同时应该尝试高斯RBF核，它在大多数情况下工作得很好。如果还有空闲时间与计算资源，则可以使用交叉验证与网格搜索，尝试一些其他的核，尤其当有专门用于训练集数据结构的内核时。

### 计算复杂度

`LinearSVC`类基于`liblinear`库，该库实现了线性SVM的[一个优化算法](https://homl.info/13)。它不支持核技巧，但它几乎与训练实例数量与特征数量成线性关系。它的训练时间复杂度大致为$O(m\times n)$。

如果要非常高的精度，该算法会花费更长的时间。这由公差超参数（tolerance  hyperparameter）$\epsilon$（在Scikit-Learn中被称为`tol`）控制。在大多数分类任务中，默认公差可以接受。

`SVC`类基于`libsvm`库，该库实现了一个支持核技巧的[算法](https://homl.info/14)。它的训练时间复杂度通常介于$O(m^2\times n)$与$O(m^3\times n)$之间，这意味着当训练实例很多时（例如，数十万个实例），它会极慢。这个算法适用于中小型训练集。它对特征数量的可扩展性很好，尤其对稀疏特征（即当实例的非零特征很少时）。在这种情况下，该算法随着每个实例非零特征的平均数量而扩展。下表比较了Scikit-Learn的SVM分类类（classes）：

| 类            | 时间复杂度                         | 核心外支持 | 需要缩放 | 核技巧 |
| ------------- | ---------------------------------- | ---------- | -------- | ------ |
| LinearSVC     | $O(m\times n)$                     | 否         | 是       | 否     |
| SGDClassifier | $O(m\times n)$                     | 是         | 是       | 否     |
| SVC           | $O(m^2\times n)$\~$O(m^3\times n)$ | 否         | 是       | 是     |

## SVM回归

SVM算法同样支持线性与非线性回归。使用SVM进行回归而非分类的技巧在于反转目标：SVM回归尝试让尽可能多的实例在街道中，同时限制边界破坏（即在街道外的实例），而不是尝试拟合两个类别间的尽可能宽的街道同时限制边界破坏。以下代码（使用Scikit-Learn的`LinearSVR`类执行线性SVM回归）展示了在某些随机线性数据上训练的两个线性SVM回归模型，其中一个使用大的间距（margin），一个使用了小的间距。

> To use SVMs for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible *on* the street while limiting margin violations (i.e., instances *off* the street).

```python
np.random.seed(42)
m = 50
X = 2 * np.random.rand(m, 1)
y = (4 + 3 * X + np.random.randn(m, 1)).ravel()
```

```python
# LinearSVM的使用演示。

from sklearn.svm import LinearSVR

svm_reg = LinearSVR(epsilon=1.5, random_state=42)
svm_reg.fit(X, y)
```

```python
svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)
svm_reg2 = LinearSVR(epsilon=0.5, random_state=42)
svm_reg1.fit(X, y)
svm_reg2.fit(X, y)

def find_support_vectors(svm_reg, X, y):
    y_pred = svm_reg.predict(X)
    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)
    return np.argwhere(off_margin)

svm_reg1.support_ = find_support_vectors(svm_reg1, X, y)
svm_reg2.support_ = find_support_vectors(svm_reg2, X, y)

eps_x1 = 1
eps_y_pred = svm_reg1.predict([[eps_x1]])
```

```python
def plot_svm_regression(svm_reg, X, y, axes):
    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)
    y_pred = svm_reg.predict(x1s)
    plt.plot(x1s, y_pred, "k-", linewidth=2, label=r"$\hat{y}$")
    plt.plot(x1s, y_pred + svm_reg.epsilon, "k--")
    plt.plot(x1s, y_pred - svm_reg.epsilon, "k--")
    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')
    plt.plot(X, y, "bo")
    plt.xlabel(r"$x_1$", fontsize=18)
    plt.legend(loc="upper left", fontsize=18)
    plt.axis(axes)

fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)
plt.sca(axes[0])
plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])
plt.title(r"$\epsilon = {}$".format(svm_reg1.epsilon), fontsize=18)
plt.ylabel(r"$y$", fontsize=18, rotation=0)
#plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], "k-", linewidth=2)
plt.annotate(
        '', xy=(eps_x1, eps_y_pred), xycoords='data',
        xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),
        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}
    )
plt.text(0.91, 5.6, r"$\epsilon$", fontsize=20)
plt.sca(axes[1])
plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])
plt.title(r"$\epsilon = {}$".format(svm_reg2.epsilon), fontsize=18)
plt.show()
```

增加更多的位于间距内的实例不会影响模型的预测，因此，该模型被称为是**$\epsilon$-不敏感的（$\epsilon$-insensitive）**。

为了处理非线性回归任务，可以使用核SVM模型（kernelized SVM model）。以下代码（使用Scikit-Learn的`SVR`类，它支持核技巧）展示了使用二次多项式核，在一个随机二次训练集上的SVM回归。左图几乎没有正则化（即`C`值很大），右图使用了多得多的正则化（即`C`值很小）。

```python
np.random.seed(42)
m = 100
X = 2 * np.random.rand(m, 1) - 1
y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()
```

```python
# SVR的使用演示。

from sklearn.svm import SVR

svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1, gamma="scale")
svm_poly_reg.fit(X, y)
```

```python
from sklearn.svm import SVR

svm_poly_reg1 = SVR(kernel="poly", degree=2, C=100, epsilon=0.1, gamma="scale")
svm_poly_reg2 = SVR(kernel="poly", degree=2, C=0.01, epsilon=0.1, gamma="scale")
svm_poly_reg1.fit(X, y)
svm_poly_reg2.fit(X, y)
```

```python
fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)
plt.sca(axes[0])
plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])
plt.title(r"$degree={}, C={}, \epsilon = {}$".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)
plt.ylabel(r"$y$", fontsize=18, rotation=0)
plt.sca(axes[1])
plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])
plt.title(r"$degree={}, C={}, \epsilon = {}$".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18)
plt.show()
```

`SVR`类是`SVC`类的回归等价版本，`LinearSVR`类是`LinearSVC`类的回归等价版本。`LinearSVR`扩展性与训练集大小呈线性关系（就像`LinearSVC`类一样），而当训练集变大时，`SVR`类会变得很慢（就像`SVC`类一样）。

> The `SVR` class is the regression equivalent of the `SVC` class, and the `LinearSVR` class is the regression equivalent of the `LinearSVC` class. The `LinearSVR` class scales linearly with the size of the training set (just like the `LinearSVC` class), while the `SVR` class gets much too slow when the training set grows large (just like the `SVC` class).

除了分类与回归，SVM还可以用于离群值检测等。

### SVM求解

线性SVM分类器模型通过计算一个决策函数$\pmb{w}^T\pmb{x}+b=w_1x_1+...+w_nx_n+b$（注意这里$\pmb{w}$为特征权重向量，不包含偏置项$b$，偏置项不会被加到输入特征向量中）来预测新实例$\pmb{x}$的类别。如果结果为非负数，则预测类别（predicted class）$\widehat{y}$为正类（1），否则它为负类（0）：
$$
\left\{
\begin{aligned}
0\ if\ \pmb{w}^T\pmb{x}+b<0\\
1\ if\ \pmb{w}^T\pmb{x}+b\ge0
\end{aligned}
\right.
$$
以下代码展示了[图2](#(支持向量机)(SVM分类)(线性SVM分类)(硬间距分类)(2))左边模型对应的决策函数。它是一个2维平面，因为数据集有两个特征。该决策边界是使得决策函数等于0的点集：它是两个平面的交，是一条直线（用粗实线表示）。更一般地，当有$n$个特征时，决策函数是一个$n$维的超平面，决策边界是一个$n-1$维超平面。

```python
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris virginica
```

```python
from mpl_toolkits.mplot3d import Axes3D

def plot_3D_decision_function(ax, w, b, x1_lim=[4, 6], x2_lim=[0.8, 2.8]):
    x1_in_bounds = (X[:, 0] > x1_lim[0]) & (X[:, 0] < x1_lim[1])
    X_crop = X[x1_in_bounds]
    y_crop = y[x1_in_bounds]
    x1s = np.linspace(x1_lim[0], x1_lim[1], 20)
    x2s = np.linspace(x2_lim[0], x2_lim[1], 20)
    x1, x2 = np.meshgrid(x1s, x2s)
    xs = np.c_[x1.ravel(), x2.ravel()]
    df = (xs.dot(w) + b).reshape(x1.shape)
    m = 1 / np.linalg.norm(w)
    boundary_x2s = -x1s*(w[0]/w[1])-b/w[1]
    margin_x2s_1 = -x1s*(w[0]/w[1])-(b-1)/w[1]
    margin_x2s_2 = -x1s*(w[0]/w[1])-(b+1)/w[1]
    ax.plot_surface(x1s, x2, np.zeros_like(x1),
                    color="b", alpha=0.2, cstride=100, rstride=100)
    ax.plot(x1s, boundary_x2s, 0, "k-", linewidth=2, label=r"$h=0$")
    ax.plot(x1s, margin_x2s_1, 0, "k--", linewidth=2, label=r"$h=\pm 1$")
    ax.plot(x1s, margin_x2s_2, 0, "k--", linewidth=2)
    ax.plot(X_crop[:, 0][y_crop==1], X_crop[:, 1][y_crop==1], 0, "g^")
    ax.plot_wireframe(x1, x2, df, alpha=0.3, color="k")
    ax.plot(X_crop[:, 0][y_crop==0], X_crop[:, 1][y_crop==0], 0, "bs")
    ax.axis(x1_lim + x2_lim)
    ax.text(4.5, 2.5, 3.8, "Decision function $h$", fontsize=16)
    ax.set_xlabel(r"Petal length", fontsize=16, labelpad=10)
    ax.set_ylabel(r"Petal width", fontsize=16, labelpad=10)
    ax.set_zlabel(r"$h = \mathbf{w}^T \mathbf{x} + b$", fontsize=18, labelpad=5)
    ax.legend(loc="upper left", fontsize=16)

fig = plt.figure(figsize=(11, 6))
ax1 = fig.add_subplot(111, projection='3d')
plot_3D_decision_function(ax1, w=svm_clf2.coef_[0], b=svm_clf2.intercept_[0])

plt.show()
```

![iris数据集的决策函数](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\iris数据集的决策函数.png)

虚线表示决策函数等于0或-1的点集：它们平行且与决策边界的距离相等，并在决策边界周围形成间距。训练一个线性SVM分类器意味着找到$\pmb{w}$与$b$，使得该间距（margin）尽可能宽，同时避免边界破坏（硬间距）或限制边界破坏（软间距）。

考虑决策函数的斜率（slope），它等于权重向量$||\pmb{w|}|$的范数（norm）。如果将该斜率除以2，则使得决策函数等于$\pm{1}$的点集与决策边界的距离将乘以2。换言之，将斜率除以2会使得间距乘以2。如以下代码所示。权重向量$\pmb{w}$越小，间距越大。

```python
def plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]):
    x1 = np.linspace(x1_lim[0], x1_lim[1], 200)
    y = w * x1 + b
    m = 1 / w

    plt.plot(x1, y)
    plt.plot(x1_lim, [1, 1], "k:")
    plt.plot(x1_lim, [-1, -1], "k:")
    plt.axhline(y=0, color='k')
    plt.axvline(x=0, color='k')
    plt.plot([m, m], [0, 1], "k--")
    plt.plot([-m, -m], [0, -1], "k--")
    plt.plot([-m, m], [0, 0], "k-o", linewidth=3)
    plt.axis(x1_lim + [-2, 2])
    plt.xlabel(r"$x_1$", fontsize=16)
    if ylabel:
        plt.ylabel(r"$w_1 x_1$  ", rotation=0, fontsize=16)
    plt.title(r"$w_1 = {}$".format(w), fontsize=16)

fig, axes = plt.subplots(ncols=2, figsize=(9, 3.2), sharey=True)
plt.sca(axes[0])
plot_2D_decision_function(1, 0)
plt.sca(axes[1])
plot_2D_decision_function(0.5, 0, ylabel=False)
plt.show()
```

所以目标是最小化$||w||$去获得大间距。如果同时要避免任何边界破坏（硬间距），则需要对所有的正训练实例，决策函数大于等于1；对所有的负训练实例，决策函数小于等于-1。定义：对于负实例（$y^{(i)}=0$），$t^{(i)}=-1$；对于正实例（$y^{(i)}=1$），$t^{(i)}=1$，则得到硬间距线性SVM分类器的目标为：
$$
\underset{\pmb{w},b}{minimize}\ \frac{1}{2}\pmb{w}^T\pmb{w}
$$
使得：
$$
t^{(i)}(\pmb{w}^T\pmb{x}^{(i)}+b)\ge1\ for\ i=1,2,...m
$$
注意，这里最小化$1/2\ \pmb{w}^T\pmb{w}$，即$1/2||\pmb{w}||^2$，而不是最小化$||\pmb{w}||$。$1/2||\pmb{w}||^2$的导数为$\pmb{w}$，而$||\pmb{w}||$在$\pmb{w}=0$处不可微。优化算法在可微函数上工作得好得多。

为了软间距分类的目标，需要为每个实例引入一个**松弛变量（slack variable）**$\zeta^{(i)}\ge0$。$\zeta^{(i)}$度量第$i$个实例允许破坏边界的程度。现在有两个冲突的目标：一个是要使得松弛变量尽可能小以减少边界破坏，一个是使用$1/2\ \pmb{w}^T\pmb{w}$尽可能小以增加间距。这是超参数`C`定义了两个目标的折中。这就得到了约束优化问题：

$$
\underset{\pmb{w},b,\zeta}{minimize}\ \frac{1}{2}\pmb{w}^T\pmb{w}+C\sum_{i=1}^{m}\zeta^{(i)}
$$

使得：
$$
t^{(i)}(\pmb{w}^T\pmb{x}^{(i)}+b)\ge1-\zeta^{(i)}\ and\ \zeta^{(i)}\ge0\ for\ i=1,2,...,m
$$

### 二次规划

硬间距与软件局问题都是具有线性约束的凸二次优化问题。这样的问题被称为**二次规划（Quadratic Programming，QP）**问题。许多现成的解决方案（solvers）可以解决QP问题。

> To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vandenberghe’s book *Convex Optimization* (Cambridge University Press, 2004) or watch Richard Brown’s series of video lectures.

二次规划问题的通用形式如下：

$$
\underset{\pmb{p}}{minimize}\frac{1}{2}\pmb{p}^T\pmb{H}\pmb{p}+\pmb{f}^T\pmb{p}
$$

使得：
$$
\pmb{A}\pmb{p}\le\pmb{b}
$$
其中：

- $\pmb{p}$为一个$n_p$维向量（$n_p$等于参数数量）。
- $\pmb{H}$为一个$n_p\times n_p$矩阵。
- $\pmb{f}$为一个$n_p$维向量。
- $\pmb{A}$为$n_c\times n_p$矩阵（$n_c$等于约束数量）。
- $\pmb{b}$为一个$n_c$维向量。

注意$\pmb{A}\pmb{p}\le\pmb{b}$定义了$n_c$个约束：$\pmb{p}^T\pmb{a}^{(i)}\le b^{i},i=1,2,...n_c$，其中$\pmb{a}^{(i)}$为包含$\pmb{A}$的第$i$行的元素的向量，$b^{(i)}$为$\pmb{b}$的第$i$个元素。

按照如下方式设置QP参数，则得到硬间距线性SVM分类器目标：

- $n_p=n+1$，其中$n$为特征数量（$+1$用于偏置项）。
- $n_c=m$，其中$m$为训练实例数量。
- $\pmb{H}$为$n_p\times n_p$单位矩阵，除了左上角的单元格中的值为零（以忽略偏置项）。
- $\pmb{f}=0$，为一个$n_p$维的向量，元素值全部为0。
- $\pmb{b}=-1$，为一个$n_c$维向量，元素值全部为-1。
- $\pmb{a}^{(i)}=-t^{(i)}\pmb{\dot{x}}^{(i)}$，其中$\pmb{\dot{x}}^{(i)}$等于$\pmb{x}^{(i)}$加上一个额外的偏置特征$\pmb{\dot{x}}_0=1$。

训练一个硬间距线性SVM分类器的一个方法是使用现成的QP求解器（solver）并将以上参数传递给它。得到的向量$\pmb{p}$包含偏置项$b=p_0$与特征权重$w_i=p_i,i=1,2,...n$。类似地，也可以使用一个QP求解器去解决软间距问题。

### 对偶问题

给定一个约束优化问题，称为**原问题（primal problem）**，可以表达出一个不同的但是密切相关的问题，它被称为**对偶问题（dual problem）**。对偶问题的解通常给出了原始问题解的下界（a lower bound），但是在某些条件下它与原始问题有相同的解。SVM问题恰好满足这些条件（目标函数为凸，不等式约束为连续可微的凸函数），所以可以选择去求解原始问题或对偶问题，两者有相同的解。以下<a name="(支持向量机)(二次规划)(对偶问题)(1)">公式</a>给出了线性SVM目标的对偶形式：

$$
\underset{\alpha}{minimize}\ \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha^{(i)}\alpha^{(j)}t^{(i)}t^{(j)}{\pmb{x}^{(i)}}^T\pmb{x}^{(j)}-\sum_{i=1}^m\alpha^{(i)}
$$

使得：
$$
\alpha^{(i)}\ge0\ for\ i=1,2,...,m
$$
一旦（使用QP求解器）找到最小化该公式的$\widehat{\pmb{\alpha}}$，则使用<a name="(支持向量机)(SVM求解)(对偶问题)(2)">以下公式</a>计算最小化原始问题的$\widehat{\pmb{w}}$与$\widehat{b}$：
$$
\widehat{\pmb{w}}=\sum_{i=1}^m\widehat{\alpha}^{(i)}t^{(i)}\pmb{x}^{(i)}\\
\widehat{b}=\frac{1}{n_s}\underset{\widehat{\alpha}^{(i)}\ge0}{\sum_{i=1}^m}(t^{(i)}-\widehat{\pmb{w}}^T\pmb{x}^{(i)})
$$

当训练实例的数量小于特征数量时，对偶问题比原始问题求解起来更快。更重要的是，对偶问题使得核技巧成为可能，而原始问题不行。

> So what is this kernel trick, anyway?

为了理解**对偶性（duality）**，首先需要理解**拉格朗日乘子（Lagrange multipliers）**法。它的基本思想是通过将约束条件转换为目标函数，将约束优化目标转换为无约束优化目标。假设想要找到在**等式约束（equality constraint）**$3x+2y+1=0$下，最小化函数$f(x,y)=x^2+2y$的$x$与$y$值。则首先定义一个新的被称为**拉格朗日（Lagrangian）**（或**拉格朗日函数（Lagrange function）**）的函数：$g(x,y,a)=f(x,y)-\alpha(3x+2y+1)$。每个约束（在该例中只有一个约束）都从原始目标中减去，并乘以一个新的变量，称为拉格朗日乘子（Lagrange multiplier）。

> The general idea is to transform a constrained optimization objective into an unconstrained one, by moving the constraints into the objective function.

Joseph-Louis Lagrange证明了如果$(\widehat{x},\widehat{y})$为约束优化问题的一个解，则必然存在$\alpha$，使得$(\widehat{x},\widehat{y},\widehat{\alpha})$为拉格朗日的一个**驻点（stationary point）**。换言之，可以计算$g(x,y,a)$关于$x$、$y$与$\alpha$的偏导，然后找到所有导数为0的点，约束优化问题的解（如果存在的话）必然在这些驻点中间。

在这个例子中，偏导为：$\left\{
\begin{aligned}
&\frac{\partial}{\partial{x}}g(x,y,\alpha)=2x-3\alpha\\
&\frac{\partial}{\partial{y}}g(x,y,\alpha)=2-2\alpha\\
&\frac{\partial}{\partial\alpha}g(x,y,\alpha)=-3x-2y-1
\end{aligned}
\right.$

当所有这些偏导都等于0，得到$2\widehat{x}-3\widehat{\alpha}=2-2\widehat{\alpha}=-3\widehat{x}-2\widehat{y}-1=0$，从而得到$\widehat{x}=\frac{3}{2}$，$\widehat{y}=-\frac{11}{4}$，以及$\widehat{\alpha}=1$。这是唯一的驻点，因为它服从约束，因此它必然是该约束优化问题的解。

然而该方法只适用于等式约束。幸运的是，在某些条件下（SVM目标服从这些条件），该方法可以被泛化到**不等式约束（inequality constraints）**（例如$3x+2y+1\ge0$。硬间距问题的**泛化的拉格朗日（generalized Lagrangian）**如下，其中$\alpha^{(i)}$变量被称为**Karush–Kuhn–Tucker（KKT）**乘子（multipliers），它们必须大于等于0：

$$
ℒ(\pmb{w},b,a)=\frac{1}{2}\pmb{w}^T\pmb{w}-\sum_{i=1}^m\alpha^{(i)}(t^{(i)}(\pmb{w}^T\pmb{x}^{(i)}+b)-1)\\
with\ \alpha^{(i)}\ge0\ for\ i=1,2,...,m
$$

与拉格朗日乘子法一样，可以计算偏导并定位驻点。如果有解，则它必然位于满足**KKT条件（KKT conditions）**的驻点$(\widehat{\pmb{w}},\widehat{b},\widehat{a})$中间，KKT条件如下：

- 服从问题约束：对于$i=1,2,...m$，$t^{(i)}(\widehat{\pmb{w}}^T\pmb{x}^{(i)}+\widehat{b})\ge1$。
- 对于$i=1,2,...m$，$\widehat{\alpha}^{(i)}\ge0$。
- 要么$\widehat{\alpha}^{(i)}=0$，要么第$i$个约束是一个**起作用约束（active constraint）**，意味着它必须满足：$t^{(i)}(\widehat{\pmb{w}}^T\pmb{x}^{(i)}+\widehat{b})=1$。这个条件被称为**互补松弛（complementary slackness）**条件。它表明要么$\widehat{\alpha}^{(i)}=0$，要么第$i$个实例位于边界上（是一个支持向量）。

> Either *α* *i* = 0 or the *i* th constraint must be an *active constraint*,

注意KKT条件为驻点为约束优化问题的解的必要条件。在某些条件下，它们也是充分条件。幸运的是，SVM优化问题恰好满足这些条件，因为任何满足KKT条件的驻点一定是约束优化问题的解。

可以通过下式计算泛化的拉格朗日关于$\pmb{w}$与$b$的偏导：
$$
\nabla_{\pmb{w}}ℒ(\pmb{w},b,a)=\pmb{w}-\sum_{i=1}^m\alpha^{(i)}t^{(i)}\pmb{x}^{(i)}\\
\frac{\partial}{\partial{b}}ℒ(\pmb{w},b,a)=-\sum_{i=1}^m\alpha^{(i)}t^{(i)}
$$
当所有这些偏导都等于0，得到<a name="(支持向量机)(SVM求解)(对偶问题)(3)">下式</a>：
$$
\widehat{\pmb{w}}=\sum_{i=1}^m\widehat{\alpha}^{(i)}t^{(i)}\pmb{x}^{(i)}\\
\sum_{i=1}^m\widehat{\alpha}^{(i)}t^{(i)}=0
$$
如果将这些结果代入泛化的拉格朗日的定义中，得到：
$$
ℒ(\widehat{\pmb{w}},\widehat{b},\alpha)=\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha^{(i)}\alpha^{(j)}t^{(i)}t^{(j)}{\pmb{x}^{(i)}}^T\pmb{x}^{(j)}-\sum_{i=1}^m\alpha^{(i)}\\
with\ \alpha^{(i)}\ge0\ for\ i=1,2,...m
$$
目标是找到向量$\widehat{\pmb{\alpha}}$最小化该函数，且要求所有实例$\widehat{\alpha}\ge0$。该目标优化问题为我们寻找的对偶问题。

一旦找到最优的$\widehat{\pmb{\alpha}}$，则可以使用[式3](#(支持向量机)(SVM求解)(对偶问题)(3))的第一行计算$\widehat{\pmb{w}}$。为了计算$\widehat{b}$，可以利用支持向量必须满足$t^{(i)}(\widehat{\pmb{w}}^T\pmb{x}^{(i)}+b)=1$的事实，因此如果第$k$个实例为一个支持向量（即$\widehat{\alpha}^{(k)}>0$），则可以使用它去计算$\widehat{b}=t^{(k)}-\widehat{\pmb{w}}^T\pmb{x}^{(k)}$。然而，在所有支持向量上计算平均值常常要更好，以获得更稳定、更精确的值：
$$
\widehat{b}=\frac{1}{n_s}\underset{\widehat{\alpha}^{(i)}>0}{\sum_{i=1}^m}[t^{(i)}-\widehat{\pmb{w}}^T\pmb{x}^{(i)}]
$$


### 核SVMs（Kernelized SVMs）

假设要对二维训练集（例如moons训练集）应用一个二次多项式变换，然后在变换后的训练集上训练一个线性SVM分类器。下式显示应用的二次多项式映射函数$\phi$：

$$
\phi(\pmb{x})=\phi(\left(
\begin{matrix}
x_1\\
x_2
\end{matrix}
\right))=\left(
\begin{matrix}
x_1^2\\
\sqrt{2}x_1x_2\\
x_2^2
\end{matrix}
\right)
$$

注意变换后的矩阵为3维而不是2维。<a name="(支持向量机)(二次规划)(核SVMs)(1)">下面</a>查看如果应用该二次多项式映射然后计算变换后的向量的点积后$\pmb{a}$与$\pmb{b}$会发生什么：

$$
\phi(\pmb{a})^T\phi(\pmb{b})=\left(
\begin{matrix}
a_1^2\\
\sqrt{2}a_1a_2\\
a_2^2
\end{matrix}
\right)^T
\left(
\begin{matrix}
b_1^2\\
\sqrt{2}b_1b_2\\
b_2^2
\end{matrix}
\right)=a_1^2b_1^2+2a_1b_1a_2b_2+a_2^2b_2^2=(a_1b_1+a_2b_2)^2\\
=(\left(
\begin{matrix}
a_1\\
a_2
\end{matrix}
\right)
\left(
\begin{matrix}
b_1\\
b_2
\end{matrix}
\right)
)=(\pmb{a}^T\pmb{b}^2)
$$

可以看到，变换后的矩阵的点积等于原始向量点积的平方：$\phi(\pmb{a})^T\phi(\pmb{b})=(\pmb{a}^T\pmb{b})^2$。

关键点在于：如果将变换$\phi$应用到所有训练实例上，则对偶问题将包含点积$\phi(\pmb{x}^{(i)})^T\phi(\pmb{x}^{(j)})$（见[式1](#(支持向量机)(二次规划)(对偶问题)(1))）。但是如果$\phi$是[式2](#(支持向量机)(二次规划)(核SVMs)(1))定义的二次多项式变换的话，则可以将该变换后的向量的点积简单替换为$({\pmb{x}^{(i)}}^T\pmb{x}^{(j)})^2$。因此，这里完全不需要变换实例，只需要在[式1](#(支持向量机)(二次规划)(对偶问题)(1))中将点积替换为它的平方即可。结果严格相等，就好像将训练集转换了，然后拟合了一个线性SVM算法，但是该技巧使得整个过程的计算效率大大提高。

> The result will be strictly the same as if you had gone through the trouble of transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient.

函数$K(\pmb{a},\pmb{b})=(\pmb{a}^T\pmb{b})^2$为二次多项式核。在机器学习中，一个**核（kernel）**为一个函数，它可以仅基于原始向量$\pmb{a}$与$\pmb{b}$计算点积$\phi(\pmb{a})^T\phi(\pmb{b})$，而不需要计算（甚至不必知道）转换$\phi$。下面列出一些最常用的核：

- 线性：$K(\pmb{a},\pmb{b})=\pmb{a}^T\pmb{b}$。
- 多项式：$K(\pmb{a},\pmb{b})=(\gamma\pmb{a}^T\pmb{b}+r)^d$。
- 高斯RBF：$K(\pmb{a},\pmb{b})=\exp(-\gamma||\pmb{a}-\pmb{b}||^2)$。
- sigmoid：$K(\pmb{a},\pmb{b})=\tanh(\gamma\pmb{a}^T\pmb{b}+r)$。

[公式2](#(支持向量机)(SVM求解)(对偶问题)(2))展示了对于线性SVM分类器，如何根据对偶问题的解得到原始问题的解。但是如果应用核技巧的话，则最终得到一个包含$\phi(x^{(i)})$的等式。实际上，$\widehat{\pmb{w}}$的维数必须与$\phi(x^{(i)})$相同，后者可能非常大甚至无穷大，因此无法计算它。可以将[公式2](#(支持向量机)(SVM求解)(对偶问题)(2))中的$\pmb{w}$代入到新实例$\pmb{x}^{(n)}$的决策函数中，并得到一个等式，它只包含向量间的点乘，从而可以在不知道$\widehat{\pmb{w}}$的情况下做出预测。这使得使用核技巧成为可能：

$$
h_{\widehat{\pmb{w}},\widehat{b}}(\phi(\pmb{x}^{(n)}))=
\widehat{\pmb{w}}^T\phi(\pmb{x}^{(n)})+\widehat{b}=(\sum_{i=1}^m\widehat{\alpha}^{(i)}t^{(i)}\phi(\pmb{x}^{(i)}))^T\phi(\pmb{x}^{(n)})+\widehat{b}=
\sum_{i=1}^m\widehat{\alpha}^{(i)}t^{(i)}(\phi(\pmb{x}^{(i)})^T\phi(\pmb{x}^{(n)}))+\widehat{b}
=\underset{\widehat{\alpha}^{(i)}>0}{\sum_{i=1}^m}\widehat{\alpha}^{(i)}t^{(i)}K(\pmb{x}^{(i)},\pmb{x}^{(n)}))+\widehat{b}
$$

注意，因为仅对支持向量有$\alpha^{i}\ne0$，这使得预测涉及计算新输入向量$\pmb{x}^{(n)}$与支持向量（而不是所有训练实例）间的点乘。当然，也可以使用相同技巧去计算偏置项$\widehat{b}$：

$$
\widehat{b}
=\frac{1}{n_s}\underset{\widehat{\alpha}^{(i)}>0}{\sum_{i=1}^m}(t^{(i)}-\widehat{\pmb{w}}^T\phi(\pmb{x}^{(i)}))
=\frac{1}{n_s}\underset{\widehat{\alpha}^{(i)}>0}{\sum_{i=1}^m}(t^{(i)}-(\sum_{j=1}^m\widehat{\alpha}^{(j)}t^{(j)}\phi(\pmb{x}^{(j)}))^T\phi(\pmb{x}^{(i)}))
=\frac{1}{n_s}\underset{\widehat{\alpha}^{(i)}>0}{\sum_{i=1}^m}(t^{(i)}-\underset{\widehat{\alpha}^{(j)}>0}{\sum_{j=1}^{m}}\widehat{\alpha}^{(j)}t^{(j)}K(\pmb{x}^{(i)},\pmb{x}^{(j)})
$$

## 在线SVMs

对于线性SVM分类器，一种实现在线SVM分类器的方法是使用梯度下降（例如，使用`SGDClassifier`）最小化下式中的损失函数，它是从原始问题推导而来的。不幸的是，梯度下降比基于QP的方法的收敛速度慢得多。
$$
J(\pmb{w},b)=\frac{1}{2}\pmb{w}^T\pmb{w}+C\sum_{i=1}^m\max(0,1-t^{(i)}(\pmb{w}^T\pmb{w}^{(i)}+b))
$$
损失函数中的第一个和将使得模型具有较小的权重向量$\pmb{w}$，导致较大的间隔。第二个和计算所有边界破坏的总和。如果实例位于街道外并位于正确一侧，则它的边界破坏等于0，否则它正比于到街道正确一侧的距离。最小化该项确保模型的边界破坏尽可能少且小。

函数$\max(0,1-t)$被称为**合页损失（hinge loss）**函数，如下所示。当$t\ge1$时它等于0。如果$t<1$，它的导数等于-1；如果$t>1$，它的导数等于0。它在$t=1$时不可微，但是就像套索回归一样，可以在$t=1$处使用任何**次导数（subderivative）**（即任何介于-1\~0之间的值）。

```python
t = np.linspace(-2, 4, 200)
h = np.where(1 - t < 0, 0, 1 - t)  # max(0, 1-t)

plt.figure(figsize=(5,2.8))
plt.plot(t, h, "b-", linewidth=2, label="$max(0, 1 - t)$")
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.axvline(x=0, color='k')
plt.yticks(np.arange(-1, 2.5, 1))
plt.xlabel("$t$", fontsize=16)
plt.axis([-2, 4, -1, 2.5])
plt.legend(loc="upper right", fontsize=16)
plt.show()
```

还可以实现在线核SVM（kernelized SVMs），就像论文[Incremental and Decremental Support Vector Machine Learning](https://homl.info/17)与[Fast Kernel Classifiers with Online and Active Learning](https://homl.info/18)中描述的一样。这些核SVMs是在Matlab与C++中实现的。对于大规模非线性问题，可以考虑使用神经网络。

# 决策树

## 分类

**决策树（Decision Tree）**是一种通用的（versatile）机器学习算法，可以执行分类和回归任务，甚至多输出（multioutput）任务。

决策树只需要很少的数据准备。实际上，决策树完全不需要特征缩放或特征居中。

> One of the many qualities of Decision Trees is that they require very little data preparation. In fact, they don’t require feature scaling or centering at all.

### 决策树构建算法

决策树是一种树形结构，其中每个内部结点基于一个属性对实例判断，每个分支代表一个判断结果的输出，每个叶结点代表一种分类结果。若需要判断某个实例所属类别，则从树的根结点开始，不断根据某个属性对结点进行判断，根据判断结果进入特定的子结点，直到进入叶结点，并查看该结点的预测类别作为实例的预测类别。

Scikit-Learn使用**分类与回归树（Classification and Regression Tree，CART）**算法去训练决策树。该算法首先根据一个属性$k$与一个阈值$t_k$将训练集划分为两个子集，算法搜索$(k,t_k)$，使得子集的纯度最高（按照子集大小加权）。算法的损失函数如下：
$$
J(k,t_k)=\frac{m_{left}}{m}G_{left}+\frac{m_{right}}{m}G_{right}
$$
其中$G_{left/right}$度量左/右子树的不纯净度，$m_{left/right}$为左/右子树的实例数量。

一旦CART算法成功将训练集一分为二，它会对每个子集按照相同的逻辑继续划分（一个递归过程）。当它最大深度（通过```max_depth```超参数定义），或无法找到一个划分能够减小不纯净度，则递归停止。一些其他的超参数可以控制额外的停止条件。

CART算法产生二叉树（binary tree），其他算法（例如ID3）产生的决策树的结点可以有多个孩子。

CART算法是贪心算法，它贪婪地在顶层搜索最优划分，然后在每个后续层级重复该过程。它不会检查划分是否会导致不纯净度在若干层后达到最低。一个贪心算法经常产生一个相当好的结果，但是不能保证该结果是最优的。

> As you can see, the CART algorithm is a *greedy algorithm*: it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal.

找到最优的树是一个NP完全问题，需要$O(\exp(m))$的时间，因此即使训练集很小也难以解决。

决策树可以预测估计实例属于特定类别$k$的概率：首先它遍历树以找到该实例的叶结点，然后返回该结点中类别为$k$的训练实例所占的比例。如果一个实例属于某个类别的估计概率最高，则决策树预测它属于该类别。注意，一个结点中所有属于特定类的训练实例的估计概率都是相同的，即使它们的特征不同。

### 不纯净度

Scikit-Learn默认使用Gini度量结点的**不纯净度（impurity）**，Gini不纯净度的定义如下：
$$
G_i=1-\sum_{k=1}^np_{i,k}^2
$$
其中，$p_{i,k}$表示第$i$个结点的训练实例中，类别$k$的实例所占的比例。

如果设置超参数```criterion```为```entropy```，则算法使用**熵（entropy）**度量不纯净度。在机器学习领域，熵经常作为不纯净度度量（impurity measure）：如果一个集合只包含一类实例，则它的熵为0。第$i$个结点的熵定义如下：

$$
H_i=-\underset{p_{i,k\ne0}}{\sum_{k=1}^n}p_{i,k}\log_2(p_i,k)
$$

大多数情况下，使用Gini不纯净度还是熵不会导致很大的不同（即它们能生成相似的树）。Gini不纯净度计算速度略快，因此是一个好的默认值。

下面使用iris数据集去展示决策树的使用。

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data[:, 2:]
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X, y)
```

```python
import os

PROJECT_ROOT_DIR = "."
CHAPTER_ID = "decision_trees"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
```

绘制决策树：

```python
from graphviz import Source
from sklearn.tree import export_graphviz

# 使用该方法输出一个图定义文件（graph definition file）iris_tree.dot。
export_graphviz(
        tree_clf,
        out_file=os.path.join(IMAGES_PATH, "iris_tree.dot"),
        feature_names=iris.feature_names[2:],
        class_names=iris.target_names,
        rounded=True,
        filled=True
    )

# 绘制。
Source.from_file(os.path.join(IMAGES_PATH, "iris_tree.dot"))
```

也可以使用Graphviz包的dot命令行工具将.dot文件转换为各种格式，例如PDF或PNG。

Graphviz是开源的可视化软件包，见http://www.graphviz.org/。

以下命令行将.dot文件转换为.png图像文件：

```
$ dot -Tpng iris_tree.dot -o iris_tree.png
```

假设有一个想要分类的鸢尾花。从根结点开始，该结点询问该花的花瓣长度是否小于（等于）2.45厘米，如果是，则进入根的左孩子结点，这是一个叶结点，因此只需要简单地查看该结点的预测类别是什么（为`class=setosa`），决策树预测该花为Iris setosa。

如果鸢尾花的花瓣长度大于2.45厘米，则进入根的右孩子结点，这不是叶结点，因此该结点询问该花的花瓣宽度是否小于（等于）1.75厘米，如果是，则该花最有可能是Iris versicolor，否则最有可能是Iris virginica。

结点的`samples`属性统计它所作用的训练实例的数量；结点的`value`属性指出它对每个类别所作用的训练实例数量；结点的`gini`属性度量它的不纯净度。例如，深度为2的最左结点的`gini`得分为$1-(0/54)^2-(49/54)^2-(5/54^2)\approx0.168$。如果采用熵度量，则它的不纯净度为$-(49/54)\log_2(49/54)-(5/54)\log_2(5/54)\approx0.445$。

下面展示该决策树的决策边界：

```python
import numpy as np
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt

def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):
    x1s = np.linspace(axes[0], axes[1], 100)
    x2s = np.linspace(axes[2], axes[3], 100)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])
    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)
    if not iris:
        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)
    if plot_training:
        plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", label="Iris setosa")
        plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", label="Iris versicolor")
        plt.plot(X[:, 0][y==2], X[:, 1][y==2], "g^", label="Iris virginica")
        plt.axis(axes)
    if iris:
        plt.xlabel("Petal length", fontsize=14)
        plt.ylabel("Petal width", fontsize=14)
    else:
        plt.xlabel(r"$x_1$", fontsize=18)
        plt.ylabel(r"$x_2$", fontsize=18, rotation=0)
    if legend:
        plt.legend(loc="lower right", fontsize=14)

plt.figure(figsize=(8, 4))
plot_decision_boundary(tree_clf, X, y)
plt.plot([2.45, 2.45], [0, 3], "k-", linewidth=2)
plt.plot([2.45, 7.5], [1.75, 1.75], "k--", linewidth=2)
plt.plot([4.95, 4.95], [0, 1.75], "k:", linewidth=2)
plt.plot([4.85, 4.85], [1.75, 3], "k:", linewidth=2)
plt.text(1.40, 1.0, "Depth=0", fontsize=15)
plt.text(3.2, 1.80, "Depth=1", fontsize=13)
plt.text(4.05, 0.5, "(Depth=2)", fontsize=11)

plt.show()
```

粗垂直线表示根节点的决策边界：花瓣长度等于2.45厘米，左侧区域是纯净的（只有Iris setosa），无法再划分。

右侧区域不纯净，因此深度为1的右结点在花瓣宽度等于1.75厘米的地方划分它（虚线表示）。

因为`max_depth`设置为2，决策树在此停止划分。如果设置`max_depth`为3，则这两个深度为2的结点都会再加一个决策边界（点线表示）。

一个区域的所有属于特定类的训练实例的估计概率都是相同的，即使它们的特征不同。

预测概率：

```python
tree_clf.predict_proba([[5, 1.5]])
```

如果花瓣长度为6厘米，宽度为1.5厘米，则预测概率不变（即使很明显在这种情况下，它很可能是Iris virginica）。

预测类别：

```python
tree_clf.predict([[5, 1.5]])
```

### 计算复杂度

进行预测需要从根到叶遍历决策树。决策树通常是近似平衡的，因此遍历一个决策树需要经过大约$O(\log_2(m))$个结点。每个结点只需要检查一个特征值，因此预测的复杂度为$O(\log_2(m))$（与特征数量无关）。因此预测是一个非常快速的过程，即使是在处理很大的训练集时。

训练算法针对每个结点的所有样本比较所有特征（如果设置了```max_feature```则要比较的特征会少一些）。针对每个结点的所有样本比较所有特征的训练复杂度为$n\times m\log_2(m)$。对于小的训练集（少于几千个实例），Scikit-Learn可以通过数据预排序（设置```presort=True```）来加速训练，但是对于大型训练集，这么做将显著降低训练速度。

### 正则化超参数

决策树对训练数据几乎不作假设（相反，线性模型假设数据是线性的）。如果不对其施加约束，则树的结构会调整以适应训练数据，高度拟合数据——很可能过拟合数据。这样的模型被称为**无参数模型（nonparametric model）**（不是因为它没有参数，而是在训练前参数的数量不确定，实际上这样的模型常常有很多参数，因此模型结构可以自由紧贴（free to stick closely to）数据）。反之，**参数模型（parametric model）**（例如线性模型）的参数数量事先确定，所有它的自由度受限，降低了过拟合的风险（但是增加了欠拟合的风险）。

为了避免过拟合训练数据，需要在训练过程中限制决策树的自由度（即正则化）。正则化超参数依赖于所使用的算法，但是通常至少可以约束决策树的最大深度。在Scikit-Learn中，这被超参数`max_depth`控制（默认为`None`，即不受限制）。减少`max_depth`会正则化模型并减少过拟合的风险。

`DecisionTreeClassifier`类还有一些其他参数限制决策树的形状：

- `min_samples_split`：结点划分前必须具有的最小样本数。
- `min_samples_leaf`：叶结点必须具有的最少样本数。
- `min_weight_fraction_leaf`：和`min_samples_leaf`类似，但是使用加权实例总数的比例来描述。
- `max_leaf_nodes`：叶结点的最大数量。
- `max_features`：在划分每个结点时最多用于评估的特征数量。

> `max_features` (the maximum number of features that are evaluated for splitting at each node).

增加`min_*`超参数或降低`max_*`超参数会正则化模型。

> Other algorithms work by first training the Decision Tree without restrictions, then *pruning* (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Standard statistical tests, such as the *χ*2 *test* (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance (which is called the *null hypothesis*). If this probability, called the *p-value*, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.

下面使用两个决策树在moons数据集上训练的结果，一个模型使用了默认超参数（即没有约束），另一个模型使用了超参数`min_samples_leaf=4`。

```python
from sklearn.datasets import make_moons
Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)

deep_tree_clf1 = DecisionTreeClassifier(random_state=42)
deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)
deep_tree_clf1.fit(Xm, ym)
deep_tree_clf2.fit(Xm, ym)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
plt.sca(axes[0])
plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)
plt.title("No restrictions", fontsize=16)
plt.sca(axes[1])
plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)
plt.title("min_samples_leaf = {}".format(deep_tree_clf2.min_samples_leaf), fontsize=14)
plt.ylabel("")

plt.show()
```

可以看到，左边模型过拟合，右边模型可能泛化得更好。

## 回归

决策树也能执行回归任务。用于回归的CART损失函数如下（区别在于不是以最小化不纯净度的方式划分训练集，而是以最小化MSE的方式划分训练集）：
$$
J(k,t_k)=\frac{m_{left}}{m}MSE_{left}+\frac{m_{right}}{m}MSE_{right}\\
\ where\ \left\{
\begin{aligned}
&MSE_{node}=\sum_{i\in node}(\widehat{y}_{node}-y^{(i)})^2\\
&\widehat{y}_{node}=\frac{1}{m_{node}}\sum_{i\in node}y^{(i)}
\end{aligned}
\right.
$$
决策树预测实例的目标值时也是遍历树找到对应的叶结点，并使用叶结点的预测值作为实例的预测值。叶结点的预测值为该叶结点中的所有结点的目标值的平均。

下面使用决策树执行回归任务。

生成有噪音的二次数据集：

```python
np.random.seed(42)
m = 200
X = np.random.rand(m, 1)
y = 4 * (X - 0.5) ** 2
y = y + np.random.randn(m, 1) / 10
```

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg.fit(X, y)
```


下面绘制出模型的预测。可以看到，每个区域（region）的预测值都是该区域内实例的目标值的平均（average target value）。该算法以使大多数训练实例尽可能接近该预测值的方式划分每个区域。

```python
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor

tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)
tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel="$y$"):
    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)
    y_pred = tree_reg.predict(x1)
    plt.axis(axes)
    plt.xlabel("$x_1$", fontsize=18)
    if ylabel:
        plt.ylabel(ylabel, fontsize=18, rotation=0)
    plt.plot(X, y, "b.")
    plt.plot(x1, y_pred, "r.-", linewidth=2, label=r"$\hat{y}$")

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
plt.sca(axes[0])
plot_regression_predictions(tree_reg1, X, y)
for split, style in ((0.1973, "k-"), (0.0917, "k--"), (0.7718, "k--")):
    plt.plot([split, split], [-0.2, 1], style, linewidth=2)
plt.text(0.21, 0.65, "Depth=0", fontsize=15)
plt.text(0.01, 0.2, "Depth=1", fontsize=13)
plt.text(0.65, 0.8, "Depth=1", fontsize=13)
plt.legend(loc="upper center", fontsize=18)
plt.title("max_depth=2", fontsize=14)

plt.sca(axes[1])
plot_regression_predictions(tree_reg2, X, y, ylabel=None)
for split, style in ((0.1973, "k-"), (0.0917, "k--"), (0.7718, "k--")):
    plt.plot([split, split], [-0.2, 1], style, linewidth=2)
for split in (0.0458, 0.1298, 0.2873, 0.9040):
    plt.plot([split, split], [-0.2, 1], "k:", linewidth=1)
plt.text(0.3, 0.5, "Depth=2", fontsize=13)
plt.title("max_depth=3", fontsize=14)

plt.show()
```

下面绘制出决策树。

```python
export_graphviz(
        tree_reg1,
        out_file=os.path.join(IMAGES_PATH, "regression_tree.dot"),
        feature_names=["x1"],
        rounded=True,
        filled=True
    )
```

```python
Source.from_file(os.path.join(IMAGES_PATH, "regression_tree.dot"))
```

这个树与前例中的分类树非常相似。主要不同点在于它在每个结点中预测一个值而不是类别。例如假设要想对$x_1=0.6$的新实例进行预测，则从根结点开始遍历树，直到到达预测$value=0.111$的叶结点。这个预测值是与该叶结点关联的110个训练实例的平均值，这110个实例的均方误差为$0.015$。

使用决策树处理回归任务也容易过拟合。

下面展示决策树的拟合情况：

```python
tree_reg1 = DecisionTreeRegressor(random_state=42)
tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

x1 = np.linspace(0, 1, 500).reshape(-1, 1)
y_pred1 = tree_reg1.predict(x1)
y_pred2 = tree_reg2.predict(x1)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)

plt.sca(axes[0])
plt.plot(X, y, "b.")
plt.plot(x1, y_pred1, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([0, 1, -0.2, 1.1])
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", fontsize=18, rotation=0)
plt.legend(loc="upper center", fontsize=18)
plt.title("No restrictions", fontsize=14)

plt.sca(axes[1])
plt.plot(X, y, "b.")
plt.plot(x1, y_pred2, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([0, 1, -0.2, 1.1])
plt.xlabel("$x_1$", fontsize=18)
plt.title("min_samples_leaf={}".format(tree_reg2.min_samples_leaf), fontsize=14)

plt.show()
```

如果没有正则化（即使用默认的超参数），得到左图，可以看出预测严重过拟合。右图设置了`min_samples_leaf=10`，模型要合理得多。

## 不稳定性（Instability）

决策树容易理解与解释，容易使用，通用（versatile）且强大（powerful），但是有一些局限性。决策树喜欢正交的决策边界（使用的划分都垂直于轴），这使得决策树对数据集旋转（rotation）很敏感。更一般地，决策树的主要问题在于其对训练数据中的微小变化非常敏感。

随机森林可以通过平均多个树的预测值来限制这种不稳定性。

下面生成线性可分的数据集，然后使用决策树划分它。对于左图，决策树很容易就划分它；对于右图，数据集被旋转了$45\degree$，可以看到决策边界变得复杂起来。这说明决策树喜欢正交的决策边界。虽然两个决策树都完美地拟合训练数据，但是右图的模型很可能无法很好地泛化。限制该问题的一种方法是使用主成分分析，这通常会导致训练数据的方向更好。

```python
np.random.seed(6)
Xs = np.random.rand(100, 2) - 0.5
ys = (Xs[:, 0] > 0).astype(np.float32) * 2

angle = np.pi / 4
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])
Xsr = Xs.dot(rotation_matrix)

tree_clf_s = DecisionTreeClassifier(random_state=42)
tree_clf_s.fit(Xs, ys)
tree_clf_sr = DecisionTreeClassifier(random_state=42)
tree_clf_sr.fit(Xsr, ys)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
plt.sca(axes[0])
plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)
plt.sca(axes[1])
plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)
plt.ylabel("")

plt.show()
```

对于iris数据集同理：

```python
X = iris.data[:, 2:]
y = iris.target
```

```python
angle = np.pi / 180 * 20
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])
Xr = X.dot(rotation_matrix)

tree_clf_r = DecisionTreeClassifier(random_state=42)
tree_clf_r.fit(Xr, y)

plt.figure(figsize=(8, 3))
plot_decision_boundary(tree_clf_r, Xr, y, axes=[0.5, 7.5, -1.0, 1], iris=False)

plt.show()
```

下面展示在相同的数据集上训练相同模型也会导致每次生成不同的模型（设置不同的`random_state`）：

```python
tree_clf_tweaked = DecisionTreeClassifier(max_depth=2, random_state=40)
tree_clf_tweaked.fit(X, y)
```

```python
plt.figure(figsize=(8, 4))
plot_decision_boundary(tree_clf_tweaked, X, y, legend=False)
plt.plot([0, 7.5], [0.8, 0.8], "k-", linewidth=2)
plt.plot([0, 7.5], [1.75, 1.75], "k--", linewidth=2)
plt.text(1.0, 0.9, "Depth=0", fontsize=15)
plt.text(1.0, 1.80, "Depth=1", fontsize=13)

plt.show()
```

如果只是移除了训练集中最宽的Iris versicolor并训练一个新的决策树，也会导致一个完全不同的模型。实际上，因为Scikit-Learn的训练算法是随机的（在每个结点，随机选取一组特征去评估），即使在相同的训练数据上也可能得到非常不同的模型（除非设置`random_state`超参数）。

# 集成学习

整合（aggregate）一组预测器（例如分类器或回归器（regressor））进行预测的方法被称为**集成学习（Ensemble Learning）**。这一组预测器被称为**集成（ensemble）**,集成学习算法被称为**集成方法（Ensemble method）**。

> Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor.

一个项目快结束的时候常常使用集成算法，一旦建立了一些好的预测器，就可以将它们结合为一个更好的预测器。事实上，机器学习竞赛中获胜的解决方法通常都包含一些集成方法。

## 投票分类器（Voting Classifiers）

假设已经训练了一些分类器（例如一个Logistic回归分类器、一个SVM分类器、一个随机森林、一个K-近邻算法，等等）且它们的性能都不差（例如每个都达到了80%左右的准确率）。

如果要创建一个更好的分类器，可以将这些分类器的预测进行整合（aggregate），然后将投票最多的类别作为预测类别。这样的分类器被称为**硬投票（hard voting）**分类器。

![训练多种不同的分类器](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\训练多种不同的分类器.png)



![硬投票分类器预测](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\硬投票分类器预测.png)

如果每个分类器都可以估计类别概率（即都有`predict_proba`方法），则可以构建**软投票（soft voting）**分类器，这样的分类器将类别概率最高的类别作为预测的类别，其中类别概率是每个单独（individual）分类器预测该类别的概率的平均。软投票的表现常常比硬投票的表现好，因为它给高信心的投票更大的权重。

投票分类器常常比集成中的最好的分类器的准确率更高。即使每个分类器都是**弱分类器（weak learner）**（意味着它比胡乱猜测稍好），它们的集成仍然可以成为**强分类器（strong learner**（取得高准确率），只要有足够多的弱分类器且它们足够多样化（diverse）。这源于大数定律（law of large numbers）。当然，前提条件是分类器完全独立、犯不相关的错误。但事实显然不是这样，因为它们都在相同的数据上训练，它们可能犯相同类型的错误，因此可能有很多的多数票投给错误类别，导致集成的准确率降低。

> They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.

下面是10个（10 series）偏性掷币（biased coin）（每个硬币正面向上的概率为51%）的投掷结果：

```python
import numpy as np

heads_proba = 0.51
coin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)
cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1)
```

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(8,3.5))
plt.plot(cumulative_heads_ratio)
plt.plot([0, 10000], [0.51, 0.51], "k--", linewidth=2, label="51%")
plt.plot([0, 10000], [0.5, 0.5], "k-", label="50%")
plt.xlabel("Number of coin tosses")
plt.ylabel("Heads ratio")
plt.legend(loc="lower right")
plt.axis([0, 10000, 0.42, 0.58])
plt.show()
```

可以看到随着投掷次数增加，正面向上的比例接近（approach）51%。最终所有硬币正面向上比例都接近（close to）51%，超过了50%。

当预测器彼此尽可能独立时，集成方法工作得最好。要让分类器尽可能多样（diverse），可以使用非常不同的算法训练它们，这增加分类器犯不同错误的概率，从而提高集成的准确率。

下面使用moons数据集展示集成方法的使用。

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
```

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression(solver="lbfgs", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
svm_clf = SVC(gamma="scale", random_state=42)

voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='hard')
```

```python
voting_clf.fit(X_train, y_train)
```

查看每个分类器的准确率：

```python
from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
```

可以看到投票分类器的表现略好于其他所有单独分类器。

如果要使用软投票，需要将`voting=hard`替换为`voting=soft`并确保所有的分类器可以估计类别概率。`SVC`类默认不能做到，因此需要将`SVC`的超参数`probability`设置为`True`使得`SVC`类使用交叉验证估计类别概率，导致训练速度降低，同时增加一个`predict_proba`方法。使用了软投票后投票分类器的准确率更高：

```python
log_clf = LogisticRegression(solver="lbfgs", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
svm_clf = SVC(gamma="scale", probability=True, random_state=42)

voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='soft')
voting_clf.fit(X_train, y_train)
```

```python
from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
```

## Bagging与Pasting

除了使用尽可能使用不同的训练算法得到一组多样的分类器外，还可以为每个预测器使用相同的训练算法，但是在训练集的不同随机子集上训练它们从而得到一组多样的分类器。如果子集是通过有放回采样得到的，则该方法被称为[**bagging**](https://homl.info/20)（**bootstrap aggregating**的缩写）；如果子集是不放回采样得到的，则该方法被称为[**pasting**](https://homl.info/21)。也就是说，bagging与pasting都允许训练实例被多个预测器多次采样，但是只有bagging允许训练实例被同一个预测器多次采样。

![bagging与pasting涉及在训练集的不同随机样本上训练若干预测器（Bagging and pasting involves training several predictors on different random samples of the training set）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\bagging与pasting.png)

一旦所有预测器训练完成，集成就可以通过整合所有预测器的预测做出预测，整合函数通常使用统计模式（statistical mode）（即最常见的预测结果，就像硬投票分类器那样）分类，或为回归计算均值。每个单独预测器的偏差大于其在原始训练集上训练得到的偏差，但是整合可以同时减少偏差与方差。通常最终结果是：相比于在原始训练集上训练的单个预测器，集成的偏差类似，方差更低。

> The aggregation function is typically the *statistical mode* (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression.

如上图所示，预测器可以在不同的CPU核甚至不同的服务器上被并行训练，同理，预测也可以并行。这是bagging与pasting方法大受欢迎的原因之一：它们的扩展性很好。

> This is one of the reasons bagging and pasting are such popular methods: they scale very well.


Scikit-Learn为bagging与pasting提供了一个简单的API：`BaggingClassifier`类（对于回归是`BaggingRegressor`）。如果基分类器可以估计类别概率（即它有一个`predict_proba`方法），则`BaggingClassifier`自动执行软投票而不是硬投票。

训练一个包含500个决策树分类器的集成，每个分类器在训练集上随机有放回采样的100个训练实例上训练（执行软投票）：

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500,
    max_samples=100, bootstrap=True, random_state=42)  # 如果要使用pasting，设置bootstrap=False；参数n_jobs告诉Scikit-Learn用于训练与预测的CPU核的数量（如果设置为-1，则Scikit-Learn使用所有可用的核）；max_samples可被设置为0.0到1.0之间的浮点数，此时要采样的最大实例数等于训练集的大小乘以max_samples。
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
```

比较集成与单个决策树的表现：

```python
print(accuracy_score(y_test, y_pred))
```

```python
tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)
print(accuracy_score(y_test, y_pred_tree))
```

比较集成与单个决策树的决策边界：

```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):
    x1s = np.linspace(axes[0], axes[1], 100)
    x2s = np.linspace(axes[2], axes[3], 100)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])
    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)
    if contour:
        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", alpha=alpha)
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", alpha=alpha)
    plt.axis(axes)
    plt.xlabel(r"$x_1$", fontsize=18)
    plt.ylabel(r"$x_2$", fontsize=18, rotation=0)
```

```python
fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)
plt.sca(axes[0])
plot_decision_boundary(tree_clf, X, y)
plt.title("Decision Tree", fontsize=14)
plt.sca(axes[1])
plot_decision_boundary(bag_clf, X, y)
plt.title("Decision Trees with Bagging", fontsize=14)
plt.ylabel("")
plt.show()
```

可以看出，集成的预测可能泛化得好得多：它们的偏差相当，但是方差更小（训练集上犯错的次数大致相同，但是决策边界更规则）。

bootstrap在每个预测器训练的子集上引入了更多的多样性，因此bagging的偏差最终比pasting大，但这也意味着每个预测器最终关联更小，因此集成的方差会减小。总体上，bagging经常会产生更好的模型，因此通常是首选的。如果时间与CPU资源充裕的话，可以使用交叉验证去评估bagging与pasting，并选择工作最好的。

### Out-of-Bag评估

当使用bagging时，对于任意给定预测器，一些实例可能被多次采样而另一些实例一次也不会被采样到。默认情况下`BaggingClassifier`有放回采样$m$个训练实例（`bootstrap=True`），其中`m`为训练集的大小。这意味着平均大约有63%的训练实例被每个预测器采样（当$m$增大时，该比例接近$1-\exp(-1)$），剩下的未被采样的训练实例被称为**out-of-bag（oob）**实例。注意对于每个预测器，oob实例并不相同。

> This means that only about 63% of the training instances are sampled on average for each predictor.

> Note that they are not the same 37% for all predictors.

因为一个预测器在训练过程中永远无法看到oob实例，所以它可以在这些实例上评估，而不用使用一个单独的验证集。可以将每个预测器的oob评估的均值作为集成的评估。

在Scikit-Learn中，可以在创建`BaggingClassifier`时设置`oob_score=True`去请求在训练后自动执行oob评估。通过`oob_score_`变量可以获得最终的评估得分。

```python
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500,
    bootstrap=True, oob_score=True, random_state=40)
bag_clf.fit(X_train, y_train)
bag_clf.oob_score_  # 大约89.9%。
```

每个训练实例的oob决策函数可以通过`oob_decision_function_`变量得到。在该例中，决策函数返回每个实例的类别概率（因为基估计器有`predict_proba`方法）。例如，oob评估估计第一个训练实例有67.72%的概率属于正类，32.28%的概率属于负类：

```python
bag_clf.oob_decision_function_
```

查看在测试集上的准确率：

```python
from sklearn.metrics import accuracy_score
y_pred = bag_clf.predict(X_test)
accuracy_score(y_test, y_pred)
```

91.2%，与oob评估得分很接近。

## 随机修复与随机子空间

除了对实例采样外，也可以对特征进行采样。同时采样实例与特征被称为[**随机修复（Random Patches）**方法](https://homl.info/22)；使用所有实例但是采样特征被称为[**随机子空间（Random Subspaces）**方法](https://homl.info/23)。

`BaggingClassifier`类支持随机修复与随机子空间（通过超参数`max_features`与`bootstrap_features`控制，它们的使用方式同`max_samples`与`bootstrap`，只是用于特征采样而不是实例采样）。例如，如果要使用随机子空间，则设置：`bootstrap=False`、`max_samples=1.0`；`bootstrap_features=True`并/或设置`max_features`为小于1.0的某个值。

采样特征导致更大的预测器多样性，造成更大的偏差与更小的方差。

## 随机森林

[**随机森林（Random Forest）**](https://homl.info/24)是决策树的集成，通常使用bagging方法训练（有时也用pasting方法训练），通常将`max_samples`设置为训练集的大小。可以直接使用`RandomForestClassifier`类而不通过构建`BaggingClassifier`并向其传递一个`DecisionTreeClassifier`来实现随机森林，前者更方便且为决策树作了优化。同样地（similarly），`RandomForestRegressor`类用于回归任务。

下面使用`RandomForestClassifier`实现随机森林。该代码使用所有可用CPU核去训练随机森林分类器，包含500个树，每个树最多有16个结点：

```python
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
```

```
y_pred_rf
```

下面使用`BaggingClassifier`实现随机森林，它大致与上述的`RandomForestClassifier`等价：

```python
bag_clf = BaggingClassifier(
 DecisionTreeClassifier(splitter="random", max_leaf_nodes=16),
 n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)
```

除了少数例外，`RandomForestClassifier`拥有`DecisionTreeClassifier`的所有超参数（用于控制树（trees）的生长方式），以及`BaggingClassifier`的所有超参数，用于控制集成本身。

随机森林算法在树生长时引入额外的随机性，它会在特征的随机子集中（而不是在所有特征中）搜索最佳特征。该算法导致更大的树多样性，造成更大的偏差与更小的方差，通常会产生一个总体上更好的模型。

### Extra-Trees

[**Extremely Randomized Trees**](https://homl.info/25)（简写为**Extra-Trees**）除了在每个结点处使用随机特征子集用于划分外，还为每个特征使用随机的阈值用于划分（而不是搜索最好的可能阈值）。这引入了更多的随机性。

同样地，该技术造成更大的偏差与更小的方差。它也使得Extra-Tree比常规的随机森林训练更快，因为在每个结点为每个特征找到最佳阈值是生长树（growing a tree）的最耗时的任务之一。

可以使用Scikit-Learn的`ExtraTreesClassifier`类创建Extra-Tree分类器，它的API与`RandomForestClassifier`类的一样。同样地，用于回归的`ExtraTreesRegressor`类与`RandomForestRegressor`类有相同的API。

`RandomForestClassifier`与`ExtraTreesClassifier`的表现孰好孰坏难以预先确定。通常，唯一的办法是使用交叉验证对比两者表现。

> Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters using grid search).

### 特征重要度

随机森林的一个重要特性是可以度量每个特征的相对重要度。Scikit-Learn通过查看使用某个特征的树结点减少不纯净度的程序的加权平均来度量该结点的重要程度，其中每个结点的权重等于与其关联的训练样本的数量。Scikit-Learn在训练后自动为每个特征计算该得分，然后缩放结果使得重要程度的和为1。

> Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it (see Chapter 6).

下面在MNIST数据集上训练一个随机森林分类器并绘制每个像素的重要度：

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1, as_frame=False)
mnist.target = mnist.target.astype(np.uint8)
```

```python
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rnd_clf.fit(mnist["data"], mnist["target"])
```

```python
def plot_digit(data):
    image = data.reshape(28, 28)
    plt.imshow(image, cmap = mpl.cm.hot,
               interpolation="nearest")
    plt.axis("off")
```

```python
import matplotlib as mpl

plot_digit(rnd_clf.feature_importances_)  # 通过feature_importances_变量获得结果。

cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])
cbar.ax.set_yticklabels(['Not important', 'Very important'])

plt.show()
```

随机森林可以很方便地用来快速了解哪些特征重要，尤其当执行特征选择的时候。

## Boosting

**Boosting**（原来叫做**hypothesis boosting**）指的是任何可以将若干弱学习器组合成一个强学习器的集成方法。大多数boosting方法的基本思想是：顺序（sequentially）训练若干预测器，每个预测器尝试纠正前一个预测器。有很多boosting方法可用，但是目前最受欢迎的是[**AdaBoost**](https://homl.info/26)（**Adaptive Boosting**的缩写）与**Gradient Boosting**。

顺序学习技术的缺点在于它不能并行化（或只能部分并行），因为每个预测器只有在前一个预测器被训练并评估完成后才能被训练。因此，它的扩展性不如bagging或pasting。

### AdaBoost

对于一个新的预测器来说，纠正其前驱的一种方法是对前驱欠拟合的训练实例给予更多的关注。这导致新的预测器越来越关注难以分类的实例。这就是AdaBoost采用的技术。

例如，当训练一个AdaBoost分类器时，算法首先训练一个基分类器（例如决策树），然后使用它在训练集上预测。算法增加被错误分类的训练实例的相对权重，然后使用更新后的权重训练第二个分类器，然后再次在训练集上预测，更新实例权重，如此下去。

![AdaBoost顺序训练与实例权重更新](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\AdaBoost顺序训练与实例权重更新.png)

一旦所有预测器训练完成，该集成预测的方式很像bagging或pasting，除了预测器的权重不一样（取决于它们在加权实例上的总体准确度）。

如果AdaBoost集成过拟合，则可以减少估计器的数量或对基估计器使用更强的正则化。

下面展示的是（用于二分类任务的）AdaBoost算法。

每个实例的初始权重为$1/m$。第$i$（初始为1）个预测器在训练集上的加权错误率$r_1$如下：
$$
r_j=\frac{\underset{\widehat{y}_j^{(i)}\ne y^{(i)}}{\sum_{i=1}^m}w^{i}}{\sum_{i=1}^mw^{(i)}}
$$
其中$\widehat{y}_j^{(i)}$为第$j$个预测器对第$i$个实例的预测。

预测器的权重$\alpha_j$计算如下：
$$
\alpha_j=\eta\log\frac{1-r_j}{r_j}
$$
其中$\eta$为超参数：学习率（默认为1，原始的AdaBoost未使用该超参数）。预测器越准确，它的权重越高。如果预测器随机猜测，则它的权重接近0；如果预测器的准确率不如随机猜测，则它的权重是负的。

> However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.

然后AdaBoost算法更新实例权重：
$$
w^{(i)}\leftarrow
\left\{
\begin{aligned}
w^{(i)}\ if\ \widehat{y}_j^{(i)}=y^{(i)}\\
w^{(i)}\exp(\alpha_j)\ if\ \widehat{y}_j^{(i)}=y^{(i)}
\end{aligned}
\right.
$$
然后所有实例的权重被规格化（normalized）（即除以$\sum_{i=1}^mw^{(i)}$）。

最后使用更新后的权重训练新的预测器，然后$i\leftarrow i+1$，重复以上过程，直到达到所需的预测器数量或找到完美的预测器。

AdaBoost预测方式如下：
$$
\widehat{y}(\pmb{x})=\underset{k}{argmax}\underset{\widehat{y}_j(\pmb{x})=k}{\sum_{j=1}^N}\alpha_j
$$
其中$N$表示预测器的数量。

Scikit-Learn使用多类（multiclass）版本的AdaBoost，称作[SAMME](https://homl.info/27)（代表**Stagewise Additive Modeling using a Multiclass Exponential loss function**）。当只有两个类别时，SAMME等价于AdaBoost。如果预测器可以估计类别概率（有`predict_proba`方法），Scikit-Learn可以使用SAMME的一个变体，称为SAMME.R（R代表“Real”），它依赖于类别概率而不是预测结果，且通常表现更好。

> For more details, see Ji Zhu et al., “Multi-Class AdaBoost,” *Statistics and Its Interface* 2, no. 3 (2009): 349–360.

下面代码基于200个**决策树桩（Decision Stump）**，使用Scikit-Learn的`AdaBoostClassifier`类训练一个AdaBoost分类器（同样地，有一个`AdaBoostRegressor`类）。决策树桩为`max_depth=1`的决策树，它只有一个决策结点加上两个叶结点，是`AdaBoostClassifier`类的默认基估计器。

```python
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200,
    algorithm="SAMME.R", learning_rate=0.5, random_state=42)
ada_clf.fit(X_train, y_train)
```

```python
plot_decision_boundary(ada_clf, X, y)
```

### 梯度Boosting

[**梯度Boosting（Gradient Boosting）**](https://homl.info/28)与AdaBoost一样，也是顺序地将预测器加到集成中，每个预测器纠正其前驱。但是，该方法尝试用新的预测器去拟合前一个预测器的**残差（residual errors）**而不是像AdaBoost那样每个迭代去调整实例权重。

下面将决策树作为基预测器，使用梯度Boosting执行回归任务。这被称为**梯度树Boosting（Gradient Tree Boosting）**或**梯度Boosted回归树（Gradient Boosted Regression Trees，GBRT）**。

生成一个有噪音的二次训练集：

```python
np.random.seed(42)
X = np.random.rand(100, 1) - 0.5
y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)
```

首先使用`DecisionTreeRegressor`拟合训练集：

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg1.fit(X, y)
```

接下来在第一个预测器产生的残差上训练第二个`DecisionTreeRegressor`：

```python
y2 = y - tree_reg1.predict(X)
tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg2.fit(X, y2)
```

然后再第二个预测器产生的残差上训练第三个回归器：

```python
y3 = y2 - tree_reg2.predict(X)
tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg3.fit(X, y3)
```

现在我们有了包含三个树的集成。接下来只要简单地将三个树的预测加起来去预测新实例：

```python
X_new = np.array([[0.8]])
```

```python
y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
```

```python
y_pred
```

下面绘制出三个树的预测（左栏）与集成的预测（右栏）：

```python
def plot_predictions(regressors, X, y, axes, label=None, style="r-", data_style="b.", data_label=None):
    x1 = np.linspace(axes[0], axes[1], 500)
    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)
    plt.plot(X[:, 0], y, data_style, label=data_label)
    plt.plot(x1, y_pred, style, linewidth=2, label=label)
    if label or data_label:
        plt.legend(loc="upper center", fontsize=16)
    plt.axis(axes)
```

```python
plt.figure(figsize=(11,11))

plt.subplot(321)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h_1(x_1)$", style="g-", data_label="Training set")
plt.ylabel("$y$", fontsize=16, rotation=0)
plt.title("Residuals and tree predictions", fontsize=16)

plt.subplot(322)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1)$", data_label="Training set")
plt.ylabel("$y$", fontsize=16, rotation=0)
plt.title("Ensemble predictions", fontsize=16)

plt.subplot(323)
plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label="$h_2(x_1)$", style="g-", data_style="k+", data_label="Residuals")
plt.ylabel("$y - h_1(x_1)$", fontsize=16)

plt.subplot(324)
plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1) + h_2(x_1)$")
plt.ylabel("$y$", fontsize=16, rotation=0)

plt.subplot(325)
plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label="$h_3(x_1)$", style="g-", data_style="k+")
plt.ylabel("$y - h_1(x_1) - h_2(x_1)$", fontsize=16)
plt.xlabel("$x_1$", fontsize=16)

plt.subplot(326)
plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$")
plt.xlabel("$x_1$", fontsize=16)
plt.ylabel("$y$", fontsize=16, rotation=0)

plt.show()
```

在第一行，集成只有一个树，因此它的预测与第一个树的预测一样；在第二行，一个新树在第一个树的残差上被训练，在右侧可以看到集成的预测等于前两个树的预测之和；同样，在第三行，另一个树在第二个树的残差上被训练。可以看到随着树被加到集成中，集成的预测逐渐变得更好。

可以使用Scikit-Learn的`GradientBoostingRegressor`类训练GBRT集成。和`RandomForestRegressor`非常相似，它也有控制决策树生长（the growth of Decision Trees）的超参数（例如`max_depth`、`min_samples_leaf`），以及控制集成训练（the ensemble training）的超参数，例如树的数量（`n_estimators`）。以下代码创建一个与之前相同的集成：

```python
from sklearn.ensemble import GradientBoostingRegressor

gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
gbrt.fit(X, y)
```

超参数`learning_rate`（学习率）控制每个树的贡献。如果将其设置为一个低值，例如0.1，则集成需要更多的树以拟合训练集，但是预测通常泛化更好。该正则化技术被称为**收缩（shrinkage）**。

下面展示使用低学习率训练的两个GBRT集成：左边的没有使用足够的树去拟合训练集；右边的使用过多的树且过拟合训练集：

```python
gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)
gbrt_slow.fit(X, y)
```

```python
fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)

plt.sca(axes[0])
plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="Ensemble predictions")
plt.title("learning_rate={}, n_estimators={}".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)
plt.xlabel("$x_1$", fontsize=16)
plt.ylabel("$y$", fontsize=16, rotation=0)

plt.sca(axes[1])
plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])
plt.title("learning_rate={}, n_estimators={}".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)
plt.xlabel("$x_1$", fontsize=16)

plt.show()
```

为了找到最优的树，可以使用早停法，一种实现方式是使用`staged_predict`方法：它返回集成在训练的每步（stage）的预测的迭代器。

以下代码训练一个包含120个树的GBRT集成，然后度量每步训练的验证误差以找到最优的树数量，并使用最优的树数量训练另一个GBRT集成。

> The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:

```python
from sklearn.metrics import mean_squared_error

X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)

gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)
gbrt.fit(X_train, y_train)

errors = [mean_squared_error(y_val, y_pred)
          for y_pred in gbrt.staged_predict(X_val)]
bst_n_estimators = np.argmin(errors) + 1

gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)
gbrt_best.fit(X_train, y_train)
```

下面绘制出验证误差（左边）与最好的模型（右边）：

```python
min_error = np.min(errors)
```

```python
plt.figure(figsize=(10, 4))

plt.subplot(121)
plt.plot(errors, "b.-")
plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], "k--")
plt.plot([0, 120], [min_error, min_error], "k--")
plt.plot(bst_n_estimators, min_error, "ko")
plt.text(bst_n_estimators, min_error*1.2, "Minimum", ha="center", fontsize=14)
plt.axis([0, 120, 0, 0.01])
plt.xlabel("Number of trees")
plt.ylabel("Error", fontsize=16)
plt.title("Validation error", fontsize=14)

plt.subplot(122)
plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])
plt.title("Best model (%d trees)" % bst_n_estimators, fontsize=14)
plt.ylabel("$y$", fontsize=16, rotation=0)
plt.xlabel("$x_1$", fontsize=16)

plt.show()
```

也可以通过提前停止训练来实现提前早停法（而不是训练很多数量的树然后回滚到最佳数量）。可以通过设置`warm_start=True`做到这一点，它会使得Scikit-Learn在调用`fit`方法时保持已有的树，从而允许增量训练。

以下代码在验证误差连续5次迭代没有改善时停止训练：

```python
gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)

min_val_error = float("inf")
error_going_up = 0
for n_estimators in range(1, 120):
    gbrt.n_estimators = n_estimators
    gbrt.fit(X_train, y_train)
    y_pred = gbrt.predict(X_val)
    val_error = mean_squared_error(y_val, y_pred)
    if val_error < min_val_error:
        min_val_error = val_error
        error_going_up = 0
    else:
        error_going_up += 1
        if error_going_up == 5:
            break
```

```python
print(gbrt.n_estimators)
```

```python
print("Minimum validation MSE:", min_val_error)
```

`GradientBoostingRegressor`类还支持`subsample`超参数，它指定训练每个树所使用的训练实例的比例。例如，如果`subsample=0.25`，则每个树在随机选取的25%的训练实例上被训练。该技术造成更大的偏差与更小的方差，它也显著加快训练速度。这被称为**随机梯度Boosting（Stochastic Gradient Boosting）**。

可以为梯度Boosting使用其他的损失函数，这通过`loss`超参数控制。

## Stacking

**stacking**（**[stacked generalization](https://homl.info/29)**的简写）集成方法训练一个模型去执行整合，而不是使用平凡函数（trivial function）（例如硬投票）去整合集成中所有预测器的预测。

下图展示了一个在新实例上执行回归任务的集成。底部的三个预测器每个预测不同的值（3.1、2.7与2.9），最后一个预测器（称作一个**blender**或一个**meta learner**）将这三个预测作为输入并作出最终预测（3.0）。

![使用blending预测器整合预测](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用blending预测器整合预测.png)

为了训练一个blender，一个常见的方法是使用留出集（hold-out set）。首先，训练集被分成两个子集，第一个子集用于训练第一层的预测器（如下图）。


![训练第一层](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\训练第一层.png)

然后，第一层的预测器在第二个子集上（second (held-out) set）作出预测。这保证了预测是“干净的”，因为预测器在训练过程中没有看到这些实例。对于每个留出集中的实例都有三个预测值。可以将这些预测值作为输入特征（features）创建一个新的训练集（如果第一层有三个预测器，那么新的训练集是3D的），并保持目标值不变。blender在这个新的训练集上被训练，所以它能在给定第一层的预测的情况下，学习预测目标值。


![训练一个blender](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\训练一个blender.png)

实际上，可以通过该方法训练若干不同的blender（例如，一个使用线性回归，一个使用随机森林回归），以得到一整层的blender。诀窍（trick）是将训练集分为三个子集：第一个用于训练第一层，第二个用于创建一个训练集并用其训练第二层（使用第一层预测器所作的预测），第三个用于创建一个训练集并用其训练第三层（使用第二层预测器所作的预测）。一旦以上步骤完成，就可以通过顺序通过（go through）每层来为新实例作出预测（如下图所示）。

![多层stacking集成的预测](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\多层stacking集成的预测.png)

Scikit Learn不直接支持stacking，但是实现自己的stacking不难。可以使用开源实现，例如[DESlib](https://github.com/Menelau/DESlib)。

# <a name="(降维)">降维</a>

许多机器学习问题的每个实例涉及大量的特征，这些特征不仅使得训练很慢，而且使得找到好的解变得更加困难。该问题被称为**维度灾难（curse of dimensionality）**。

> Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only do all these features make training extremely slow, but they can also make it much harder to find a good solution, as we will see. This problem is often referred to as the *curse of dimensionality*.

![0维到4维超立方体](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\0维到4维超立方体.png)

> Watch a rotating tesseract projected into 3D space at *https://homl.info/30*. Image by Wikipedia user NerdBoy1392 (Creative Commons BY-SA 3.0). Reproduced from *https://en.wikipedia.org/wiki/Tesseract*. 

随着维度增大，超立方体内随机选取的一个点接近边缘的概率不断增大（接近1），超立方体内随机选取的两个点的距离不断增大。因此，高维数据集容易变得十分稀疏：大多数训练实例可能彼此远离对方，这也意味着新实例可能远离任意训练实例，使得预测的可靠性远小于低维情况下的预测。简而言之，训练集的维度越高，过拟合风险越大。

理论上，解决维度灾难的一个方法是增加训练集的大小，直到训练实例的密度足够大。但是在实践中，达到给定密度（密度可以用实例之间的距离度量）所需的训练实例的数量随着维度指数级增加。

在现实问题中，特征的数量通常可以显著减少，使得一个棘手（intractable）的问题转变为一个可处理（tractable）问题。例如，对于MNIST图像，位于图像边缘的像素几乎总是白色的，因此可以将这些像素从训练集中完全去除，且不会丢失太多信息（集成学习例子中绘制的每个的像素重要度证实了这些像素对分类任务完全不重要）。另外，两个相邻的像素通常高度相关，如果将它们合并为单个像素（例如取两个像素灰度的均值），也不会丢失大量信息。

当然，降维确实会造成一些信息丢失，因此即使它能加速训练，它也会使得系统表现稍微差些，它也会使得管道稍微复杂一些，因此更难维护。因此，如果训练太慢，首先应该在考虑使用降维前使用原始数据训练系统。在某些情况下，训练数据降维可以过滤一些噪音与不必要的细节，导致表现更好。但是通常这不会发生，它只能加速训练。

除了加速训练外，降维对数据可视化（data visualization，或DataViz）十分有用。将维度降到2或3使得在图中绘制高维训练集的精简视图成为可能，通过视觉检测模式（例如簇）常常可以获得一些重要见解。

## 降维的主要方法

降维的两种主要方法是：投影（projection）与流形学习（Manifold Learning）。

### 投影

在大多数现实问题中，训练实例并不是均匀地分布在所有维度上。许多特征几乎不变，而其他特征则高度相关（正如前面针对MNIST所讨论的那样）。因此，所有训练实例位于（或接近）高维空间的一个低维子空间中。

![接近二维子空间的三维数据集](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\接近二维子空间的三维数据集.png)

在上图中，所有训练实例都接近一个平面：这是高维（3维）的一个低维（2维）子空间。如果将每个训练实例垂直投影到这个子空间中（如连接实例到平面的短线所示），则得到下图所示的2维数据集。注意，轴对应于新特征$z_1$和$z_2$（平面上投影的坐标）。

![投影后新的2D数据集](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\投影后新的2D数据集.png)

但是，投影并不总是最好的降维方法。在很多情况下，子空间可能扭曲（twist）与转动（turn），例如瑞士卷玩具数据集（Swiss roll toy dataset）:

![瑞士卷数据集](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\瑞士卷数据集.png)

简单投影到平面上会将不同层的Swiss roll挤压到一起（如左图所示），而实际想要的是展开瑞士卷，以得到右图所示的2维数据集：


![通过投影到平面挤压（左）与展开瑞士卷（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\通过投影到平面挤压（左）与展开瑞士卷（右）.png)

### 流形学习

瑞士卷是2维**流形（manifold）**的例子。一个2维流形就是一个2维形状，该形状可以在高维空间中被弯曲（bent）与扭曲。更一般地，一个$d$维流形是一个$n$维空间（其中$d<n$）空间的一部分，其局部类似于一个$d$维超平面。就瑞士卷而言，$d=2$且$n=3$：它局部类似于一个2维平面，但是在第三维中卷曲。

许多降维算法都是通过对训练实例所在的流形进行建模来实现的，这被称作**流形学习（Manifold Learning）**。它依赖于**流形假设（manifold assumption，manifold hypothesis）**，该假设认为大多数真实世界中的高维数据集位于一个低维流形附近。这个假设常常能由经验观察得到。

例如，对于MNIST数据集，所有的手写字体图像都有一些相似处：它们都由连接线（connected lines）组成，边界是白色的，它们或多或少是居中的。如果随机产生一些图像，只有极少数看起来像是手写字体。换言之，如果尝试创建数字图像，则可用的自由度远远低于允许生成任何所需图像时的自由度。这些约束倾向于将数据集压缩（squeeze）到低维流形中。

流形假设通常伴随着（accompanied by）另一个隐式假设：如果手头工作（如分类或回归）如果在流形的低维空间中表示，则它们会更简单。例如下图顶行的瑞士卷被分成两个类别：在3维空间中（左边），决策边界相当复杂，但是在2维展开流形空间（2D unrolled manifold space）中（右边），决策边界是一条直线。

但是这一假设并不总是成立。例如，在下图底行中，决策边界在$x_1=5$处，这个决策边界在3维空间中非常加单（一个垂直平面），但是在展开的流形中，它看上去更复杂（四条独立线段的集合）。

![在低维空间中，决策边界并不总是更简单](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\在低维空间中，决策边界并不总是更简单.png)

简言之，训练模型前对训练集降维通常能加速训练，但是并不总是能导致更好的或更简单的解决方案，而是依赖于数据集。

## 降维算法

### PCA

**主成分分析（Principal Component Analysis，PCA）**是目前最流行的降维算法。首先它识别出最接近数据的超平面，然后将数据投影到其上。

下面绘制一个简单的2D数据集，以及三个不同的轴（axes）（即1维超平面）（左），以及数据集在每个轴的投影结果（右）：

可以看到，在实线上的投影能够保持最大方差，在点线上的投影保持很小的方差，在虚线上的投影保持中等数量的方差（intermediate amount of variance）。

```python
import numpy as np
import matplotlib.pyplot as plt

angle = np.pi / 5
stretch = 5
m = 200

np.random.seed(3)
X = np.random.randn(m, 2) / 10
X = X.dot(np.array([[stretch, 0],[0, 1]]))  # stretch
X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]])  # rotate

u1 = np.array([np.cos(angle), np.sin(angle)])
u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])
u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])

X_proj1 = X.dot(u1.reshape(-1, 1))
X_proj2 = X.dot(u2.reshape(-1, 1))
X_proj3 = X.dot(u3.reshape(-1, 1))

plt.figure(figsize=(8,4))
plt.subplot2grid((3,2), (0, 0), rowspan=3)
plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], "k-", linewidth=1)
plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], "k--", linewidth=1)
plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], "k:", linewidth=2)
plt.plot(X[:, 0], X[:, 1], "bo", alpha=0.5)
plt.axis([-1.4, 1.4, -1.4, 1.4])
plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')
plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')
plt.text(u1[0] + 0.1, u1[1] - 0.05, r"$\mathbf{c_1}$", fontsize=22)
plt.text(u3[0] + 0.1, u3[1], r"$\mathbf{c_2}$", fontsize=22)
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$x_2$", fontsize=18, rotation=0)
plt.grid(True)

plt.subplot2grid((3,2), (0, 1))
plt.plot([-2, 2], [0, 0], "k-", linewidth=1)
plt.plot(X_proj1[:, 0], np.zeros(m), "bo", alpha=0.3)
plt.gca().get_yaxis().set_ticks([])
plt.gca().get_xaxis().set_ticklabels([])
plt.axis([-2, 2, -1, 1])
plt.grid(True)

plt.subplot2grid((3,2), (1, 1))
plt.plot([-2, 2], [0, 0], "k--", linewidth=1)
plt.plot(X_proj2[:, 0], np.zeros(m), "bo", alpha=0.3)
plt.gca().get_yaxis().set_ticks([])
plt.gca().get_xaxis().set_ticklabels([])
plt.axis([-2, 2, -1, 1])
plt.grid(True)

plt.subplot2grid((3,2), (2, 1))
plt.plot([-2, 2], [0, 0], "k:", linewidth=2)
plt.plot(X_proj3[:, 0], np.zeros(m), "bo", alpha=0.3)
plt.gca().get_yaxis().set_ticks([])
plt.axis([-2, 2, -1, 1])
plt.xlabel("$z_1$", fontsize=18)
plt.grid(True)

plt.show()
```

因此[PCA](https://homl.info/pca)的思想是：选择能够保持最大方差的轴并将数据投影到该轴上，因为相比于其他投影方法，它最有可能丢失更少的信息。另一种证明这种选择的方法是：选择一个轴，它能够最小化原始数据集以及它到该轴的投影的平均平方距离。

PCA首先识别出训练集中可获得最大方差的轴。在上图中，它是一个实线。然后它找到第二个轴，它与第一个轴正交，且可获得最大的剩余方差。在上图中，它只能是点线，但是对于更高维的数据集，PCA还会找到第三个轴，与前两个轴都正交，以及第四个、第五个轴，等等，一直找到与数据集中维度数的轴数。

第$i$个轴被称为该数据的第$i$个**主成分（principal component，PC）**。在上图中，第一个主成分是向量$\pmb{c}_1$位于的轴，第二个主成分是向量$\pmb{c}_2$位于的轴。在图“接近二维子空间的三维数据集”中，前两个主成分两个箭头位于的正交轴，第三个PC是与该平面正交的轴。

对于每个主成分，PCA会找到指向主成分方向的零均值化的单位向量。由于两个相反的单位向量位于同一个轴上，因此PCA返回的单位向量的方向不确定：如果轻微扰动数据集并重新运行PCA，则单位向量可能指向与原始向量相反的方向。但是，它们通常仍位于同一轴上。在某些情况下，一对单位向量可能旋转（rotate）或交换（swap）（如果沿着两个轴的方差接近的话），但是它们定义的平面通常保持不变。

对于训练集矩阵$\pmb{X}$，**奇异值分解（Singular Value Decomposition，SVD）**可以将其分解为三个矩阵的乘积：$\pmb{U}\pmb{\Sigma}\pmb{V}^T$，其中$\pmb{V}$包含定义所有主成分的单位向量：
$$
\pmb{V}=\begin{pmatrix}
|&|&...&|\\
\pmb{c_1}&\pmb{c_2}&...&\pmb{c_n}\\
|&|&...&|
\end{pmatrix}
$$
以下代码使用NumPy的`svd`函数获取训练集的所有主成分，然后提取出定义前两个主成分的单位向量。注意，PCA假定数据集以原点为中心，Scikit-Learn的PCA类已考虑到居中数据。如果要自己实现PCA，或使用其他的库，则不要忘记首先居中数据。

```python
np.random.seed(4)
m = 60
w1, w2 = 0.1, 0.3
noise = 0.1

angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5
X = np.empty((m, 3))
X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2
X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2
X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)
```

```python
X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt.T[:, 0]
c2 = Vt.T[:, 1]
```

一旦识别出所有的主成分，就可以将数据集投影到前$d$个主成分定义的超平面，从而该数据集降维到$d$维。选择这个超平面保证了投影会保持尽可能大的方差。例如，在图“接近二维子空间的三维数据集”中，3维数据集投影到前两个主成分定义的2维平面上，保持了数据集的大部分方差（a large part of the dataset's variance）。结果，2维投影看起来非常像原始的3维数据集。

下式将训练集投影到超平面并获得简化的（reduced）$d$维数据集$X_{d-proj}$，其中$\pmb{X}$表示训练集矩阵，$\pmb{W}_d$表示包含$\pmb{V}$的前$d$列：
$$
\pmb{X}_{d-proj}=\pmb{X}\pmb{W}_d
$$
以下代码将训练集投影到前两个主成分定义的平面上：

```python
W2 = Vt.T[:, :2]
X2D = X_centered.dot(W2)
```

Scikit-Learn的PCA类使用SVD分解（decomposition）去实现PCA（就像前面所做的那样）。以下代码应用PCA将数据集降维到2维（注意它自动考虑数据居中）：

> note that it automatically takes care of centering the data

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```

拟合PCA转换器后，它的`components_`属性保存$W_d$的转置。

```python
pca.components_
```

#### 方差解释率

主成分的**方差解释率（explained variance ratio）**指示数据集沿该主成分的方差比例。

> The ratio indicates the proportion of the dataset’s variance that lies along each principal component.

通过`explained_variance_ratio_`变量获得前两个主成分的方差解释率：

```python
pca.explained_variance_ratio_
```

可以看到84.2%的数据集方差位于第一个主成分上，14.6%位于第二个主成分上，这使得第三个主成分的方差解释率不到1.2%，因此有理由认为第三个主成分携带很少的信息。

#### 选择正确数量的维度

与其任意选择要减少到的维度，更简单的选择维度方法是选择加起来到方差解释率能够达到足够占比（例如 95%）的维度的数量。除非想要降维实现可视化，此时需要将维度降到2维或3维。

以下代码执行PCA且不执行降维，然后计算出保留训练集方差的95%所需的最小维数：

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1, as_frame=False)
mnist.target = mnist.target.astype(np.uint8)
```

```python
from sklearn.model_selection import train_test_split

X = mnist["data"]
y = mnist["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y)
```

```python
pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
```

绘制出方差解释率关于维度的函数（简单地绘制`cumsum`即可）：

```python
plt.figure(figsize=(6,4))
plt.plot(cumsum, linewidth=3)
plt.axis([0, 400, 0, 1])
plt.xlabel("Dimensions")
plt.ylabel("Explained Variance")
plt.plot([d, d], [0, 0.95], "k:")
plt.plot([0, d], [0.95, 0.95], "k:")
plt.plot(d, 0.95, "ko")
plt.annotate("Elbow", xy=(65, 0.85), xytext=(70, 0.7),
             arrowprops=dict(arrowstyle="->"), fontsize=16)
plt.grid(True)
plt.show()
```

通常，曲线会有一个肘部（elbow），在这里方差解释率停止快速增长。在此例中，可以看到将维度降到大约100不会丢失太多的方差解释率。

接下来可以设置`n_components=d`并重新运行PCA。但是更好的选择是：设置`n_components`为0.0到1.0之间的某个浮点数，指示想要保留的方差比率（而不是指定想要保留的主成分数量）。

```python
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)
```

```python
pca.n_components_
```

```python
np.sum(pca.explained_variance_ratio_)
```

#### PCA压缩（PCA for Compression）

降维后，训练集占用的空间会少得多。因此PCA可用于数据压缩，且会加快算法速度。

也可以通过PCA投影的逆变换，将降维后的数据集解压到原来大小。由于部分信息丢失，这不会得到原始的数据，但是可能接近原始数据。原始数据与重构（即压缩再解压）数据之间的均方距离被称为**重构误差（reconstruction error）**。

逆变换方程如下：
$$
\pmb{X}_{recovered}=\pmb{X}_{d-proj}\pmb{W}_d^T
$$
当对MNIST数据集应用PCA且保留95%的方差解释率后，可以发现每个实例只有154个特征，而不是原来的784个特征。因此，大部分的方差得以保留，且数据集的大小不到原来大小的20%。这是一个合理的（reasonable）压缩率，且会大大加速分类算法（例如SVM）。

下面将MNIST数据集压缩到154维，然后使用` inverse_transform`将它解压回784维：

```python
pca = PCA(n_components=154)
X_reduced = pca.fit_transform(X_train)
X_recovered = pca.inverse_transform(X_reduced)
```

下面展示一些来自原始数据集的数字（左边）与对应的压缩并解压后的数字：

```python
def plot_digits(instances, images_per_row=5, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size,size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row : (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)
    plt.imshow(image, cmap = mpl.cm.binary, **options)
    plt.axis("off")
```

```python
import matplotlib as mpl

plt.figure(figsize=(7, 4))
plt.subplot(121)
plot_digits(X_train[::2100])
plt.title("Original", fontsize=16)
plt.subplot(122)
plot_digits(X_recovered[::2100])
plt.title("Compressed", fontsize=16)
```

可以看到有轻微的图像质量损失，但是数字仍然基本完好。

#### 随机PCA

如果设置超参数`svd_solver=randomized`，则Scikit-Learn使用被称为**随机PCA（Randomized PCA）**的随机（stochastic）算法，它可以快速找到前$d$个主成分的近似值。它的计算复杂度为$O(m\times d^2)+O(d^3)$，而不是SVD方法的$O(m\times n^2)+O(n^3)$。所以当$d$远小于$n$时，它的速度远快于SVD。

下面使用随机PCA：

```python
rnd_pca = PCA(n_components=154, svd_solver="randomized", random_state=42)
X_reduced = rnd_pca.fit_transform(X_train)
```

默认情况下，`svd_solver`被设置为`auto`：如果$m$或$n$大于500且$d$小于$m$或$n$的80%，Scikit-Learn使用随机PCA；否则使用完整的（full）SVD方法。如果要强制Scikit-Learn使用完整的SVD，则可设置超参数`svd_solver=full`。

#### 增量PCA

前面的PCA实现的一个问题是它们要求内存容纳整个训练集以便算法运行。**增量PCA（Incremental PCA，IPCA）**算法允许将训练集划分为多个mini-batch，每次向IPCA算法输入一个mini-batch。当训练集很大或需要在线应用PCA（即在新实例到达时即时运行）时这很有用。

> They allow you to split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time.

以下代码将MNIST数据集分为100个mini-batch，然后将其输入Scikit-Learn的[`IncrementalPCA`类](https://homl.info/32)将MNIST数据集降维到154维。注意，必须对每个mini-batch调用`partial_fit`而不是对整个训练集调用`fit`方法：

```python
from sklearn.decomposition import IncrementalPCA

n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
for X_batch in np.array_split(X_train, n_batches):
    print(".", end="")
    inc_pca.partial_fit(X_batch)

X_reduced = inc_pca.transform(X_train)
```

```python
X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)
```

```python
plt.figure(figsize=(7, 4))
plt.subplot(121)
plot_digits(X_train[::2100])
plt.subplot(122)
plot_digits(X_recovered_inc_pca[::2100])
plt.tight_layout()
```

也可以使用Numpy的`memmap`类，它允许操作存储在磁盘上的二进制文件中的大型数组，就好像它完全在内存中一样；该类仅在需要时加载内存中所需的数据。既然`IncrementalPCA`类在任意给定时间只使用数组的一小部分，内存使用仍受控制。这使得调用`fit`方法成为可能：

> the class loads only the data it needs in memory, when it needs it. 

```python
filename = "my_mnist.data"
m, n = X_train.shape

X_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))
X_mm[:] = X_train
```

```python
del X_mm  # 删除memmap，触发Python回收器（finalizer），回收器保证数据保存到磁盘中。
```

```python
X_mm = np.memmap(filename, dtype="float32", mode="readonly", shape=(m, n))

batch_size = m // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mm)
```

### LLE

[**局部线性嵌入（LLE）**](https://homl.info/lle)是另一个强大的**非线性降维（nonlinear dimensionality reduction，NLDR）**技术。和前一个算法一样，它是流形学习技术，不依赖于投影。LLE首先计算每个实例如何线性关联它的最近邻居（c.n.），然后找到训练集的一个低维表示，在该表示中这些局部关系得以保留。该方法特别适合展开扭曲的流形，尤其在没有太多噪音的时候。

> In a nutshell, LLE works by first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly).  This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much
>
LLE分为两步：

第一步，对局部关系线性建模：对于每个训练实例$\pmb{x}^{(i)}$，算法识别出它的$k$个最近邻居，然后重构出$\pmb{x}^{(i)}$关于这些邻居的非线性函数。具体来说，它找到权重$w_{i,j}$使得$\pmb{x}^{(i)}$与$\sum_{j=1}^mw_{i,j}\pmb{x}^{(j)}$之间的平方距离尽可能小（如果$\pmb{x}^{(j)}$不是$\pmb{x}^{(i)}$的$k$个最近邻居之一则$w_{i,j}=0$）。该约束优化问题描述如下（其中$\pmb{W}$为包含所有权重$w_{i,j}$的权重矩阵），其中第二个约束简单地归一化（normalize）每个训练实例$\pmb{x}^{(i)}$的权重：

$$
\widehat{\pmb{W}}=\underset{\pmb{W}}{argmin}\sum_{i=1}^m(\pmb{x}^{(i)}-\sum_{j=1}^mw_{i,j}\pmb{x}^{(j)})^2\\
subject\ to\left\{
\begin{aligned}
w_{i,j}=0\ if\ \pmb{x}^{(j)}\ is\ not\ one\ of\ the\ k\ c.n.\ of\ \pmb{x}^{(i)}\\
\sum_{j=1}^mw_{i,j}=1\ for\ i=1,2,...,m
\end{aligned}
\right.
$$

执行该步后，权重矩阵$\widehat{\pmb{W}}$编码训练实例间的局部线性关系（local linear relationships）。第二步将训练实例映射到$d(d<n)$维空间并尽可能保留这些局部关系。如果$\pmb{z}^{(i)}$为$\pmb{x}^{(i)}$在该$d$维空间的图像（image），则我们希望$\pmb{z}^{(i)}$与$\sum_{j=1}^m\widehat{w}_{i,j}\pmb{z}^{(j)}$之间的距离越小越好。该思想引出了如下的非约束优化问题（与第一步很相似，但是保持权重不变并在低维空间中找到实例图像的最优位置，而不是保持实例不变并找到最优权重）：
$$
\widehat{\pmb{Z}}=\underset{\pmb{Z}}{argmin}\sum_{i=1}^m(\pmb{z}^{(i)}-\sum_{j=1}^m\widehat{w}_{i,j}\pmb{z}^{(j)})^2
$$
Scikit-Learn的LLE实现（implementation）的计算复杂度如下：找到最近的$k$个邻居：$O(m\log(m)n\log(k))$；最优化权重：$O(mnk^3)$；重构低维表示（representation）：$O(dm^2)$。最后一个计算复杂度导致算法不能很好地扩展到大规模数据集上。

下面使用Scikit-Learn的`LocallyLinearEmbedding`类去展开（unroll）瑞士卷。

```python
from sklearn.datasets import make_swiss_roll

X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)
```

```python
from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_reduced = lle.fit_transform(X)
```

下面绘制出得到的2维数据集：

```python
import matplotlib.pyplot as plt

plt.title("Unrolled swiss roll using LLE", fontsize=14)
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
plt.xlabel("$z_1$", fontsize=18)
plt.ylabel("$z_2$", fontsize=18)
plt.axis([-0.065, 0.055, -0.1, 0.12])
plt.grid(True)

plt.show()
```

可以看到，瑞士卷完全展开，实例之间的距离被很好地局部保留。但是，在更大尺度上，距离没有保留：展开的瑞士卷的左半部分被拉伸，而右半部分被挤压。尽管如此，LLE仍然能很好地对流形建模。

# 无监督学习技术

虽然如今大多数机器学习应用基于监督学习（因此，大部分的投资也作用于此），但是大量的可用数据都是无标签的：我们有输入特征$\pmb{X}$但是没有标签$\pmb{y}$。无监督学习潜力巨大，而我们才在起步阶段。

很多时候，获取大量数据比较简单、快速，但是为这些数据打标签是非常耗时、昂贵且乏味，因此通常只为可用数据的一个小的子集打标签。因此，有标签的数据集很小，导致有监督学习预测器的性能不够好；另外，如果需求发生变化，需要为新的数据打标签，整个过程需要从头开始。如果该算法可以只利用无标签的数据，而不用人工标记（label）每张图片，将会是一件很好的事，这正是无监督学习的任务。

## 聚类

聚类的任务是识别出相似的实例并将为它们分配**簇（cluster）**。在该任务中，我们不知道每个实例的标签，但是可以将相似实例分为若干组。与分类类似，每个实例被分配到一个组；与分类不同，聚类是无监督任务。例如下面代码绘制了两张图，左边是iris数据集，每个实例的种类（species）用不同的标记（marker）表示，它是有标签的数据集，分类算法很适合它。右边是相同的数据集，但是没有标签，因此不能使用分类算法。这是聚类算法发挥作用的地方：很多聚类算法可以很容易地检测出左下角的簇，人眼也能很容易地看出这一点。但是不太容易发现右上角的簇由两个不同的子簇组成。也就是说，数据集有两个额外的特征（萼片长度和宽度），这里没有表示，而聚类算法可以很好地利用所有特征，因此事实上它们可以很好地识别三个聚类（例如，使用高斯混合模型，150个实例中只有5个被分配到错误的簇）。

> This is called *clustering*: it is the task of identifying similar instances and assigning them to *clusters*, or groups of similar instances.

> many of them can easily detect the lower-left cluster. It is also quite easy to see with our own eyes, but it is not so obvious that the upper-right cluster is composed of two distinct sub-clusters. That said, the dataset has two additional features (sepal length and width), not represented here, and clustering algorithms can make good use of all features, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mixture model, only 5 instances out of 150 are assigned to the wrong cluster).

```python
from sklearn.datasets import load_iris
```

```python
data = load_iris()
X = data.data
y = data.target
data.target_names
```

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(9, 3.5))

plt.subplot(121)
plt.plot(X[y==0, 2], X[y==0, 3], "yo", label="Iris setosa")
plt.plot(X[y==1, 2], X[y==1, 3], "bs", label="Iris versicolor")
plt.plot(X[y==2, 2], X[y==2, 3], "g^", label="Iris virginica")
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(fontsize=12)

plt.subplot(122)
plt.scatter(X[:, 2], X[:, 3], c="k", marker=".")
plt.xlabel("Petal length", fontsize=14)
plt.tick_params(labelleft=False)

plt.show()
```

使用高斯混合模型聚类：

```python
from sklearn.mixture import GaussianMixture
```

```python
y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)
```

将每个簇映射到一个类别（为每个簇选择最常见的类别，而不是硬编码映射）：

```python
import numpy as np
from scipy import stats

mapping = {}
for class_id in np.unique(y):
    mode, _ = stats.mode(y_pred[y==class_id])
    mapping[mode[0]] = class_id

mapping
```

```python
y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])
```

查看结果：

```python
plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], "yo", label="Cluster 1")
plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], "bs", label="Cluster 2")
plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], "g^", label="Cluster 3")
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=12)
plt.show()
```

```
np.sum(y_pred==y)
```

```
np.sum(y_pred==y) / len(y_pred)
```

### K-Means

K-Means算法是1957年Stuart Lloyd在贝尔实验室提出了一种脉冲编码调制技术，但[1982年](https://homl.info/36)才在公司外发表。在1965年，Edward W.forgy发表了几乎相同的算法，所以K-Means有时也被称为Lloyd-Forgy。

K-Means算法要求预先指定簇的数量$k$，然后算法依次执行如下过程：

1. 随机为每个簇选择一个质心（centroid）（例如，随机选择$k$个实例，将它们的位置作为质心）。
2. 将每个实例分配给最近的质心所在的簇（即标记（label）实例）。
3. 根据每个簇内的实例，更新该簇的质心，然后返回步骤2。

当质心不再移动时，K-Means算法停止。该算法保证经过有限步（通常步骤相当少）收敛（因为实例与它们最近的质心之间的均方距离以及最近质心只能在每步减少）。

训练完成后，K-Means算法将实例分配给最近的质心所在的簇。

K-Means算法的计算复杂度通常与实例数量$m$、簇数量$k$与维度数量$n$成线性关系。但是，这只在数据有簇结构（clustering structure）时成立。否则，在最坏的情况下，计算复杂度可以随着实例数量增长而指数级增长。在实践中，这很罕见，且K-Means算法通常是最快的聚类算法之一。

> If it does not, then in the worst-case scenario the complexity can increase exponentially with the number of instances.

下面演示K-Means的使用。

首先生成一个无标签数据集，包含五个实例块（five blobs of instances）：

```python
from sklearn.datasets import make_blobs
```

```python
blob_centers = np.array(
    [[ 0.2,  2.3],
     [-1.5 ,  2.3],
     [-2.8,  1.8],
     [-2.8,  2.8],
     [-2.8,  1.3]])
blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])
```

```python
X, y = make_blobs(n_samples=2000, centers=blob_centers,
                  cluster_std=blob_std, random_state=7)
```

```python
def plot_clusters(X, y=None):
    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)
    plt.xlabel("$x_1$", fontsize=14)
    plt.ylabel("$x_2$", fontsize=14, rotation=0)
```

```python
plt.figure(figsize=(8, 4))
plot_clusters(X)
plt.show()
```

K-Means算法是一个简单的算法，可以快速、高效地对这种数据集聚类，通常只需要几次（a few）迭代即可。下面在该数据集上训练一个K-Means聚类器（clusterer）：

```python
from sklearn.cluster import KMeans
```

```python
k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)
```

每个实例都被分到5个簇中的一个。在聚类中，每个实例的`label`是算法给该实例分配的簇索引（注意它与分类中的类别标签不同）。`KMeans`通过`labels_`实例变量保留它所训练的实例的标签副本。

```python
y_pred
```

```python
y_pred is kmeans.labels_
```

查看算法发现的5个质心（centroids）：

```python
kmeans.cluster_centers_
```

很容易将新实例分配给最近的质心所在的簇：

```python
X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
kmeans.predict(X_new)
```

<a name="(无监督学习)(聚类)(K-Means)(1)">绘制决策边界，得到一个Voronoi tessellation</a>。

```python
def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)

def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='o', s=35, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=2, linewidths=12,
                color=cross_color, zorder=11, alpha=1)

def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
```

```python
plt.figure(figsize=(8, 4))
plot_decision_boundaries(kmeans, X)
plt.show()
```

绝大多数实例明显被分到了合适的（appropriate）簇，但是少数实例可能被错分（mislabeled），尤其是接近左上角与中间的簇之间的边界的实例。事实上，当块（blobs）的直径非常不同时，K-Means算法表现不会很好，因为它在分配实例到簇时只考虑实例到质心的距离。

K-Means算法可以将实例分配给一个单独的簇，这被称为**硬聚类（hard clustering）**，也可以针对每个实例给每个簇一个得分，这被称为**软聚类（soft clustering）**。得分可以是实例与质心之间的距离，也可以是相似度得分，例如Gaussian Radial Basis Function。

`KMeans`类的`transform`方法度量每个实例到每个质心的距离（软聚类）：

```python
kmeans.transform(X_new)
```

下面绘制出K-Means算法前3次迭代的质心变化：

```python
kmeans_iter1 = KMeans(n_clusters=5, init="random", n_init=1,
                     algorithm="full", max_iter=1, random_state=0)
kmeans_iter2 = KMeans(n_clusters=5, init="random", n_init=1,
                     algorithm="full", max_iter=2, random_state=0)
kmeans_iter3 = KMeans(n_clusters=5, init="random", n_init=1,
                     algorithm="full", max_iter=3, random_state=0)
kmeans_iter1.fit(X)
kmeans_iter2.fit(X)
kmeans_iter3.fit(X)
```

```python
plt.figure(figsize=(10, 8))

plt.subplot(321)
plot_data(X)
plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')
plt.ylabel("$x_2$", fontsize=14, rotation=0)
plt.tick_params(labelbottom=False)
plt.title("Update the centroids (initially randomly)", fontsize=14)

plt.subplot(322)
plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)
plt.title("Label the instances", fontsize=14)

plt.subplot(323)
plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)
plot_centroids(kmeans_iter2.cluster_centers_)

plt.subplot(324)
plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)

plt.subplot(325)
plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)
plot_centroids(kmeans_iter3.cluster_centers_)

plt.subplot(326)
plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)

plt.show()
```

首先随机初始化实例（左上角），然后标记实例（右上角），然后质心被更新（中间左），然后实例被重新标记（relabeled）（中间右），等等。可以看到，仅仅三次迭代，算法已经找到了接近最优解的簇。

（原始的）K-Means虽然保证收敛，但是未必会收敛到正确的解（即可能收敛到一个局部最优解），这取决于如何初始化质心。该方法的一个主要问题在于：当多次调用K-Means或为K-Means使用不同的随机数种子时，它会收敛到非常不同的解。

<a name="(无监督学习)(聚类)(K-Means)(2)">下面显示两个某些初始化值会导致（原始的）K-Means算法收敛到次优解</a>：

```python
def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):
    clusterer1.fit(X)
    clusterer2.fit(X)

    plt.figure(figsize=(10, 3.2))

    plt.subplot(121)
    plot_decision_boundaries(clusterer1, X)
    if title1:
        plt.title(title1, fontsize=14)

    plt.subplot(122)
    plot_decision_boundaries(clusterer2, X, show_ylabels=False)
    if title2:
        plt.title(title2, fontsize=14)
```

```python
kmeans_rnd_init1 = KMeans(n_clusters=5, init="random", n_init=1,
                         algorithm="full", random_state=2)
kmeans_rnd_init2 = KMeans(n_clusters=5, init="random", n_init=1,
                         algorithm="full", random_state=5)

plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,
                          "Solution 1", "Solution 2 (with a different random init)")

plt.show()
```

下面是一些通过改进质心初始化来降低这种风险的几种方法。

> Let’s look at a few ways you can mitigate this risk by improving the centroid initialization.

#### 质心初始化方法

如果碰巧知道质心的大概位置（例如之前运行过另一个聚类算法），则可以使用设置`init`超参数为包含质心列表的Numpy数组，并设置`n_init=1`：

```python
good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
kmeans.fit(X)
```

另一个方法是使用不同的随机初始值（random initialization）运行算法多次，并选择最好的解。随机初始值的数量由超参数`n_init`控制，默认为10，因此当调用`fit`方法时之前的算法运行10次，然后Scikit-Learn选择最优解。**惰性（inertia）**为判定解的好坏的性能指标，它表示每个实例到离其最近的质心的均方距离。对于[图2](#(无监督学习)(聚类)(K-Means)(2))的左边模型，它大约是223.3；对于[图2](#(无监督学习)(聚类)(K-Means)(2))的右边模型，它大约是237.5；对于[图1](#(无监督学习)(聚类)(K-Means)(1))中的模型，它大约是211.6。`KMeans`类运行算法`n_init`次，并选择inertia最低的模型。在该例中，它选择[图1](#(无监督学习)(聚类)(K-Means)(1))的模型。

模型的惰性可通过`inertia_`实例变量获取：

```python
kmeans.inertia_
```

`score`方法返回负inertia（因为预测器的`score`方法必须遵循Scikit-Learn的“越大越好”的规则：如果一个预测器比其他的好，则它的`score`方法返回一个更大的得分）：

```python
kmeans.score(X)
```

David Arthur与Sergei Vassilvitskii在[2006年的一篇论文](https://homl.info/37)中提出对K-Means算法的一个重要改进：**K-Means++**。他们引入了一个更智能（smarter）的初始化步骤，它倾向于选择彼此远离的质心，这大大减小了K-Means算法收敛到次优解的可能性。他们表明该更智能的初始化步骤带来的额外计算是值得的，因为它使得算法找到最优解需要的运行次数大幅度减少。K-Means++初始化算法如下：

- 从数据集中随机选择一个质心$\pmb{c}^{(1)}$。
- 选择一个实例$\pmb{x}^{(i)}$作为新的质心$\pmb{c}^{(i)}$，其中$\pmb{x}^{(i)}$被选择的概率等于$D(\pmb{x}^{(i)})^2/\sum_{j=1}^mD(\pmb{x}^{(j)})^2$，其中$D(\pmb{x}^{(i)})$表示实例$\pmb{x}^{(i)}$到离其最近的已经选择的质心的距离。
- 重复上述步骤直到找到$k$个质心。

`KMeans`类默认使用该初始化方法。如果要强制其使用原始方法（随机选择$k$个实例以定义初始质心），则可以设置超参数`init`为`random`。极少需要这么做。

#### 加速K-Means与小批量K-Means

Charles Elkan在[2003年的一篇论文](https://homl.info/38)中提出了对K-Means算法的另一个重要改进。它通过避免大量不必要的距离计算，大大加快了算法的速度。这是`KMeans`类默认使用的算法（如果要强制其使用原始算法，则可以设置超参数`algorithm`为`full`，当然几乎肯定不需要这么做）。

> Elkan achieved this by exploiting the triangle inequality (i.e., that a straight line is always the shortest distance between two points5) and by keeping track of lower and upper bounds for distances between instances and centroids.

David Sculley在[2010年的一篇论文](https://homl.info/39)中提出了K-Means算法的另一个重要变体。算法在每次迭代可以使用小批量（而不是整个数据集）将质心稍微移动。这通常会将算法的速度提高3\~4倍且使得对不能容纳进内存的大规模数据集进行聚类称为可能。Scikit-Learn在`MiniBatchKMeans`类中实现了该算法，可以像使用`KMeans`类一样使用它：

```python
from sklearn.cluster import MiniBatchKMeans
```

```python
minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)
minibatch_kmeans.fit(X)
```

```python
minibatch_kmeans.inertia_
```

如果数据集不能容纳进内存，最简单的选择是使用`memmap`类。也可以每次传递一个小批量给`partial_fit`方法，但是这需要更多工作，因为要自己执行多次初始化并选择最好的模型。

下面加载MNIST数据集并使用`memmap`类实现训练：

```python
import urllib.request
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1, as_frame=False)
mnist.target = mnist.target.astype(np.int64)
```

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    mnist["data"], mnist["target"], random_state=42)
```

```python
filename = "my_mnist.data"
X_mm = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)
X_mm[:] = X_train
```

```python
minibatch_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=10, random_state=42)
minibatch_kmeans.fit(X_mm)
```

如果数据过大导致无法使用`memmap`，则事情更复杂，下面使用`partial_fit`：

```python
# 用于加载下一个批的函数。
# 在现实中，需要从磁盘中加载数据。
def load_next_batch(batch_size):
    return X[np.random.choice(len(X), batch_size, replace=False)]
```

```python
np.random.seed(42)
```

```python
k = 5
n_init = 10
n_iterations = 100
batch_size = 100
init_size = 500  # more data for K-Means++ initialization
evaluate_on_last_n_iters = 10

best_kmeans = None

for init in range(n_init):
    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, init_size=init_size)
    X_init = load_next_batch(init_size)
    minibatch_kmeans.partial_fit(X_init)

    minibatch_kmeans.sum_inertia_ = 0
    for iteration in range(n_iterations):
        X_batch = load_next_batch(batch_size)
        minibatch_kmeans.partial_fit(X_batch)
        if iteration >= n_iterations - evaluate_on_last_n_iters:
            minibatch_kmeans.sum_inertia_ += minibatch_kmeans.inertia_

    if (best_kmeans is None or
        minibatch_kmeans.sum_inertia_ < best_kmeans.sum_inertia_):
        best_kmeans = minibatch_kmeans
```

```python
best_kmeans.score(X)
```

下面比较两个算法的运行时间：

```python
%timeit KMeans(n_clusters=5, random_state=42).fit(X)
```

```python
%timeit MiniBatchKMeans(n_clusters=5, random_state=42).fit(X)
```

可以看到小批量K-Means远快于普通的K-Means。

虽然小批量K-Means算法比普通的K-Means算法快得多，但是它的惰性通常要略差，尤其当簇的数量增加时：

```python
from timeit import timeit
```

```python
times = np.empty((100, 2))
inertias = np.empty((100, 2))
for k in range(1, 101):
    kmeans_ = KMeans(n_clusters=k, random_state=42)
    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42)
    print("\r{}/{}".format(k, 100), end="")
    times[k-1, 0] = timeit("kmeans_.fit(X)", number=10, globals=globals())
    times[k-1, 1]  = timeit("minibatch_kmeans.fit(X)", number=10, globals=globals())
    inertias[k-1, 0] = kmeans_.inertia_
    inertias[k-1, 1] = minibatch_kmeans.inertia_
```

```python
plt.figure(figsize=(10,4))

plt.subplot(121)
plt.plot(range(1, 101), inertias[:, 0], "r--", label="K-Means")
plt.plot(range(1, 101), inertias[:, 1], "b.-", label="Mini-batch K-Means")
plt.xlabel("$k$", fontsize=16)
plt.title("Inertia", fontsize=14)
plt.legend(fontsize=14)
plt.axis([1, 100, 0, 100])

plt.subplot(122)
plt.plot(range(1, 101), times[:, 0], "r--", label="K-Means")
plt.plot(range(1, 101), times[:, 1], "b.-", label="Mini-batch K-Means")
plt.xlabel("$k$", fontsize=16)
plt.title("Training time (seconds)", fontsize=14)
plt.axis([1, 100, 0, 6])

plt.show()
```

左图比较了使用不同数量的簇$k$在前述数据集上训练的小批量K-Means与普通的K-Means模型的惰性。两条曲线之间的差异相当稳定，但是随着$k$增加，差异越来越明显，因为惰性越来越小。从右图可以看出小批量K-Means远快于普通的K-Means，且这种差异随着$k$的增加而增加。

#### 选择最优数量的簇

目前，我们设置簇的数量$k=5$，这是因为通过查看数据可以很明显发现正确的簇的数量。但是通常不会这么容易知道如何设置$k$，如果错误地设置它，则结果可能相当糟糕。例如，设置$k$为3或8导致相当糟糕的模型：

```python
kmeans_k3 = KMeans(n_clusters=3, random_state=42)
kmeans_k8 = KMeans(n_clusters=8, random_state=42)

plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, "$k=3$", "$k=8$")
plt.show()
```

```python
kmeans_k3.inertia_
```

```python
kmeans_k8.inertia_
```

选择惰性最小的模型是行不通的。当$k=3$时，模型的惰性为653.2，这远高于$k=5$时的惰性，但是$k=8$时，惰性为119.1。惰性不是选择$k$的好的性能指标，因为它会随着$k$的增加而减少。实际上，簇越多，实例会越接近离它最近的质心，从而惰性会越低。下面绘制惰性关于$k$的函数：

```python
kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)
                for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]
```

```python
plt.figure(figsize=(8, 3.5))
plt.plot(range(1, 10), inertias, "bo-")
plt.xlabel("$k$", fontsize=14)
plt.ylabel("Inertia", fontsize=14)
plt.annotate('Elbow',
             xy=(4, inertias[3]),
             xytext=(0.55, 0.55),
             textcoords='figure fraction',
             fontsize=16,
             arrowprops=dict(facecolor='black', shrink=0.1)
            )
plt.axis([1, 8.5, 0, 1300])
plt.show()
```

随着$k$增加到4，惰性下降得非常快，然后随着$k$继续增加，惰性下降慢得多。曲线大致像一条臂（arm），在$k=4$时有一个肘（elbow），如果我们不能知道更多，4可能是一个好的选择：任意更低的值都变化剧烈，任何更高的值都不会有太大帮助，我们可能在没有好的理由的情况下，中途停止簇的分裂。

$k=4$的决策边界如下：

```python
plot_decision_boundaries(kmeans_per_k[4-1], X)
plt.show()
```

可以看出$k=4$是一个相当好的选择，但不完美，因为它将左下角的两个块看做一个簇。

这种选择簇的数量的最优值的技术相当粗糙（coarse），更精确的方法（同时计算成本也更高）的方法是使用**轮廓得分（silhouette score）**，它是所有实例的**轮廓系数（silhouette coefficient）**的均值。一个实例的轮廓系数等于$(b-a)/\max(a,b)$，其中$a$为实例到同簇中其他实例的平均距离（即平均簇内距离），$b$是平均最近簇距离（the mean nearest-cluster distance）（即实例到第二近的簇中实例的距离，第二近的簇即除了实例自己的簇，能够最小化$b$的簇）。轮廓系数介于-1\~1之间。如果系数接近1，则意味着实例位于自己的簇内且远离其他簇，系数接近0意味着它接近簇的边缘，系数接近-1意味着实例可能被分到错误的簇中。

> The silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.

要计算轮廓得分，使用Scikit-Learn的`silhouette_score`函数，并给定数据集中的所有实例以及为其分配的标签。

```python
from sklearn.metrics import silhouette_score
```

```python
silhouette_score(X, kmeans.labels_)
```

比较不同数量的簇的轮廓得分：

```python
silhouette_scores = [silhouette_score(X, model.labels_)
                     for model in kmeans_per_k[1:]]
```

```python
plt.figure(figsize=(8, 3))
plt.plot(range(2, 10), silhouette_scores, "bo-")
plt.xlabel("$k$", fontsize=14)
plt.ylabel("Silhouette score", fontsize=14)
plt.axis([1.8, 8.5, 0.55, 0.7])
plt.show()
```

可以看出，它比前一个可视化揭示更丰富的信息：虽然它证实了$k=4$是一个很好的选择，它也强调了$k=5$相当好，并且远好于$k=6$或$k=7$。

如果绘制每个实例的轮廓系数，并根据分配的簇以及系数值进行排序，则可以揭示更多的信息。这被称为**轮廓图（silhouette diagram）**：

> An even more informative visualization is obtained when you plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. 

```python
import matplotlib as mpl

from sklearn.metrics import silhouette_samples
from matplotlib.ticker import FixedLocator, FixedFormatter

plt.figure(figsize=(11, 9))

for k in (3, 4, 5, 6):
    plt.subplot(2, 2, k - 2)
    
    y_pred = kmeans_per_k[k - 1].labels_
    silhouette_coefficients = silhouette_samples(X, y_pred)

    padding = len(X) // 30
    pos = padding
    ticks = []
    for i in range(k):
        coeffs = silhouette_coefficients[y_pred == i]
        coeffs.sort()

        color = mpl.cm.Spectral(i / k)
        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,
                          facecolor=color, edgecolor=color, alpha=0.7)
        ticks.append(pos + len(coeffs) // 2)
        pos += len(coeffs) + padding

    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))
    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))
    if k in (3, 5):
        plt.ylabel("Cluster")
    
    if k in (5, 6):
        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
        plt.xlabel("Silhouette Coefficient")
    else:
        plt.tick_params(labelbottom=False)

    plt.axvline(x=silhouette_scores[k - 2], color="red", linestyle="--")
    plt.title("$k={}$".format(k), fontsize=16)

plt.show()
```

每个图包含每个簇的一个刀形。每个形状（shape）的高度指示簇包含的实例数量，宽度表示簇内实例的排序后的轮廓系数（越宽越好）。垂直的虚线指示轮廓系数的均值，它代表所有簇的平均轮廓得分。如果簇内大多数实例的系数比该得分小，则该簇相当糟糕，因为这意味着它的实例过于接近其他簇。可以看到，当$k=3$与$k=6$时，得到的簇不好，当$k=4$或$k=5$时，簇看上去相当好：大多数实例延伸到虚线右侧，接近1.0（closer to 1.0）。当$k=4$，索引为1的簇（顶部起第3个）相当大；当$k=5$，所有的簇由相似的大小。因此，即使$k=4$时的整体轮廓得分略高于$k=5$时的，但看上去使用$k=5$使得每个簇大小相似是一个好主意。

> Each diagram contains one knife shape per cluster.

#### K-Means的局限性

尽管K-Means有许多优点，最明显的是它的快速性和可扩展性，但它并不完美。为了避免次优解，算法需要多次运行；需要指定簇的数量，这可能很棘手。另外，当簇的大小不一、密度不同或不是球状时，K-Means表现不会很好。

<a name=(无监督学习技术)(聚类)(K-Means)(K-Means的局限性)(1)>下面展示了在K-Means在包含三个不同维度、密度与方向的椭球簇的数据集上的聚类：</a>

> Means clusters a dataset containing three ellipsoidal clusters of different dimensions, densities, and orientations.

```python
X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 + [6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]
```

```python
plot_clusters(X)
```

```python
kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42)
kmeans_bad = KMeans(n_clusters=3, random_state=42)
kmeans_good.fit(X)
kmeans_bad.fit(X)
```

```python
plt.figure(figsize=(10, 3.2))

plt.subplot(121)
plot_decision_boundaries(kmeans_good, X)
plt.title("Inertia = {:.1f}".format(kmeans_good.inertia_), fontsize=14)

plt.subplot(122)
plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)
plt.title("Inertia = {:.1f}".format(kmeans_bad.inertia_), fontsize=14)

plt.show()
```

可以看到，两种解决方法都不好。左边的解决方法更好，但是仍然将砍掉中间簇的25%并将其分配到右边的簇中。右边的解决方法很糟糕，即使它的惰性更低。因此，取决于数据，其他聚类算法可能表现更好。对于这种椭圆状簇（elliptical clusters），高斯混合模型表现得很好。

> So, depending on the data, different clustering algorithms may perform better. On these types of elliptical clusters, Gaussian mixture models work great.

因此在运行K-Means算法前，缩放输入特征很重要。否则簇可能被拉伸得很厉害（very stretched），导致K-Means表现很糟糕。缩放特征并不能保证所有的簇很好且是球状，但是通常可以改善结果。

> Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things.

### DBSCAN

DBSCAN基于局部密度估计（local density estimation），它将簇定义为高密度的连续区域（region）：

- 对于每个实例，算法计算有多少实例位于一个它的小距离$\epsilon$内。这个区域被称为实例的**$\epsilon$-邻域（$\epsilon$-neighborhood）**。
- 如果实例在它的$\epsilon$-邻域内有至少`min_sample`个实例（包括它自己），则它被认为是一个**核心实例（core instance）**。换言之，核心实例是那些位于高密度区域的实例。
- 所有位于某个核心实例邻域内的实例属于相同的簇。这个邻域可能包含其他实例，所以，一个长的邻居核心实例序列构成了一个单独的簇。
- 如果一个实例不是核心实例，且它的邻域内没有核心实例，则它被认为是一个异常（anomaly）。

如果所有簇的密度足够大，且它们被低密度区域分开，则该算法能够很好地工作。

下面在moons数据集上测试该算法：

```python
from sklearn.datasets import make_moons
```

```python
X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)
```

```python
from sklearn.cluster import DBSCAN
```

```python
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
```

通过`labels_`实例变量获得所有实例的标签：

```python
dbscan.labels_[:10]
```

注意一些实例的簇索引等于-1，这意味着它们被算法认为是异常。通过`core_sample_indices_ `实例变量获得核心实例的索引，核心实例本身可以通过`components`实例变量获得：

```python
len(dbscan.core_sample_indices_)
```

```python
dbscan.core_sample_indices_[:10]
```

```python
dbscan.components_[:3]
```

下面绘制出簇：

```python
dbscan2 = DBSCAN(eps=0.2)
dbscan2.fit(X)
```

```python
def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]
    
    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask], marker='o', s=size, cmap="Paired")
    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])
    plt.scatter(anomalies[:, 0], anomalies[:, 1],
                c="r", marker="x", s=100)
    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=".")
    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
    plt.title("eps={:.2f}, min_samples={}".format(dbscan.eps, dbscan.min_samples), fontsize=14)
```

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(9, 3.2))

plt.subplot(121)
plot_dbscan(dbscan, X, size=100)

plt.subplot(122)
plot_dbscan(dbscan2, X, size=600, show_ylabels=False)

plt.show()
```

聚类在左图中表示，可以看出它识别出相当多的异常，以及7个不同的簇。如果通过增加`eps`到0.2，来扩大（widen）每个实例的邻域，则得到右图所示的聚类，看起来很完美。下面继续使用这个模型：

```python
dbscan = dbscan2
```

`DBSCAN`类有一个`fit_predict`方法，但是没有`predict`方法。也就是说它不能预测新实例属于哪个簇。之所以作出这个实现决策（implementation decision），是因为不同的分类算法适用于不同的任务，所以作者决定让用户选择使用哪个分类算法。而且，它很难实现。

> This implementation decision was made because different classification algorithms can be better for different tasks, so the authors decided to let the user choose which one to use.

下面训练一个`KNeighborsClassifier`：

```python
from sklearn.neighbors import KNeighborsClassifier
```

```python
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])
```

给定新实例，可以预测它最可能属于哪个簇，甚至估计它属于每个簇的概率：

```python
X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])
knn.predict(X_new)
```

```python
knn.predict_proba(X_new)
```

以上代码在核心实例上训练分类器，当然也可以在所有实例上训练分类器，或在非异常实例上训练它。这取决于最终任务。

决策边界如下：

```python
plt.figure(figsize=(6, 3))
plot_decision_boundaries(knn, X, show_centroids=False)
plt.scatter(X_new[:, 0], X_new[:, 1], c="b", marker="+", s=200, zorder=10)
plt.show()
```

$\cross$表示`X_new`中的4个实例。注意训练集中没有异常，所以分类器总是选择一个簇，即使该簇很远。可以引入一个最大距离，在这种情况下两个远离两个簇的实例被分类为异常。使用`KNeighborsClassifier`的`kneighbors`方法实现这点。给定一组实例，它返回训练集中最近的$k$个邻居的距离与索引（两个矩阵，每个$k$列）：

```python
y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
y_pred[y_dist > 0.2] = -1
y_pred.ravel()
```

简言之，DBSCAN是简单而强大的算法，它可以识别任意形状、任意数量的簇。它对离群值很健壮，并且只有两个超参数（`eps`与`min_samples`）。如果簇中的密度变化很大，它不能合适地捕获所有的簇。它的计算复杂度大致为$O(m\log m)$，这使得它与实例数量大致呈线性关系，但是如果`eps`很大，Scikit-Learn的实现需要$O(m^2)$内存。

> In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any number of clusters of any shape. It is robust to outliers, and it has just two hyperparameters (`eps` and `min_samples`). If the density varies significantly across the clusters, however, it can be impossible for it to capture all the clusters properly. Its computational complexity is roughly $O(m\log m)$, making it pretty close to linear with regard to the number of instances, but Scikit-Learn’s implementation can require up to $O(m^2)$ memory if eps is large.

[scikit-learn-contrib project](https://github.com/scikit-learn-contrib/hdbscan/)实现了**层次DBSCAN（Hierarchical DBSCAN，HDBSCAN）**。

### 聚类的应用

#### 图像分割

**图像分割（image segmentation）**是将图像分割成（partition）多个部分（segments）的任务。在**语义分割（semantic segmentation）**中，所有属于同一个物体类型的像素被分到相同部分。例如，在自动驾驶汽车的视觉系统（self-driving car’s vision system）中，所有属于行人图像的像素都被分到“行人”部分（有一个部分包含所有的行人）。在**实例分割（instance segmentation）**中，所有属于同一个体对象的像素被分到相同部分，在这种情况下每个行人都有一个不同的部分。目前语义或实例分割的最新技术是使用基于卷积神经网络的复杂结构实现的。这里我们实现一个简单的任务：**颜色分割（color segmentation）**：如果像素有相似的颜色，则将它们分到相同部分。在某些应用中，这可能足够了。例如：通过分析卫星图像来度量一个地区的森林总面积。

```python
# 下载瓢虫图像。
# 下载地址不可用，这里仅作演示。
# 数据集可以从https://github.com/ageron/handson-ml2上获得。

import os

PROJECT_ROOT_DIR = "."

images_path = os.path.join(PROJECT_ROOT_DIR, "images", "unsupervised_learning")
os.makedirs(images_path, exist_ok=True)
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
filename = "ladybug.png"
print("Downloading", filename)
url = DOWNLOAD_ROOT + "images/unsupervised_learning/" + filename
urllib.request.urlretrieve(url, os.path.join(images_path, filename))
```

```python
from matplotlib.image import imread

image = imread(os.path.join(images_path, filename))
image.shape
```

图像表示为3维数组。第一维的大小为高度，第二维的大小为宽度，第三维的大小为颜色通道（color channels）数，在这种情况下是红、绿、蓝（RGB）。换言之，每个像素都有一个3维向量，包含红、绿、蓝的灰度（intensities），每个灰度介于0.0\~1.0之间（如果使用`imageio.imread`方法，则介于0\~255之间）。以下代码将数组的形状改变，以获得RGB颜色的长列表，然后它使用K-Means聚类这些颜色：

```python
X = image.reshape(-1, 3)
kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape)
```

对于每个像素，它找到该像素所属簇的颜色均值，然后用该其替代该像素当前颜色。最后将颜色长列表的形状重新变为原来图像的形状。

下面使用不同的簇数量，并显示得到后的图像：

```python
segmented_imgs = []
n_colors = (10, 8, 6, 4, 2)
for n_clusters in n_colors:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_imgs.append(segmented_img.reshape(image.shape))
```

```python
plt.figure(figsize=(10,5))
plt.subplots_adjust(wspace=0.05, hspace=0.1)

plt.subplot(231)
plt.imshow(image)
plt.title("Original image")
plt.axis('off')

for idx, n_clusters in enumerate(n_colors):
    plt.subplot(232 + idx)
    plt.imshow(segmented_imgs[idx])
    plt.title("{} colors".format(n_clusters))
    plt.axis('off')

plt.show()
```

注意，当簇的数量小于8时，瓢虫的鲜红色无法形成自己的簇，它与环境中的颜色合并。这是因为K-Means喜欢大小相似的簇，而瓢虫比图片的其他部分小得多，因此即使它的颜色鲜艳（flashy），K-Means也不能为其指定一个簇。

> When you use fewer than eight clusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it gets merged with colors from the environment. This is because K-Means prefers clusters of similar sizes. 

#### 预处理

聚类是一种高效的降维方法，尤其作为监督学习算法的预处理步骤。

> Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing step before a supervised learning algorithm.

下面处理一个数字数据集（一个简单的类MNIST（MNIST-like）数据集，包含1797张表示数字0\~9的$8\times 8$灰度图像）。

```python
from sklearn.datasets import load_digits
```

```python
X_digits, y_digits = load_digits(return_X_y=True)
```

将其划分为训练集与测试集：

```python
from sklearn.model_selection import train_test_split
```

```python
X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)
```

拟合一个Logistic回归模型：

```python
from sklearn.linear_model import LogisticRegression
```

```python
log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
log_reg.fit(X_train, y_train)
```

在测试集上评估它的准确率：

```python
log_reg_score = log_reg.score(X_test, y_test)
log_reg_score
```

这是我们的基线（baseline）：96.9%的准确率。

下面创建一个管道，它首先将训练集聚类为50个簇，并将图像替换为它们到这50个簇的距离，然后应用一个Logistic回归模型：

```python
from sklearn.pipeline import Pipeline
```

```python
pipeline = Pipeline([
    ("kmeans", KMeans(n_clusters=50, random_state=42)),
    ("log_reg", LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)),
])
pipeline.fit(X_train, y_train)
```

需要注意的是，虽然一共有10个不同的数字，但是每个数字可以用若干不同方式书写，因此最好使用更多的簇，例如50。

然后评估该分类管道：

```python
pipeline_score = pipeline.score(X_test, y_test)
pipeline_score
```

```python
1 - (1 - pipeline_score) / (1 - log_reg_score)  # 错误率下降了多少？
```

可以看到，错误率降低了近30%。

以上簇的数量是任意选择的，但是可以做得更好。既然K-Means只是分类管道的一个预处理步骤，找到最好的$k$值比之前的做法简单得多。这里不需要执行轮廓分析或最小化惰性，最好的$k$就是能够使得交叉验证过程中分类性能最好的值。下面使用`GridSearchCV`找到最好的簇数量：

```python
from sklearn.model_selection import GridSearchCV
```

```python
param_grid = dict(kmeans__n_clusters=range(2, 100))
grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
grid_clf.fit(X_train, y_train)
```

查看$k$的最好值，以及最终管道（the resulting pipeline）的性能：

```python
grid_clf.best_params_
```

```python
grid_clf.score(X_test, y_test)
```

当$k=99$时，我们得到了显著的准确率提升，在测试集上达到了98.22%的准确率。因为99是探索范围内的最大值，所以接下来可以探索更大的$k$值。

#### 半监督学习

下面在来自数字数据集的50个标记实例的样本上训练一个Logistic回归模型：

> Let’s train a Logistic Regression model on a sample of 50 labeled instances from the digits dataset:

```python
n_labeled = 50
```

```python
log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", random_state=42)
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
log_reg.score(X_test, y_test)
```

可以看到模型在验证集上准确率只有83.3%，比先前在整个训练集上训练模型时的的准确率低很多。下面看看如何做得更好。

首先将训练集聚类为50个类，然后对于每个簇，选择最接近质心的图像，我们称这些图像为**代表图像（representative images）**：

```python
k = 50
```

```python
kmeans = KMeans(n_clusters=k, random_state=42)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]
```

下面绘图展示这50张图像：

```python
plt.figure(figsize=(8, 2))
for index, X_representative_digit in enumerate(X_representative_digits):
    plt.subplot(k // 10, 10, index + 1)
    plt.imshow(X_representative_digit.reshape(8, 8), cmap="binary", interpolation="bilinear")
    plt.axis('off')

plt.show()
```

查看每张图像并手动打标签：

```python
# 由于训练集本身有标签，这里直接采用利用这些标签。
#! 在实际项目中，这些标签通常是不存在的，需要手动打标签。
#! 例如手动书写：y_representative_digits = np.array([...])
y_representative_digits = y_train[representative_digit_idx]
```

查看性能是否更好：

```python
log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
log_reg.fit(X_representative_digits, y_representative_digits)
log_reg.score(X_test, y_test)
```

可以看到准确率大幅提升，虽然仍然在50个实例上训练模型。因为标记实例常常代价高（costly）且痛苦（painful），尤其是当实例必须由专家手动标记时，所以标记代表实例（representative instances）而不是随机的实例是一个好主意。

进一步地，可以将标签传播给同一簇内的其他所有实例，这被称为**标签传播（label propagation）**：

```python
y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]
```

再次训练模型并查看它的表现：

```python
log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
log_reg.fit(X_train, y_train_propagated)
```

```python
log_reg.score(X_test, y_test)
```

可以看到合理的（reasonable）准确率提升，但是并不震撼。问题在于我们将每个代表实例的标签传播给同一簇内的所有实例，包括位于簇边界的实例，它们更容易被错误标记。下面只将标签传播给最接近质心的20%实例：

```python
percentile_closest = 20

X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster]
    cutoff_distance = np.percentile(cluster_dist, percentile_closest)
    above_cutoff = (X_cluster_dist > cutoff_distance)
    X_cluster_dist[in_cluster & above_cutoff] = -1
```

```python
partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]
```

再次在部分传播的数据集（partially propagated dataset）上训练模型：

```python
log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
```

```python
log_reg.score(X_test, y_test)
```

这样，仅仅使用50个有标签实例，模型的准确率达到94.0%，与在整个有标签数字数据集上训练的Logistic回归模型的性能（96.9%）相当接近。这种好的性能是因为传播的标签相当好：它们的准确率接近99%：

```python
np.mean(y_train_partially_propagated == y_train[partially_propagated])
```

## 高斯混合

**高斯混合模型（Gaussian mixture model，GMM）**是概率模型，它假定实例是从若干参数未知的高斯分布的混合中产生的。所有产生于单个高斯分布的实例形成一个簇，通常它看起来像一个椭圆（ellipses）。每个簇有不同的椭圆（ellipsoidal）形状、大小、密度与方向（orientation），如[图1](#(无监督学习技术)(聚类)(K-Means)(K-Means的局限性)(1))所示。当你观察一个实例时，你知道它是由其中一个高斯分布生成的，但是不知道是哪一个，并且也不知道这些分布的参数。

GMM有若干变体（variants）。`GaussianMixture`类实现了最简单的一个变体。在该变体中，必须预先指定高斯分布的数量$k$。数据集$\pmb{X}$假定通过以下概率过程生成：

- 对于每个实例，随机从$k$个簇中选取一个簇。选择第$j$个簇的概率由簇的权重$\phi^{(j)}$定义。第$i$个实例所属的簇的索引记为$z^{(i)}$。
- 如果$z^{(i)}=j$，意味着第$i$个实例被分到第$j$个簇。该实例的位置$\pmb{x}^{(i)}$从高斯分布中随机抽样得到，该高斯分布的均值为$\pmb{\mu}^{(j)}$，协方差矩阵为$\pmb{\Sigma}^{(j)}$。这被记为$\pmb{x}^{(i)}\sim N(\pmb{\mu}^{(j)},\Sigma^{(j)})$。

> the location $\pmb{x}^{(i)}$ of this instance is sampled randomly from the Gaussian distribution with mean $\pmb{\mu}^{(j)}$ and covariance matrix $\pmb{\Sigma}^{(j)}$. This is noted $\pmb{x}^{(i)}\sim N(\pmb{\mu}^{(j)},\Sigma^{(j)})$.

该生成过程可以用一个图形模型（graphical model）来表示。下面表示随机变量之间的条件依赖结构：

![高斯混合模型的图形表示，包括它的参数（正方形）、随机变量（圆形）以及它们的条件依赖（实心箭头）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\高斯混合模型的图形表示.png)

该图的解释如下：

[^图的解释]: Most of these notations are standard, but a few additional notations were taken from the Wikipedia article on [plate notation](https://en.wikipedia.org/wiki/Plate_notation).

- 圆形表示随机变量。
- 正方形表示固定值（即模型参数）。
- 大的长方形被称为**盘（plates）**。它们指示其内容重复若干次。
- 每个盘右下角的数字指示其内容被重复的次数。因此，有$m$个随机变量$z^{(i)}$（从$z^{(1)}$到$z^{(m)}$），$m$个随机变量$\pmb{x}^{(i)}$。同时有$k$个均值$\pmb{\mu}^{(j)}$以及$k$个协方差矩阵$\pmb{\Sigma}^{(j)}$。最后，只有一个权重向量$\pmb{\phi}$（包括所有权重$\phi^{(1)}$\~$\phi^{(k)}$）。
- 每个变量$z^{(i)}$由权重为$\pmb{\phi}$的**类别分布（categorical distribution）**产生的，每个变量$\pmb{x}^{(i)}$由正态分布产生，正态分布的均值与协方差由它的簇$z^{(i)}$定义。
- 实心箭头表示条件依赖。例如，每个随机变量$z^{(i)}$的概率分布依赖于权重向量$\pmb{\phi}$。注意当一个箭头穿过盘的边界时，意味着实心箭头作用于盘的所有重复项。例如，权重向量$\pmb{\phi}$决定了所有随机变量$\pmb{x}^{(1)}$到$\pmb{x}^{m}$的概率分布。
- 从$z^{(i)}$到$\pmb{x}^{(i)}$的弯曲箭头（squiggly arrow）表示一个转移（switch）：依据$z^{(i)}$的值，实例$\pmb{x}^{(i)}$从不同的高斯分布中采样。例如，如果$z^{(i)}=j$，则$\pmb{x}^{(i)}\sim N(\pmb{\mu}^{(j)},\pmb{\Sigma}^{(j)})$。
- 着色结点（shaded nodes）指示值已知。因此，在这种情况下，只有随机变量$\pmb{x}^{(i)}$的值已知，它们被称为**观测变量（observed variables）**。未知的随机变量$z^{(i)}$被称为**潜在变量（latent variables）**。

给定数据集$\pmb{X}$，通常可以先估计权重$\pmb{\phi}$、所有的分布参数$\pmb{\mu}^{(1)}$到$\pmb{\mu}^{(k)}$，以及$\pmb{\Sigma}^{(1)}$到$\pmb{\Sigma}^{(k)}$。Scikit-Learn的`GaussianMixture`可以轻松做到：

```python
X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 + [6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]
```

```python
from sklearn.mixture import GaussianMixture
```

```python
gm = GaussianMixture(n_components=3, n_init=10, random_state=42)
gm.fit(X)
```

查看算法估计的参数：

```python
gm.weights_
```

```python
gm.means_
```

```python
gm.covariances_
```

实际上用来产生数据的权重为0.4、0.4与0.2，与算法发现的很接近。同样地，均值与协方差矩阵也与算法发现的非常接近。

`GaussianMixture`依赖于**期望最大化（Expectation-Maximization，EM）**算法，它与K-Means算法有很多相似点：它也首先随机初始化参数，然后重复以下两个过程直到收敛：

- **期望步（expectation step）**：将实例分给簇。
- **最大化步（maximization step）**：更新簇。

在聚类背景下，可以认为EM是K-Means的泛化，它不仅找到簇的中心（$\pmb{\mu}^{(1)}$到$\pmb{\mu}^{(k)}$），而且找到它们的大小、形状与方向（$\pmb{\Sigma}^{(1)}$到$\pmb{\Sigma}^{(k)}$），以及它们的相对权重（$\phi^{(1)}$到$\phi^{(k)}$）。与K-Means不同的是，EM使用软聚类算法（而不是硬分配（hard assignments））。对于每个实例，在期望步，算法估计它属于每个簇的概率（基于当前的簇参数）。然后，在最大化步，使用数据集中的所有实例更新每个簇，每个实例根据其属于该簇的估计概率进行加权，这些概率被称为簇对该实例的**责任（responsibility）**。在最大化步，每个簇的更新主要受其最负责的实例的影响。

> In the context of clustering, you can think of EM as a generalization of K-Means

与K-Means一样，EM也可能收敛到不好的解（poor solutions），因此需要被运行多遍，并选择最好的解。这也是设置`n_init=10`的原因。注意，默认情况下`n_init`设置为1。

检查算法是否收敛以及迭代次数：

```python
gm.converged_
```

```python
gm.n_iter_
```

现在已经得到每个簇的位置、大小、形状、方向与相对权重，模型可以很容易地将实例分给最可能的簇（硬聚类）或估计它属于某个类的概率（软聚类）。使用`predict`方法进行硬聚类，使用`predict_proba`方法进行软聚类：

```python
gm.predict(X)
```

```python
gm.predict_proba(X)
```

高斯混合模型是**生成模型（generative model）**，意味着可以通过它采样新实例（注意它们按簇索引排序）：

```python
X_new, y_new = gm.sample(6)
X_new
```

```python
y_new
```

通过`score_samples`方法可以估计模型在任意给定位置的密度：对于每个给定实例，模型估计在该位置的**概率密度函数（probability density function，PDF）**的log值，得分越高，密度越高：

```python
gm.score_samples(X)
```

如果计算这些得分的指数，将得到在给定实例的位置处的PDF。这些不是概率，而是概率密度：它们可以是任意正值，而不仅是位于0\~1之间的值。为了估计一个实例位于一个特定区域的概率，可以将该区域的PDF值集成。如果对实例可能位于的所有位置组成的整个空间这么做，将得到1，下面验证这一点（只是近似）：

```python
resolution = 100
grid = np.arange(-10, 10, 1 / resolution)
xx, yy = np.meshgrid(grid, grid)
X_full = np.vstack([xx.ravel(), yy.ravel()]).T

pdf = np.exp(gm.score_samples(X_full))
pdf_probas = pdf * (1 / resolution) ** 2
pdf_probas.sum()
```

<a name=(无监督学习)(高斯混合)(1)>下面展示模型的簇均值、决策边界（虚线）以及密度等值线：</a>

```python
from matplotlib.colors import LogNorm

def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z,
                 norm=LogNorm(vmin=1.0, vmax=30.0),
                 levels=np.logspace(0, 2, 12))
    plt.contour(xx, yy, Z,
                norm=LogNorm(vmin=1.0, vmax=30.0),
                levels=np.logspace(0, 2, 12),
                linewidths=1, colors='k')

    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx, yy, Z,
                linewidths=2, colors='r', linestyles='dashed')
    
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)
    plot_centroids(clusterer.means_, clusterer.weights_)

    plt.xlabel("$x_1$", fontsize=14)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
```

```python
plt.figure(figsize=(8, 4))

plot_gaussian_mixture(gm, X)

plt.show()
```

算法找到了好的解。当然，该任务已被简化：使用一系列2维高斯分布生成数据（实际的数据并不总是高斯分布与低维），并且为算法提供了正确的簇的数量。当维度很多、簇的数量很多或实例很少时，EM可能难以收缩到最优解。此时可以限制算法需要学习的参数数量，以减少任务的难度。一种方法是限制簇的形状与方向范围。这可以通过对协方差矩阵施加约束来实现。要做到这点，设置超参数`covariance_type`为下列值之一：

- `spherical`：每个簇必须是球状的（spherical），但它们可以有不同的直径（即不同的方差）。
- `diag`：簇可以是任意大小的任意椭球（ellipsoidal）形状，但是椭球（ellipsoid）的轴必须与坐标轴平行（即协方差矩阵必须是对角矩阵）。
- `tied`：所有的簇必须有相同的椭球形状、大小与方向（即所有的簇共享相同的协方差矩阵）。

默认情况下，`covariance_type`等于`full`，意味着每个簇可以是任意形状、大小与方向（它有自己的不受约束的协方差矩阵）。下面绘制当`covariance_type`设置为不同值时，EM算法找到的解：

```python
gm_full = GaussianMixture(n_components=3, n_init=10, covariance_type="full", random_state=42)
gm_tied = GaussianMixture(n_components=3, n_init=10, covariance_type="tied", random_state=42)
gm_spherical = GaussianMixture(n_components=3, n_init=10, covariance_type="spherical", random_state=42)
gm_diag = GaussianMixture(n_components=3, n_init=10, covariance_type="diag", random_state=42)
gm_full.fit(X)
gm_tied.fit(X)
gm_spherical.fit(X)
gm_diag.fit(X)
```

```python
def compare_gaussian_mixtures(gm1, gm2, X):
    plt.figure(figsize=(9, 4))

    plt.subplot(121)
    plot_gaussian_mixture(gm1, X)
    plt.title('covariance_type="{}"'.format(gm1.covariance_type), fontsize=14)

    plt.subplot(122)
    plot_gaussian_mixture(gm2, X, show_ylabels=False)
    plt.title('covariance_type="{}"'.format(gm2.covariance_type), fontsize=14)
```

```python
compare_gaussian_mixtures(gm_tied, gm_spherical, X)

plt.show()
```

```python
compare_gaussian_mixtures(gm_full, gm_diag, X)
plt.tight_layout()
plt.show()
```

训练一个`GaussianMixture`模型的计算复杂度取决于实例数量$m$、维度$n$、簇的数量$k$与对协方差矩阵施加的约束。如果`covariance_type`为`spherical`或`diag`，它是$O(kmn)$（假设数据有簇结构）；如果`covariance_type`为`tied`或`full`，它是$O(kmn^2+kn^3)$，因此它可能不能扩展到大量的特征上。

### 使用高斯混合进行异常检测

使用高斯混合模型进行异常检测很简单：所有位于低密度区域的实例可被认为是异常。其中，密度的阈值要预先定义。例如，在一家试图检测有缺陷产品的制造公司，有缺陷产品的比例通常已知。假设它是4%，则设置密度阈值为某个值，其中4%的实例位于低于该阈值密度的区域。如果发现有太多的假阳性（即标记为有缺陷的完好产品），则可以降低阈值；反之，如果有太多的假阴性（即没有被标记为有缺陷的有缺陷产品），则可以提升阈值。这就是通常的精确率/召回率折中。

下面使用第四百分位最低密度最为阈值识别离群值（即大约4%的实例被标记为异常）：

```python
densities = gm.score_samples(X)
density_threshold = np.percentile(densities, 4)
anomalies = X[densities < density_threshold]
```

下面将异常标记为星（stars）：

```python
plt.figure(figsize=(8, 4))

plot_gaussian_mixture(gm, X)
plt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')
plt.ylim(top=5.1)

plt.show()
```

高斯混合模型尝试拟合所有数据，包括离群值。因此如果离群值太多，会导致模型对“异常”的理解发生偏差，一些离群值会被错误地认为是正常值。如果这样，则可以尝试拟合模型并使用它去检测并移除最极端的离群值，然后再次在清理后的数据集上拟合模型。另一个方法是使用健壮的协方差估计模型（见`EllipticEnvelope`类）。

### 选择簇的数量

高斯混合不能使用K-Means的惰性或轮廓得分去选择合适的簇数量，因为当簇不是球状的或有不同的大小时，它们不可靠。可以尝试寻找一个模型，它能最小化一个**理论信息准则（theoretical information criterion）**，例如**贝叶斯信息准则（Bayesian information criterion，BIC）**或**Akaike信息准则（Akaike information criterion，AIC）**：
$$
BIC=\log(m)p-2\log(\widehat{L})
$$

$$
AIC=2p-2log(\widehat{L})
$$

其中：

- $m$为实例数量。
- $p$为模型学习的参数数量。
- $\widehat{L}$为模型**似然函数（likelihood function）**的最大值。

BIC与AIC都惩罚有更多要学习的参数（例如更多的簇）的模型，奖励很好地拟合数据的模型。它们最终常常选择同一模型，如果选择的模型不同，BIC选择的模型倾向于更简单（参数更少），但是拟合效果要差（尤其对于大规模数据集）。

使用`bic`与`aic`方法计算BIC与AIC：

```python
gm.bic(X)
```

```python
gm.aic(X)
```

也可以手动计算BIC与AIC：

```python
n_clusters = 3
n_dims = 2
n_params_for_weights = n_clusters - 1
n_params_for_means = n_clusters * n_dims
n_params_for_covariance = n_clusters * n_dims * (n_dims + 1) // 2
n_params = n_params_for_weights + n_params_for_means + n_params_for_covariance
max_log_likelihood = gm.score(X) * len(X) # log(L^)
bic = np.log(len(X)) * n_params - 2 * max_log_likelihood
aic = 2 * n_params - 2 * max_log_likelihood
```

```python
bic, aic
```

```python
n_params
```

下面展示在不同数量的簇的情况下的BIC与AIC得分：

> Figure 9-21 shows the BIC for different numbers of clusters $k$. 

```python
gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)
             for k in range(1, 11)]
```

```python
bics = [model.bic(X) for model in gms_per_k]
aics = [model.aic(X) for model in gms_per_k]
```

```python
plt.figure(figsize=(8, 3))
plt.plot(range(1, 11), bics, "bo-", label="BIC")
plt.plot(range(1, 11), aics, "go--", label="AIC")
plt.xlabel("$k$", fontsize=14)
plt.ylabel("Information Criterion", fontsize=14)
plt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50])
plt.annotate('Minimum',
             xy=(3, bics[2]),
             xytext=(0.35, 0.6),
             textcoords='figure fraction',
             fontsize=14,
             arrowprops=dict(facecolor='black', shrink=0.1)
            )
plt.legend()
plt.show()
```

可以看出，当$k=3$时，BIC与AIC均最低，因此它最可能是最佳选择。注意，也可以搜索超参数`covariance_type`的最佳值。例如假设它是`spherical`而不是`full`，模型要学习的参数数量显著减少，但是拟合效果较差。

> then the model has significantly fewer parameters to learn, but it does not fit the data as well.

```python
min_bic = np.infty

for k in range(1, 11):
    for covariance_type in ("full", "tied", "spherical", "diag"):
        bic = GaussianMixture(n_components=k, n_init=10,
                              covariance_type=covariance_type,
                              random_state=42).fit(X).bic(X)
        if bic < min_bic:
            min_bic = bic
            best_k = k
            best_covariance_type = covariance_type
```

```python
best_k
```

```python
best_covariance_type
```

# 人工神经网络

**人工神经网络（Artificial Neural Networks，ANN）**是机器学习模型，其灵感来源于我们大脑中的生物神经元网络，但是它的概念与后者逐渐不同。

## 基本概念

### 使用神经元进行逻辑运算

McCulloch和Pitts提出了一个非常简单的生物神经元模型，后来被称为**人工神经元（artificial neuron）**：它有一个或多个二元（开/关）输入与一个二元输出。当超过一定数量的输入被激活时，人工神经元激活其输出。在他们的论文中，他们展示了即使使用如此简单的模型也可以构建一个可以计算任意逻辑命题的人工神经元网络。为了展示这样的网络如何工作，下图构建一些执行各种逻辑运算的ANN，并假设当一个神经元至少有两个输入被激活时，该神经元被激活：

![执行简单逻辑运算的ANN](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\执行简单逻辑运算的ANN.png)

这些神经元工作方式如下：

- 第一个网络是恒等函数（identity function）：如果神经元A被激活，则神经元C被激活（因为它从A接收两个输入信号（signals））；如果A被关闭（off），则C也被关闭。
- 第一个网络执行一个逻辑与：只有当神经元A与B都被激活，神经元C才被激活。
- 第三个网络执行逻辑或：当神经元A或B被激活（或都被激活），神经元C被激活。
- 最后，假设一个输入连接可以抑制神经元的活性（activity）（生物神经元中存在这种情况），则第四个网络计算一个稍微更复杂的逻辑命题：只有当神经元A被激活且神经元B被关闭，神经元C才被激活。如果神经元A一直被激活，则得到一个逻辑非：神经元C被激活当且仅当神经元B被关闭。

### 感知器

**感知器（Perceptron）**是最简单的ANN结构之一，它由Frank Rosenblatt发明于1957年。它基于一个**阈值逻辑单元（threshold logic unit，TLU）**，有时也被称为**线性阈值单元（linear threshold unit，LTU）**，如下图所示。TLU的输入与输出都是数字（而不是开/关值），每个输入连接过关联一个权重。TLU计算输入的加权和（$z=w_1x_1+w_2x_2+...+w_nx_n=\pmb{x}^T\pmb{w}$，然后对求和结果应用一个**阶跃函数（step function）**并输出结果：$h_{\pmb{w}}(\pmb{x}=step(z))$，其中$z=\pmb{x}^T\pmb{w}$。

![阈值逻辑单元](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\阈值逻辑单元.png)

感知器中最常用的阶跃函数是**Heaviside阶跃函数（Heaviside step function）**：
$$
heaviside(z)=
\left\{
\begin{aligned}
0\ if\ z<0\\
1\ if\ z\ge0
\end{aligned}
\right.
$$
有时也用符号函数（sign function）代替之：
$$
sign(z)=
\left\{
\begin{aligned}
-1\ if\ z<0\\
0\ if\ z=0\\
+1\ if\ z>0
\end{aligned}
\right.
$$


单个TLU可用于简单的线性二分类。它计算输入的一个线性组合，如果结果超过一个阈值，则输出正类，否则输出负类（就像Logistics回归或线性SVM分类器那样）。可以使用单个TLU，基于花瓣长度与宽度（并加上一个额外偏置（bias）特征$x_0=1$）去分类iris花，就像之前做过的那样。在这种情况下，训练一个TLU意味着找到正确的$w_0$、$w_1$与$w_2$值。

一个感知器仅由一层TLU组成（感知器有时也用来表示一个只有一个TLU的小型网络），每个TLU连接到所有输入。当一层的所有神经元都连接到前一层的每个神经元（即它的输入神经元），该层被称为**全连接层（fully connected layer）**，或**密集层（dense layer）**。感知器的输入被送到特殊的直通神经元，被称为**输入神经元（input neurons）**。它们将传递给它们的输入输出。所有的输入神经元形成**输入层（input layer）**。另外，通常加上一个额外的偏置（bias）特征（$x_0=1$）：它通常用一个特殊类型的神经元表示，该神经元被称作**偏置神经元（bias neuron）**，它总是输出1。下图表示了一个有两个输入与三个输出的感知器，该感知器可以同时将实例分类为三个不同的二分类，这使得它称为一个多输出分类器。

![一个有两个输入神经元、一个偏置神经元与三个输出神经元的感知器的结构](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个有两个输入神经元、一个偏置神经元与三个输出神经元的感知器的结构.png)

<a name="(神经网络)(基本概念)(感知器)(1)">多个实例的全连接层的输出计算如下：</a>


$$
h_{\pmb{W},\pmb{b}}=\phi(\pmb{X}\pmb{W}+\pmb{b})
$$
其中：

- $\pmb{X}$表示输入特征矩阵，每个实例一行，每个特征一列。
- 权重矩阵$\pmb{W}$包含所有连接权重，除了来自偏置神经元的权重。每个输入神经元一行，每个该层的人工神经元一列。
- 偏置向量$\pmb{b}$包含所有偏置神经元与人工神经元之间的权重。每个人工神经元一个偏置项。
- 函数$\phi$被称为**激活函数（activation function）**。当人工神经元是TLU时，它是阶跃函数。

> It has one row per input neuron and one column per artificial neuron in the layer.

Rosenblatt提出的感知器训练算法很大程度上受到了**Hebb规则（Hebb's rule）**的启发。在他的1949年的著作《The Organization of Behavior》 (Wiley)中，Donald Hebb提出，当一个生物神经元经常触发另一个神经元时，两个神经元之间的联系（connection）会变得更强。这个规则后来被称为Hebb规则（或**Hebbian学习（Hebbian Learning）**。感知器使用该规则的一个变体训练，它考虑到了网络预测时的错误，感知器的学习规则增强（reinforce）有助于减少错误的连接。更具体地，感知器每个接受一个训练实例，对每个实例，它都作出预测。对于每个产生错误预测的输出神经元，它增强来自输入的、有助于作出正确预测的连接权重。感知器学习规则如下：
$$
w_{i,j}^{(next\ step)}=w_{i,j}+\eta(y_j-\widehat{y}_j)x_i
$$
其中：

- $w_{i,j}$为第$i$个输入神经元与第$j$个输出神经元之间的连接权重。
- $x_i$为当前训练实例的第$i$个输入值。
- $\widehat{y}_j$为当前训练实例的第$j$个输出神经元的输出。
- $y_j$为当前训练实例的第$j$个输出神经元的目标输出。
- $\eta$为学习率。

> Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire together, wire together”; that is, the connection weight between two neurons tends to increase when they fire simultaneously. 

每个输出神经元的决策边界是线性的，因此感知器不能学习复杂模式（就像Logistic回归分类器一样）。但是如果训练实例线性可分，Rosenblatt证明该算法将收敛到一个解（注意这个解不是唯一的：当数据点线性可分，有无数的超平面可以将它们分开）。这被称为**感知器收敛理论（Perceptron convergence theorem）**。

Scikit-Learn提供了`Perceptron`类，它实现了单个TLU（a single-TLU）网络。下面在iris数据集上使用它：

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:, (2, 3)]  # petal length, petal width
y = (iris.target == 0).astype(np.int)

per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)
per_clf.fit(X, y)

y_pred = per_clf.predict([[2, 0.5]])
```

感知器学习算法与随机梯度下降十分相似。事实上，Scikit-Learn的`Perceptron`类等价于给定以下超参数使用`SGDClassifier`：`loss="perceptron"`、`learning_rate="constant"`、`eta0=1`（学习率）以及`penalty=None`（没有正则化）。

与Logistic回归分类器不同，感知器不会输出类别概率，而是基于硬阈值（hard threshold）作出预测。这是优先使用Logistic回归而不是感知器的原因之一。

在他们1969年的专著《Perceptrons》中，Marvin Minsky与Seymour Papert强调感知器的一系列严重缺点——尤其是，它们不能解决一些简单（trivial）问题（例如[图1](#(神经网络)(基本概念)(感知器)(多层感知器)(1))的异或分类问题）。任意线性分类模型都存在这些问题（例如Logistic回归分类器）。

#### 多层感知器

感知器的一些局限性可以通过堆叠多个感知器来消除，由此产生的ANN被称为**多层感知器（Multilayer Perceptron，MLP）**。MLP可以解决异或问题，如下图所示：输入为$(0,0)$或$(1,1)$，该网络输出$0$；输入为$(0,1)$或$(1,0)$，该网络输出$1$。所有连接的权重为1，除了四个权重如图所示的连接（注意层间有阶跃函数）。

<a name="(神经网络)(基本概念)(感知器)(多层感知器)(1)">![异或分类问题以及一个解决它的MLP](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\异或分类问题以及一个解决它的MLP.png)</a>

一个MLP由一个（直通（passthrough））**输入层（input layer）**、一个或多个TLUs（称为**隐藏层（hidden layers）**）以及一个最终的TLU（称为**输出层（output layer）**）组成，如下所示。接近输入层的层通常被称为**下层（lower layers）**，接近输出层的层通常被称为**上层（upper layers）**。除了输出层，所有层都包含一个偏置神经元，并且全连接到下一层。

![一个多层感知器的结构，它有两个输入、一个包含4个神经元的隐藏层以及3个输出神经元（这里显示了偏置神经元，但通常它们是隐含的）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\多层感知器.png)

由于信号只能沿一个方向流动（从输入到输出），所以这种结构是**前馈神经网络（feedforward neural network，FNN）**的一个例子。

当一个ANN包含一个深度堆叠隐藏层，它被称为**深度神经网络（deep neural network，DNN）**（在20世纪90年代，一个包含两个以上隐藏层的ANN被认为是深度的。现在，经常能看见有数十层甚至数百层的ANN，因此“深度”的定义相当模糊）。深度学习领域研究DNN，更一般地，它研究包含深度堆叠运算的模型。即使如此，只要涉及神经网络（即使是浅层的），很多人就认为它涉及深度学习。

##### 回归MLP

MLP可用于回归任务。如果要预测单个值，则只需要1个输出神经元，它的输出就是预测值。对于多变量回归，则每个输出维度都需要一个输出神经元。例如，要定位图像中物体的中心，需要预测2维坐标，因此需要2个输出神经元。如果还要在对象周围放置边界框，则还需要两个数字：对象的宽度与高度，因此此时需要4个输出神经元。

一般来说，当为回归构建MLP时，对于输出神经元不使用任何激活函数，因此它们可以输出任意范围的值。如果需要保证输出总是正值，则可以在输出层使用ReLU激活函数，也可以使用softplus激活函数。如果要保证预测值位于给定范围内，则可以使用logistic函数或双曲正切，并缩放标签到正确（appropriate）的范围内：对于logistic函数，它是0\~1；对于双曲正切，它是-1\~1。

训练过程中的损失函数通常是均方误差，但是如果训练集中的离群值太多，则可以使用平均绝对误差。也可以使用Huber损失（Huber loss），它是两者的结合：
$$
Huber(y, \widehat{y})=\left\{
\begin{aligned}
\frac{1}{2}(y-\widehat{y})^2\ if\ \abs{y-\widehat{y}}\le\sigma\\
\sigma\abs{y-\widehat{y}}-\frac{1}{2}\sigma^2\ otherwise
\end{aligned}
\right.
$$

当误差小于某个阈值$\sigma$（通常为1）时Huber损失是二次的；当误差大于$\sigma$时，Huber误差是线性的。线性部分使得Huber误差对离群值的敏感度小于均方误差；二次部分使得Huber误差比平均绝对误差的收敛速度更快且更准确。

下表总结了回归MLP的典型结构：

<table>
    <tr>
        <th>超参数</th>
        <th>典型值</th>
    </tr>
    <tr>
        <td>输入神经元</td>
        <td>每个输入特征1个</td>
    </tr>
    <tr>
        <td>隐藏层</td>
        <td>取决于问题，通常1~5层</td>
    </tr>
    <tr>
        <td>每个隐藏层的神经元</td>
        <td>取决于问题，通常10~100个</td>
    </tr>
    <tr>
        <td>输出神经元</td>
        <td>每个预测维度1个</td>
    </tr>
    <tr>
        <td>隐藏激活函数（hidden activation）</td>
        <td>ReLU或SELU</td>
    </tr>
    <tr>
        <td>输出激活函数（output activation）</td>
        <td>没有；ReLU/softplus（正输出）；logistic/tanh（有界输出）</td>
    </tr>
    <tr>
        <td>损失函数</td>
        <td>MSE；MAE；Huber（离群值）</td>
    </tr>
</table>

##### 分类MLP

MLP也可用于回归任务。对于二分类问题，只需要单个输出神经元并使用logistic激活函数，输出为介于0\~1之间的一个数字，可被解释为正类的估计概率。负类的估计概率等于1减去该数字。

对于多标签二分类任务，则每个正类对应一个输出神经元。例如，如果要构建一个邮件分类系统，去预测到来的邮件是不是垃圾邮件，同时预测它是不是紧急邮件，则需要两个输出神经元，它们同时使用logistic激活函数，分别输出邮件为垃圾邮件的概率、邮件为紧急邮件的概率。注意输出概率之和不需要等于1。这允许模型输出任意标签组合：非紧急正常邮件、紧急正常邮件、非紧急垃圾邮件甚至是紧急垃圾邮件（可能是一个错误）。

如果每个实例只能属于三个或更多可能类别（例如数字图像分类的类别0\~9）中的一个类别，则每个类别需要一个输出神经元，并对整个输出层使用sofmax激活函数（如图）。softmax函数保证所有的估计概率介于0\~1且加起来等于1（如果类别互斥则这是必要的）。这被称为多类分类（multiclass classification）。

![用于分类的现代MLP（包括ReLU与softmax）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\分类MLP.png)

因为要预测概率分布，所以通常可以使用交叉熵损失（cross-entropy loss）（也被称为log损失（log loss））。

下表总结了分类MLP的典型结构：

<table>
    <tr>
        <th>超参数</th>
        <th>二分类</th>
        <th>多标签二分类</th>
        <th>多类分类</th>
    </tr>
    <tr>
        <td>输入与隐藏层</td>
        <td>与回归相同</td>
        <td>与回归相同</td>
        <td>与回归相同</td>
    </tr>
    <tr>
        <td>输出神经元</td>
        <td>1</td>
        <td>每个标签1个</td>
        <td>每个类别一个</td>
    </tr>
    <tr>
        <td>输出层激活函数</td>
        <td>Logistic</td>
        <td>Logistic</td>
        <td>Softmax</td>
    </tr>
    <tr>
        <td>损失函数</td>
        <td>交叉熵</td>
        <td>交叉熵</td>
        <td>交叉熵</td>
    </tr>
</table>

### 反向传播

很多年来研究人员在苦苦寻找训练MLP的方法，但是都失败了。1986年，David Rumelhart、Geoffrey Hinton与Ronald Williams发表了一篇[开创性的论文](https://homl.info/44)，它引入了（introduced）**反向传播（backpropagation）**训练算法，至今仍被使用。它是梯度下降，但是使用了反向自动微分去自动计算梯度。当得到这些梯度后，它就执行普通的梯度下降步，整个过程重复直到网络收敛到解。

> In short, it is Gradient Descent (introduced in Chapter 4) using an efficient technique for computing the gradients automatically:

> This technique was actually independently invented several times by various researchers in different fields, starting with Paul Werbos in 1974.

下面是对算法更详细的介绍：

- 它每次处理一个小批量（例如每次包含32个实例），然后多次遍历整个训练集。每次遍历被称作一**代（epoch）**。
- 每个小批量被传递到网络的输入层，然后输入层将其送到第一个隐藏层。然后算法计算该层所有神经元的输出（对于小批量的每个实例都如此），结果被传递到下一层，计算它的输出并将其传递到下一层，如此下去直到获得输出层的输出。这就是**前向传递（forward pass）**，它就像预测一样，只是所有中间结果必须保留，因此在**反向传递（reverse pass）**过程中需要它们。
- 接下来，算法度量网络的输出误差（即使用损失函数比较网络的期望输出与实际输出，并返回误差的某个度量）。
- 然后算法计算每个输出连接对误差的贡献程度。这是通过应用**链式法则（chain rule）**来实现的，它使得该步骤快速、准确。
- 然后算法再次使用链式法则度量这些误差贡献中有多少来自下层的每个连接，直到算法到达输入层。反向传递通过传播误差梯度，高效地度量网络中所有的权重矩阵的误差梯度。
- 最后，算法使用刚刚计算好的误差梯度，执行一个梯度下降步去微调网络中的所有连接权重。

> The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer. As explained earlier, this reverse pass
>
总而言之，对于每个训练实例，反向传播算法首先作出预测（前向传递）并度量误差，然后反向通过每一层去度量来自每个连接的误差贡献（反向传递），最后微调连接权重去减少误差（梯度下降步）。

使用反向传播算法需要注意，所有隐藏层的连接权重需被随机初始化（偏置可以不用随机初始化），否则训练会失败。例如，如果将所有权重与偏置都初始化为0，则给定层的所有神经元将完全相同，因此反向传播将以完全一样的方式影响它们，因此它们将保持不变，这样模型就好像每层只有一个神经元一样。如果随机初始化所有权重，则可以打破对称性（break the symmetry）并允许反向传播去训练多样的神经元。

### 激活函数

如果连接若干个线性变换，得到的结果仍然是线性变换。因此如果层之间没有非线性（nonlinearity），即使一组深度堆叠的层也等价于一个层，因此无法用它来解决复杂问题。这是为什么需要激活函数的原因：一个拥有非线性激活函数的足够大的DNN理论上可以近似任何连续函数。

为了使得反向传播算法正常工作，它的作者们对ML的结构作出一个关键改变：他们用logistic（sigmoid）函数：$\sigma(z)=1/1(1+\exp(-z))$，替换了阶跃函数。这很重要，因为阶跃函数只包括平段（flat segments），因此没有梯度可供使用（梯度下降不能在平面（flat surface）上移动），而logistic函数在每个地方都有定义明确的（well-defined）非零偏导，这允许梯度下降每步前进。反向传播同样能与很多其他激活函数使用，包括：

- **双曲正切函数（the hyperbolic tangent function）**：$tanh(z)=2\sigma(2z)-1$。与logistic函数一样，它也是S形的、连续的并且可微，但是它的输出范围为-1\~1。这个范围倾向于使得训练开始时，每层的输出或多或少集中在0左右，从而有助于加速收敛。
- **修正线性单元函数（the Rectified Linear Unit function）**：$ReLU(z)=max(0,z)$。该函数连续但在$z=0$处不可微（斜率急剧变化，这回导致梯度下降弹跳），并且当$z<0$时导数为0。但是在实践中，它工作得很好，并且计算快速，所以它成了默认设置。更重要的是，因为它没有最大输出值，这有助于减轻梯度下降过程中的一些问题。
- **softplus**激活函数，它是ReLU的平滑变体（a smooth variant of ReLU）：$softplus(z)=\log(1+exp(z))$。当$z$为负数时，它接近0，当$z$为正数时，它接近$z$。

下面绘制出这些常见的激活函数及其导数：

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def relu(z):
    return np.maximum(0, z)

def derivative(f, z, eps=0.000001):
    return (f(z + eps) - f(z - eps))/(2 * eps)
```

```python
import matplotlib.pyplot as plt

z = np.linspace(-5, 5, 200)

plt.figure(figsize=(11,4))

plt.subplot(121)
plt.plot(z, np.sign(z), "r-", linewidth=1, label="Step")
plt.plot(z, sigmoid(z), "g--", linewidth=2, label="Sigmoid")
plt.plot(z, np.tanh(z), "b-", linewidth=2, label="Tanh")
plt.plot(z, relu(z), "m-.", linewidth=2, label="ReLU")
plt.grid(True)
plt.legend(loc="center right", fontsize=14)
plt.title("Activation functions", fontsize=14)
plt.axis([-5, 5, -1.2, 1.2])

plt.subplot(122)
plt.plot(z, derivative(np.sign, z), "r-", linewidth=1, label="Step")
plt.plot(0, 0, "ro", markersize=5)
plt.plot(0, 0, "rx", markersize=10)
plt.plot(z, derivative(sigmoid, z), "g--", linewidth=2, label="Sigmoid")
plt.plot(z, derivative(np.tanh, z), "b-", linewidth=2, label="Tanh")
plt.plot(z, derivative(relu, z), "m-.", linewidth=2, label="ReLU")
plt.grid(True)
#plt.legend(loc="center right", fontsize=14)
plt.title("Derivatives", fontsize=14)
plt.axis([-5, 5, -0.2, 1.2])

plt.show()
```

## 微调神经网络超参数

神经网络的灵活性也是它们的主要缺点之一：有很多的超参数需要微调。神经网络的结构众多，然而即使一个简单的MLP，层的数量、每层的神经元的数量、每层使用的激活函数的类型、权重初始化逻辑等等都可以改变。下面是一些寻找最优的超参数组合的方法。

一种方法是尝试许多的超参数组合，看哪个组合在验证集上工作得最好（或使用交叉验证）。例如，可以使用`GridSearchCV`或`RandomizedSearchCV`去探索超参数空间。随机搜索不太困难，并且对很多相当简单的模型工作得很好。如果训练很慢（如对有更大的数据集的更复杂的问题），该方法只能探索超参数空间的很小一部分。此时可以手动协助搜索过程：首先使用大范围（wide ranges of）内的超参数值运行一个快速随机搜索，然后使用在第一次运行时发现的最好的超参数值为中心的较小范围内的值运行另一个搜索。该方法有希望聚焦一组好的超参数。但是，该方法非常耗时，未必是最好的方式。

有许多其他技术比随机搜索更能有效探索搜索空间。它们的核心思想很简单：当某个空间区域表现很好，则它应该被探索更多。这些技术考虑了“聚焦”过程并可在短得多的时间内产生更好的结果。一些可以用于优化超参数的Python库括：[Hyperopt](https://github.com/hyperopt/hyperopt)；[Hyperas](https://github.com/maxpumperla/hyperas)、[kopt](https://github.com/Avsecz/kopt)与[Talos](https://github.com/autonomio/talos)；[Keras Tuner](https://homl.info/kerastuner)；[Scikit-Optimize (`skopt`)](https://scikit-optimize.github.io/)；[Spearmint](https://github.com/JasperSnoek/spearmint)；[Hyperband](https://github.com/zygmuntz/hyperband)；[Sklearn-Deap](https://github.com/rsteca/sklearn-deap)。


(Introduction to Artificial Neural Networks with Keras)(Fine-Tuning Neural Network Hyperparameters)(Fortunately, there are many techniques ...))

尽管取得了诸多进展，有很多的工具有服务可供使用，了解每个超参数的合理值仍然有助于构建一个快速原型并限制搜索空间。下面提供一些指南，用于选择MLP的隐藏层与神经元的数量，并未一些主要超参数选择好的取值。有关调整神经网络超参数的更多最佳实践，查看Leslie Smith [2018年的优秀论文](https://homl.info/1cycle)。


### 隐藏层数量

对于许多问题，可以先选择单个隐藏层并得到合理（reasonable）结果。一个只有一个隐藏层的MLP理论上可以对最复杂的函数建模，只有它有足够多的神经元。但是对于复杂问题，深度网络比浅层网络的**参数效率（parameter efficiency）**要高得多：相比浅层网络，它们可以使用少的多的神经元对复杂函数建模，这使得它们能够在相同数量的训练集下，取得好得多的性能。

做个类比，假设要使用绘图软件绘制一个森林，并且不能拷贝或粘贴任何内容。这将花费大量的时间，因为必须一枝一叶地，独立地绘制所有的树。如果能够绘制一片树叶，通过复制并粘贴它来绘制一个树枝，然后复制并粘贴树枝来创建一个树，最后复制并粘贴这个树来得到一个森林，则无需花费很多时间。现实数据常常有这样的层状结构，深度神经网络自动利用了这个事实：低隐藏层建模低层结构（例如各种形状与方向的线段），中间隐藏层组合这些低层结构去建模中层结构（例如正方形、圆形），最高的隐藏层与输出层结构这些中间结构去建模高层结构（例如人脸（faces））。


这种层状结构不仅能帮助DNN快速收敛到好的解，而且能够改善它们泛化到新数据集的能力。例如，如果已经训练了一个模型去识别图片中的人脸，现在需要训练一个新的神经网络去识别发型（hairstyles），则可以重用第一个网络的低层来启动训练。此时可以新神经网络的前几层的权重与偏置初始化为第一个神经网络低层的权重与偏置值，而不是随机初始化它们。这样，网络不需要从头学习出现在大多数图片中的所有低层结构，它只需要学习高层结构（如发型）。这被称为**迁移学习（transfer learning）**。

总之，对于许多问题可以先只选择一两个隐藏层，并且神经网络可以工作得很好。例如，在MNIST数据集上，只需使用一个包含数百个神经元的隐藏层，即可轻松达到97%以上的准确率；使用两个隐藏层，并且神经元总数相同，在大致相同的训练时间内，可以达到98%以上的准确率。对于更复杂的问题，可以增加隐藏层数量直到开始过拟合训练集。对于非常复杂的问题，例如大型图像分类或语音识别，通常需要几十层的网络（甚至几百层，但不是全连接的），并且它们需要大量的训练集。很少需要从头训练这样的网络，重用执行类似任务的经过预训练的先进模型的一部分要常见得多，此时训练将快得多且需要少得多的数据。

## 每层神经元的数量

输入与输出层的神经元数量由任务所需的输入输出类型决定。例如，MNIST任务需要$28\times 28=784$个输入神经元与10个输出神经元。

对于隐藏层，以前常见的做法是将它们的大小调整为金字塔，使得每层的神经元数量越来越少。这么做的理由是许多低层特征可以合并为少得多的高层特征。对于MNIST，一个典型的神经网络可能含有3个隐藏层，第一层有300个神经元，第二层200，第三层100。但是这种实践在很大程度上已被放弃，因为看上去在所有隐藏层使用相同数量的神经元，在大多数情况下的性能一样好，甚至更好，并且这样的话，只有一个超参数需要调整，而不是每层一个。

与层的数量一样，可以逐渐增大神经元的数量直到网络开始过拟合。但在实践中，选择一个网络，它的层与神经元的数量比实际需要的更多，然后使用早停法或其他正则化技术防止其过拟合，常常更简单，也更高效。通过这种方法，可以避免可能破坏模型的瓶颈层；另一方面，如果某层的神经元数量太少，它将没有足够的表示能力去保存输入中的所有有用信息（例如，一个有两个神经元的层只能输出2维数据，因此如果处理3维数据，一些信息会丢失），不管剩余层多大多强，这些信息永远不能恢复。

通常，增加层的数量带来的收益比增加每层神经元带来的收益更大。

## 学习率

学习率可以说是最重要的超参数。通常，最优的学习率大约是最大学习率（超过最大学习率，学习算法将发散）的一半。一种找到好的学习率的方法是：通过几百次迭代来训练模型，开始于非常低的学习率（例如$10^{-5}$），然后逐渐增加它直到一个非常大的值（例如$10$）。这是通过在每次迭代时将学习率乘以一个常数因子实现的（例如乘以$\exp(\log(10^6)/500)$使得通过500次迭代从$10^{-5}$增加到$10$）。如果绘制出损失相对于学习率的函数（使用学习率的对数尺度），会发现它首先下降，但是过了一段时间，学习率太大，因此损失迅速增加。最优学习率略低于损失开始上升的点（通常比转折点低10倍左右）。然后可以重新初始化模型并使用这个好的学习率正常训练模型。

注意，最优学习率依赖于其他超参数，尤其是批大小。因此，如果修改了任何超参数，确保更新学习率。

## 优化器

选择一个比普通的小批量梯度下降更好的优化器（并调整它的超参数）也很重要。

## 批大小

批大小对模型性能与训练时间影响显著。使用大批的主要优点在于像GPU这样的硬件加速器可以高效处理它们，因此训练算法每秒可以看到更多实例。因此，需要研究人员与从业者建议使用GPU内存可以容纳的最大批。但是在实践中，大批常常导致训练不稳定性，尤其在训练的开始，并且得到的模型的泛化能力可能不如使用小批得到的模型。Dominic Masters与 Carlo Luschi在[2018年发表的论文](https://homl.info/smallbatch)中得到的结论是：使用小批（2到32）要更好，因为小批能够在更短的训练时间内产生更好的模型。其他论文则持相反观点。在2017年，[Elad Hoffer等人](https://homl.info/largebatch)以及[Priya Goyal等人](https://homl.info/largebatch2)的论文表明，使用各种技术，例如预热（warming up）学习率（使用一个小的学习率开始训练，然后逐步加大它），可以使用非常大的学习率（到8192）。这导致训练时间很短，并且没有任何泛化差距（gap）。所以，一种策略是尝试使用大批并使用学习率预热（warmup），如果训练不稳定或最终性能不好，则尝试使用小批。



## 激活函数

前面提到，通常，对于所有隐藏层，ReLU是好的默认设置。对于输出层，则取决于任务。

## 迭代次数

在大多数情况下，实际上不需要调整训练迭代次数，只需使用早停法即可。

# <a name=(模型构建)>模型构建</a>

## 顺序API

### 使用顺序API构建分类模型

下面使用Keras构建一个简单的图像分类器。

导入TensorFlow与keras并查看版本：

```python
import tensorflow as tf
from tensorflow import keras
```

```python
tf.__version__
```

```python
keras.__version__
```

首先需要加载数据集。本例处理Fashion MNIST数据集，这是MNIST的替代版本。它的格式与MNIST一样（70000张灰度图像，每张28 * 28像素，10个类别），但是图像表示时尚用品而不是手写数字，所以每个类别更加多样（diverse），并且（分类）问题明显比MNIST更具挑战性。例如一个简单的线性模型在MNIST上可以得到92%的准确率，但是在Fashion MNIST上只能得到83%的准确率。

Keras提供了一些实用函数（utility functions）去获取并加载常见的数据集，包括MNIST、Fashion MNIST与California housing数据集（见[机器学习工程清单](#(机器学习工程清单))），下面加载Fashion MNIST：

```python
fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()
```

当使用Keras而不是Scikit-Learn加载MNIST与Fashion MNIST时，一个重要的区别在于每张图片被表示成$28\times28$的数组而不是大小为$784$的一维数组。并且像素灰度（intensities）用0\~255之间的整型表示（而不是0.0\~255.0之间的浮点数）。查看训练集的形状与数据类型：

```python
X_train_full.shape
```

```python
X_train_full.dtype
```

数据集已经被划分为训练集与测试集，但是没有验证集，所有现在创建验证集。因为要使用梯度下降训练神经网络，因此必须要缩放输入特征。为了简便，这里将像素灰度除以255.0缩放到0\~1的范围（这同时将它们转换为浮点数）：

```python
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
X_test = X_test / 255.
```

列出类别名称：

```python
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
```

有了类别名称与标签，就可以知道每个实例表示什么物品，例如第一张图片表示一个“coat”：

```python
class_names[y_train[0]]
```

查看验证集、测试集的大小：

```python
X_valid.shape
```

```python
X_test.shape
```

下面展示数据集中的一些样本：

```python
import matplotlib.pyplot as plt

n_rows = 4
n_cols = 10
plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))
for row in range(n_rows):
    for col in range(n_cols):
        index = n_cols * row + col
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(X_train[index], cmap="binary", interpolation="nearest")
        plt.axis('off')
        plt.title(class_names[y_train[index]], fontsize=12)
plt.subplots_adjust(wspace=0.2, hspace=0.5)
plt.show()
```

下面开始构建神经网络。以下是一个包含两个隐藏层的分类MLP：

```python
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="relu"))
model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dense(10, activation="softmax"))
```

第一行创建了一个`Sequential`模型，这是最简单的Keras神经网络模型，它只包含顺序连接的堆叠层。这被称为顺序API（Sequential API）。

然后构建第一层并将其加到模型上，这是`Flatten`层，用于将每张输入图片转换为一维数组（接受输入数据`X`，计算`X.reshape(-1, 1）`。这层没有任何参数，只做简单的预处理工作。因为它是模型的第一层，所以要指定`input_shape`（不包括批大小，只是实例的形状）。也可以将`keras.layers.InputLayer`作为第一层，并设置`input_shape=[28, 28]`。

接下来加一个`Dense`隐藏层，包含300个神经元，它使用ReLU激活函数。每个`Dense`层管理它自己的权重矩阵，其包含神经元与其输入之间的所有连接权重。它也管理一个偏置项向量（每个神经元一个）。当接收输入数据，它计算[公式1](#(神经网络)(基本概念)(感知器)(1))。

然后再加一个`Dense`隐藏层，包含100个神经元，同样使用`ReLU`激活函数。

最好再加一个`Dense`输出层，包含10个神经元（每个类别一个），使用softmax激活函数（因为各个类别互斥）。

指定`activation="relu"`等价于指定`activation=keras.activations.relu`。其他的激活函数见`keras.activations`包。完整列表见[https://keras.io/activations/](https://keras.io/activations/)。

除了每次添加一个层外，也可以在创建`Sequential`模型时传递层的列表。

```python
import numpy as np

keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])
```

使用模型的`summary`方法列出模型的所有层，包括每个层的名字（自动生成，除非在创建层时设置它）、输出形状（`None`表示批大小任意）与它的参数数量：

```python
model.summary()
```

摘要（summary）的最后是参数的总数量，包括可训练与不可训练的参数。这里只有可训练的参数。模型的参数很多，这给了模型很大的灵活度以拟合训练数据，但也意味着模型有过拟合的风险，尤其当没有很多训练数据时。

使用`keras.utils.plot_model`生成模型的图像：

```python
keras.utils.plot_model(model, show_shapes=True)
```

获得模型的层列表：

```python
model.layers
```

根据索引或名称获取层：

```python
hidden1 = model.layers[1]
hidden1.name
```

```python
model.get_layer(hidden1.name) is hidden1
```

一个层的所有参数可以通过`get_weights`与`set_weight`访问，对于`Dense`层，这包括连接权重与偏置项：

```python
weights, biases = hidden1.get_weights()
```

```python
weights
```

```
weights.shape
```

```
biases
```

```
biases.shape
```

`Dense`层随机初始化连接权重，而偏置被初始化为0，这很好。如果要使用一个不同的初始化方法，可以在创建层时设置`kernel_initializer`（“kernel”是连接权重矩阵的另一个名字）或`bias_initializer`。初始化器（initializer）的完整列表见[https://keras.io/initializers/](https://keras.io/initializers/)。

权重矩阵的形状取决于输入数量。这也是推荐在创建`Sequential`模型的第一层时指定`input_shape`的原因。如果不指定输入形状，也没问题，Keras会在实际构建模型前一直等待直到它知道输入形状。这会在为其提供实际数据（例如在训练过程中）或调用它的`build`方法时发生。在模型被实际构建前，层不会有任何权重，此时无法执行某些操作（例如打印模型摘要或保存模型）。因此，如果创建模型时直到输入形状，最好指定它。

模型创建后，必须调用它的`compile`方法去指定要使用的损失函数与优化器（optimizer）。也可以指定在训练或评估过程中需要计算的额外指标列表：

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer="sgd",
              metrics=["accuracy"])
```

使用`loss="sparse_categorical_crossentropy"`等价于使用`loss=keras.losses.sparse_categorical_crossentropy`。类似地，指定`optimizer="sgd" `等价于指定`optimizer=keras.optimizers.SGD()`；指定`metrics=["accuracy"]`等价于指定`metrics=[keras.metrics.sparse_categorical_accuracy] `。损失、优化器与指标的完整列表见：[https://keras.io/losses](https://keras.io/losses)、[https://keras.io/optimizers](https://keras.io/optimizers)与[https://keras.io/metrics](https://keras.io/metrics)。

因为我们有稀疏标签（即对于每个实例，有一个目标类别的索引）并且类别互斥，所以这里使用`sparse_categorical_crossentropy`损失。如果每个实例的每个类别有一个目标概率（例如独热向量，如`[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]`表示类别3），则需要使用`categorical_crossentropy`损失。如果处理二分类任务（一个或多个二元（binary）标签），则需要在输出层使用`sigmoid`（即logistic）激活函数替代`softmax`激活函数，并使用`binary_crossentropy`损失。

如果要将稀疏标签（即类别索引）转换为独热向量标签，可以使用`keras.utils.to_categorical`函数；反之，使用`np.argmax`函数并指定`axis=1`。

优化器`sgd`意味着我们将使用简单的随机梯度下降去训练模型，Keras将使用反向自动微分与梯度下降执行反向传播。

当使用SGD优化器时，调整学习率很重要。因此，通常需要使用`optimizer=keras.optimizers.SGD(lr=*???*)`去设置学习率，而不是使用`optimizer="sgd"`，它默认`lr=0.01`。

最后，因为这是一个分类器，所以在训练与评估过程中度量它的`accuracy`很有用。

调用模型的`fit`方法训练它：

```python
history = model.fit(X_train, y_train, epochs=30,
                    validation_data=(X_valid, y_valid))
```

我们传递给它输入特征、目标类别、训练的代数（默认为1，显然不足以收敛到好的解）以及验证集（可选）。Keras将会在每代的末尾，在验证集上度量损失与额外的指标，这有助于查看模型的实际性能如何。如果在训练集上的性能远远好于在验证集上的训练，则模型可能过拟合训练集（或存在错误（bug），例如训练集与验证集间数据不一致）。

也可以设置`validation_split`为希望Keras用来验证的训练集的比例，而不是使用`validation_data`实参传递一个验证集。

在训练的每代，Keras展示已处理的实例数量（伴随一个进度条）、每个样本的平均训练时间以及损失与在训练集与验证集上的准确率（或任意其他要求的额外指标）。可以看到训练误差下降，验证集准确率上升，且与训练集准确率相差不大，因此似乎没有太多的过拟合。

如果训练集非常倾斜，一些类别过多而另一些类别过少，则可以在调用`fit`方法时设置`class_weight`实参，并给过少的类别较大的权重，给过多的类别较小的权重。Keras在计算损失时会使用这些权重。如果需要为每个实例设置权重，则设置`sample_weight`实参（如果同时提供了`class_weight`与`sample_weight`，则Keras会将它们相乘）。如果一些实例由专家打标签，而另一些实例通过众包平台（crowdsourcing platform）打标签，则为每个实例设置权重很有用，此时可以给前者更大的权重。还可以为验证集提供样本权重（但不是类别权重），方法是将样本权重设置为`validation_data`元组的第三项。

`fit`方法返回一个`History`对象，它包含训练参数（`history.params`）、它经过的代的列表（`history.epoch`）以及一个字典（`history.history`），它包含损失与在训练集与验证集每代末尾度量的额外指标（如果有的话）。可以使用该字典创建一个pandas DataFrame并调用它的`plot`方法以得到如下所示的学习曲线（learning curves）：

```python
history.epoch
```

```python
history.params
```

```python
history.history.keys()
```

```python
import pandas as pd

pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()
```

可以看到训练准确率与验证准确率都在训练过程中稳定上升，而训练误差与验证误差都下降。验证曲线与训练曲线接近，这意味着没有太多的过拟合。在该例子中，看起来在训练开始，模型在验证集上的性能要好于在训练集上的性能。但事实并非如此，验证误差在每代末尾被计算，而训练误差则是每代过程中的运行均值。因此训练曲线应该向左移动半代，此时可以看到训练曲线与验证曲线在训练开始时几乎完全重合。

训练集性能最终超过了验证集性能，当训练足够长的时间后通常都会出现这种情况。模型并未完全收敛，因为验证误差还在下降，所以可以继续训练。只要再次调用`fit`方法即可，因为Keras会继续从停下的训练（最终应该能够达到接近89%的验证准确率）。

如果对模型性能不满意，则应该调整超参数。首先应该调整学习率，如果这没有用，则选择其他的优化器（注意改变任意超参数后都要重新调整学习率）。如果性能仍然不够好，则尝试调整模型超参数，例如层的数量、每层的神经元的数量，以及用于每个隐藏层的激活函数的类型。还可以尝试调整其他超参数，例如批大小（可通过`fit`方法的`batch_size`实参设置）。如果对模型的验证准确率满意，则在部署模型到生产环境前，应该在测试集上评估模型以估计泛化误差。这可以通过`evaluate`方法做到（它支持一些其他实参，例如`batch_size`与`sample_weight`）：

```python
model.evaluate(X_test, y_test)
```

前面提到，测试集上的性能常常比验证集上的低，因为超参数在验证集上被调整，而不是在在测试集上被调整（虽然本例没有做任何超参数调整，因此准确率低只是因为运气不好）。注意此时不要在测试集上微调超参数，否则泛化误差估计将会过于乐观。

接下来，可以使用模型的`predict`方法对新实例预测。因为没有实际的新实例，这里使用测试集的前三个实例：

```python
X_new = X_test[:3]
y_proba = model.predict(X_new)
y_proba.round(2)
```

对于每个实例，模型为每个类别估计一个概率。如果只关心估计概率最高的类别（即使该概率很低），则使用`np.argmax`方法：

```
y_pred = np.argmax(model.predict(X_new), axis=-1)
y_pred
```

```python
np.array(class_names)[y_pred]
```

这里，分类器将三个图像正确分类，这些图像如下：

```
y_new = y_test[:3]
y_new
```

```python
plt.figure(figsize=(7.2, 2.4))
for index, image in enumerate(X_new):
    plt.subplot(1, 3, index + 1)
    plt.imshow(image, cmap="binary", interpolation="nearest")
    plt.axis('off')
    plt.title(class_names[y_test[index]], fontsize=12)
plt.subplots_adjust(wspace=0.2, hspace=0.5)
plt.show()
```

### 使用顺序API构建回归模型

下面使用回归神经网络处理加州房价问题（the California housing problem）。

使用Scikit-Learn的`fetch_california_housing`函数加载数据。这个数据集比[机器学习工程清单](#(机器学习工程清单))中使用的数据集简单，因为它只包含数值特征（没有`ocean_proximity`特征），并且没有缺失值。数据加载后，将其分类为训练集、验证集与测试集并缩放所有特征：

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)
```

使用顺序API构建、训练与评估，以及使用回归MLP去预测与分类的方式类似。主要区别在于输出层只有一个神经元并且没有激活函数，并且损失函数是均方误差。因为数据集有相当多的噪音，为了防止过拟合，这里只使用一个隐藏层，且神经元数量比先前的更少：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=X_train.shape[1:]),
    keras.layers.Dense(1)
])
model.compile(loss="mean_squared_error", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))
mse_test = model.evaluate(X_test, y_test)
X_new = X_test[:3]  # 假设这些是新实例。
y_pred = model.predict(X_new)
```

```python
plt.plot(pd.DataFrame(history.history))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()
```

```python
y_pred
```

顺序API很好用，但是即使`Sequential`模型很常见，有时构建拓扑结构更复杂或拥有多输入输出的神经网络也很有用。Keras为此提供了函数式API（Function API）。

### 函数式API

非顺序神经网络的一个例子是**Wide & Deep**神经网络。该神经网络结构由Heng-Tze Cheng等人在[2016年的一篇论文](https://homl.info/widedeep)中引入。它将输入的全部或部分直接连接到输出，如图所示。该结构使得神经网络可以同时学习深度模式（使用深路径）与简单规则（通过短路径）。反之，一个普通的MLP强制所有数据流过所有层，因此数据中的简单模式最终可能被一系列转换所扭曲。

![Wide & Deep神经网络](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\Wide & Deep神经网络.png)

下面构建这样的神经网络去处理加州房价问题：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
input_ = keras.layers.Input(shape=X_train.shape[1:])
hidden1 = keras.layers.Dense(30, activation="relu")(input_)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_, hidden2])
output = keras.layers.Dense(1)(concat)
model = keras.models.Model(inputs=[input_], outputs=[output])
```

首先创建了`Input`对象，这是模型获得的输入类型的规格说明，包括它的`shape`与`dtype`。后面可以看到，模型可以有多输入。

> This is a specification of the kind of input the model will get, including its `shape` and `dtype`.

然后，我们创建了一个含有30个神经元的`Dense`层，并使用ReLU激活函数。一旦它被创建，可以像函数一样调用它，并将输入传递给它。这也是这被称作函数式API的原因。注意我们只告诉了Keras如何将层连接起来，目前并未处理任何实际数据。

> As soon as it is created, notice that we call it like a function, passing it the input. This is why this is called the Functional API.

然后创建了第二个隐藏层，再次像一个函数一样使用它。注意我们将第一个隐藏层的输出传递给它。

> We then create a second hidden layer, and again we use it as a function.

然后，我们使用`keras.layers.concatenate`函数去创建一个`Concatenate`层并直接传递给它输入与第二个隐藏层的输出以将两者拼接。也可以先创建一个`keras.layers.Concatenate`层，然后将其当做一个函数使用（即`concat = keras.layers.Concatenate()([input_, hidden2])`）。

然后，我们创建了输出层，它只有一个神经元并且没有激活函数。我们像调用函数一样调用它，将拼接结果传递给它。

最后，我们创建了Keras `Model`，指定使用的输入与输出。

一旦构建好了Keras模型，接下来的一切与之前相同：

```python
model.summary()
```

```python
model.compile(loss="mean_squared_error", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid))
mse_test = model.evaluate(X_test, y_test)
y_pred = model.predict(X_new)
```

如果要让一组特征子集通过宽路径（wide path）、另一组子集（两组子集可能重叠）通过深路径（deep path）（如图），一种方法是使用多输入。例如，假设我们希望让5个特征子集通过宽路径（特征0\~4）、6个特征子集通过深路径（特征2\~7）：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
input_A = keras.layers.Input(shape=[5], name="wide_input")
input_B = keras.layers.Input(shape=[6], name="deep_input")
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="output")(concat)
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])
```

![处理多输入](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\处理多输入.png)

注意至少要对最重要的层进行命名，尤其当模型变得有点复杂时。这里在创建模型时指定了`inputs=[input_A, input_B]`。下面就可以像往常一样编译模型了，但是在调用`fit`方法时，需要传递一对矩阵`(X_train_A, X_train_B)`，每个输入一个。也可以传递一个将输入名字映射到输入值的字典，像`{"wide_input": X_train_A, "deep_input": X_train_B}`这样。当输入很多时，这对避免顺序错误尤其有用。对于`X_valid`、`X_test`（当调用`evaluate`方法时）与`X_new`（当调用`predict`方法时），这同样成立。

```python
model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))

X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]

history = model.fit((X_train_A, X_train_B), y_train, epochs=20,
                    validation_data=((X_valid_A, X_valid_B), y_valid))
mse_test = model.evaluate((X_test_A, X_test_B), y_test)
y_pred = model.predict((X_new_A, X_new_B))
```

很多情况下都需要多输出：

- 任务需要。例如，定位并分类图片中的主要对象。这同时是个回归任务（找到对象的坐标以及它的宽度与高度）与分类任务。
- 基于相同数据执行多个独立的任务。此时可以为每个任务训练一个神经网络，但在许多情况下，训练一个神经网络并未每个任务指定一个输出可以使得所有任务获得更好的结果。因为这个神经网络可以学习到数据中的特征，这些特征对各个任务都有用。例如，可以在人脸图片上执行**多任务分类（multitask classification）**，使用一个输出去分类人的面部表情、另个输出去识别他们是否戴了眼镜。
- 将多输出作为正则化技术（即一个训练约束，它的目的是降低过拟合风险从而提高模型泛化能力）。例如，可以在神经网络结构中添加一些辅助输出（auxiliary outputs）（如图）以确保网络的底层部分可以在不依赖于网络剩余部分的情况下学习到一些有用的东西。

![处理多输出。在该例中，添加一个辅助输出，用于正则化](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\处理多输出.png)

添加额外输出的方式很简单：将它们连接到合适的（appropriate）层，并将它们添加到模型的输出列表。下面构建上图所示的网络：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
input_A = keras.layers.Input(shape=[5], name="wide_input")
input_B = keras.layers.Input(shape=[6], name="deep_input")
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="main_output")(concat)
aux_output = keras.layers.Dense(1, name="aux_output")(hidden2)
model = keras.models.Model(inputs=[input_A, input_B],
                           outputs=[output, aux_output])
```

每个输出需要它自己的损失函数，因此当编译模型时，要传递损失的列表，如果传递单个损失，Keras假定同样的损失必须用于所有输出。也可以传递一个将每个输出名字映射到对应损失的字典。当输出很多时，这对避免顺序错误尤其有用。默认情况下，Keras会计算所有损失并简单地将它们相加以得到用于训练的最终损失。这里，相比于辅助输出，我们更关心主输出（因为辅助输出只用于正则化），所以我们希望给主输出的损失大得多的权重，此时可以在编译模型时设置所有的损失权重。损失权重与指标同样可以使用字典设置。

```python
model.compile(loss=["mse", "mse"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))
```

```python
history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,
                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))
```

当评估模型时，Keras会返回总损失，以及所有的单独损失：

```python
total_loss, main_loss, aux_loss = model.evaluate(
    [X_test_A, X_test_B], [y_test, y_test])
```

同样地，`predict`方法会为每个输出返回预测：

```python
y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])
```

### 子类化API

顺序API与函数式API都是声明式的（declarative）：首先声明需要使用哪些层以及它们是如何连接的，然后才可以为模型提供数据用于训练或推断（inference）。这有很多有点：模型可以很容易地被保存、复制与分享，模型结构可以被展示与分析，框架（the framework）可以推断形状、检查类型，所以错误可以在任何数据通过模型前被捕获。它也很容易调试，因为整个模型是层的静态图（a static graph of layers）。但是它的缺点在于：它是静态的。一些模型涉及循环（loops）、可变形状（varying shapes）、条件分支与其他动态行为。在这些情况下，或更喜欢命令式编程风格，可以使用子类化API（Subclassing API）：继承（subclass）`Model`类，在构造器中创建需要的层（layers），在`call`方法中执行想要的计算。

下面定义一个`WideAndDeepModel`类，创建它的一个实例，这提供了一个模型，它与之前使用函数式API构建的模型等价。然后可以编译、评估它，并使用它作出预测，就像之前所做的那样。

```python
class WideAndDeepModel(keras.models.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = keras.layers.Dense(units, activation=activation)
        self.hidden2 = keras.layers.Dense(units, activation=activation)
        self.main_output = keras.layers.Dense(1)  # Keras模型有一个output属性，因此不能将该名字用于主输出层。这是我们将其重命名为main_output的原因。
        self.aux_output = keras.layers.Dense(1)
        
    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = keras.layers.concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output

model = WideAndDeepModel(30, activation="relu")
```

这个例子看起来很像函数式API，只是不需要创建输入（inputs），而是只需要使用`call`方法的`input`实参，并且将构造器中层的创建与`call`方法中对层的使用分开。这样可以在`call`方法中做几乎任意想做的事：`for`循环、`if`语句、低层（low-level）TensorFlow操作。这使得它成为研究人员尝试新想法的一个很好的API。

> The big difference is that you can do pretty much anything you want in the `call()` method:

> This makes it a great API for researchers experimenting with new ideas.

```python
model.compile(loss="mse", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))
history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,
                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))
total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))
y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))
```

这种额外的灵活性是有代价的：模型的结构隐藏在`call`方法中，因此Keras不能轻松探查它。Keras不能保存或复制它；当调用`summary`方法时，只能获得层的列表，没有任何关于它们如何相互连接的信息；Keras不能提前检查类型与形状，因此很容易犯错。因此除非真的需要额外的灵活性，也许应该坚持使用顺序API或函数式API。

## 保存与恢复模型

当使用顺序API或函数式API时，保存一个经过训练的Keras模型很简单：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=[8]),
    keras.layers.Dense(30, activation="relu"),
    keras.layers.Dense(1)
])    
```

```python
model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
mse_test = model.evaluate(X_test, y_test)
```

```python
model.save("my_keras_model.h5")
```

Keras使用HDF5格式保存模型的结构（包括每层的超参数）与每个层的所有模型参数的值（例如连接权重与偏置），它还保存优化器（包括它的超参数与它可能拥有的所有状态）。

通常你会有一个脚本训练模型并保存它，以及一个或多个脚本（或web服务）加载模型并用它来作出预测。加载模型也很简单：

```python
model = keras.models.load_model("my_keras_model.h5")
```

```python
model.predict(X_new)
```

如果使用子类化API，则以上方法不奏效。此时至少可以通过`save_weights`与`load_weights`方法保存与恢复模型参数，但是需要手动保存与恢复所有其他内容。

```python
model.save_weights("my_keras_weights.ckpt")
```

```python
model.load_weights("my_keras_weights.ckpt")
```

## 回调

如果模型训练时间过长，则不仅要在训练末尾保存模型，同时要在训练期间定期保存检查点（checkpoints），以防止当计算机崩溃时丢弃一切。此时需要告诉`fit`方法去保存检查点，可以使用回调（callbacks）做到这一点。

`fit`方法接受一个`callbacks`实参，它允许你指定对象列表，Keras会在训练开始与结束、每代的开始与结束甚至是处理每个批的前后调用这些对象。例如，`ModelCheckpoint`回调会在训练过程中定期模型的检查点（默认在每代的末尾）：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=[8]),
    keras.layers.Dense(30, activation="relu"),
    keras.layers.Dense(1)
])    
```

```python
model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5", save_best_only=True)
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb])
model = keras.models.load_model("my_keras_model.h5") # rollback to best model
mse_test = model.evaluate(X_test, y_test)
```

上面在创建`ModelCheckpoint`时设置了`save_best_only=True`。在这种情况下，只有当模型在验证集上的性能到目前为止最好，它才被保存。这样，就不用担心训练时间过长与过拟合训练集：只要在训练后恢复最后一个被保存的模型，这就是在验证集上最佳模型。以上代码可以看做实现早停法的简单方式。

另一种实现早停法的方式是使用`EarlyStopping`回调，当模型在验证集上的误差在给定数量的代（由`patience`实参定义）后没有减少，则它中断训练过程，并可可选地回滚到最佳模型。可以结合这两个回调去保存模型的检查点（以防计算机崩溃），并在没有更多进展时提早中断训练（以防止浪费时间与资源）：

> It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model.

```python
model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                                  restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=100,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb, early_stopping_cb])
mse_test = model.evaluate(X_test, y_test)
```

代的数量可以设置为一个很大的值，因为当没有更多进展时，训练自动停止。这样就没有必要恢复保存的最佳模型，因为`EarlyStopping`回调会记录最佳权重并在训练结束时恢复它们。

更多的回调函数见[`keras.callbacks` 包](https://keras.io/callbacks/)。

如果想要额外的控制，可以自定义回调。以下自定义的回调会展示训练过程中验证误差与训练误差的比值（例如用于检测过拟合）：

```python
class PrintValTrainRatioCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        print("\nval/train: {:.2f}".format(logs["val_loss"] / logs["loss"]))
```

```python
val_train_ratio_cb = PrintValTrainRatioCallback()
history = model.fit(X_train, y_train, epochs=1,
                    validation_data=(X_valid, y_valid),
                    callbacks=[val_train_ratio_cb])
```

同样，可以实现`on_train_begin`、`on_train_end`、`on_epoch_begin`、`on_epoch_end`、`on_batch_begin`与` on_batch_end`方法。回调也能在评估与预测过程中被使用（例如，用于调试）。对于评估，需要实现`on_test_begin`、`on_test_end`、`on_test_batch_begin`或`on_test_batch_end`方法（通过`evaluate`方法调用）；对于预测，需要实现`on_predict_begin`、`on_predict_end`、`on_predict_batch_begin`或`on_predict_batch_end`方法（通过`predict`方法调用）。

## TensorBoard

TensorBoard是交互式可视化工具，它可以用来查看训练过程中的学习曲线、比较多次运行间的学习曲线、可视化计算图、分析训练统计数据（training statistics）、查看模型生成的图像、可视化投影到3维的复杂多维数据并自动分簇，等等。当安装TensorFlow时，TensorBoard被自动安装。

要使用它，必须修改程序，使得它将需要可视化的数据输出到特殊的二进制日志文件中，该二进制文件被称为**事件文件（event files）**。每个二进制数据记录被称为**摘要（summary）**。TensorBoard服务器会监视日志目录，并自动发现更改并更新可视化，这让可视化实时数据，例如训练过程中的学习曲线成为可能。总之，将TensorBoard服务器指向根日志目录，并配置程序使得它在每次运行时将不同的子目录。这样，同样的TensorBoard服务器实例允许可视化并比较程序多次运行的数据而不会将它们弄混。

下面定义用于TensorBoard日志的根日志目录，以及一个基于当前日期与事件生成子目录路径的函数，以使得每次运行子目录路径都不同：

```python
import os

root_logdir = os.path.join(os.curdir, "my_logs")
```

```python
def get_run_logdir():
    import time
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir()
run_logdir
```

也可以在日志目录名中包含额外信息，如正在测试的超参数值，便更容易了解在TensorBoard中看到的内容。

TensorBoard提供了`TensorBoard`回调：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=[8]),
    keras.layers.Dense(30, activation="relu"),
    keras.layers.Dense(1)
])    
model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
```

```python
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
history = model.fit(X_train, y_train, epochs=30,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb, tensorboard_cb])
```

如果运行以上代码，`TensorBoard`回调会创建日志目录（如果需要，会创建它的父目录（parent directories），在训练过程中它会创建事件文件并将摘要写到上面。当第二次运行程序后（也许改变了一些超参数的值），将得到类似于如下的目录结构：

```
my_logs/
├── run_2019_06_07-15_15_22
│   ├── train
│   │   ├── events.out.tfevents.1559891732.mycomputer.local.38511.694049.v2
│   │   ├── events.out.tfevents.1559891732.mycomputer.local.profile-empty
│   │   └── plugins/profile/2019-06-07_15-15-32
│   │       └── local.trace
│   └── validation
│       └── events.out.tfevents.1559891733.mycomputer.local.38511.696430.v2
└── run_2019_06_07-15_15_49
    └── [...]
```

每次运行一个目录，每个目录包含一个存放训练日志的子目录、一个存放验证日志的子目录。两个目录都包含事件文件，但是训练日志还包含剖析轨迹（profiling traces）：这允许TensorBoard准确地展示在所有设备上，模型在模型的每个部分上所花费的时间，对于定位性能瓶颈十分有用。

接下来启动TensorBoard服务器。下面在终端中运行命令启动TensorBoard：

```
$ tensorboard --logdir=./my_logs --port=6006
```

也可以通过运行以下命令，直接在Jupyter中使用TensorBoard：

```
%load_ext tensorboard
%tensorboard --logdir=./my_logs --port=6006
```

TensorFlow在`tf.summary`包中提供了低层API。以下代码使用`create_file_writer`函数创建了一个`SummaryWriter`，它使用这个writer作为日志标量、直方图、图像、音频与文本的上下文，所有这些可以通过TensorBoard可视化：

```python
test_logdir = get_run_logdir()
writer = tf.summary.create_file_writer(test_logdir)
with writer.as_default():
    for step in range(1, 1000 + 1):
        tf.summary.scalar("my_scalar", np.sin(step / 10), step=step)
        data = (np.random.randn(100) + 2) * step / 100 # some random data
        tf.summary.histogram("my_hist", data, buckets=50, step=step)
        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images
        tf.summary.image("my_images", images * step / 1000, step=step)
        texts = ["The step is " + str(step), "Its square is " + str(step**2)]
        tf.summary.text("my_text", texts, step=step)
        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)
        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])
        tf.summary.audio("my_audio", audio, sample_rate=48000, step=step)
```

## 微调超参数

为了能够使用`GridSearchCV`或`RandomizedSearchCV`去探索超参数空间，需要将Keras模型包装成一个模仿普通Scikit-Learn回归器的对象。第一步是创建一个函数，给定一组超参数，它构建并编译一个Keras模型：

> To do this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors.

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape=input_shape))
    for layer in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation="relu"))
    model.add(keras.layers.Dense(1))
    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)
    model.compile(loss="mse", optimizer=optimizer)
    return model
```

该函数创建了一个简单的`Sequential`模型用于单变量回归，模型具有给定的形状、给定的隐藏层与神经元数量，函数使用SGD优化器编译模型，并配置了指定的学习率。就像Scikit-Learn所做的那样，给尽可能多的超参数提供合理的默认值（defaults）是好的做法。

下面基于`build_model`函数创建一个`KerasRegressor`：

```python
keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)
```

`KerasRegressor`对象是使用`build_model`函数构建的Keras模型的一个瘦包装器。因为当创建它的时候没有指定任何超参数，所有它使用`build_model`函数中定义的默认超参数。现在可以像使用一个普通的Scikit-Learn回归器一样使用该对象了：使用它的`fit`方法训练它，然后使用它的`score`方法评估它，并使用它的`predict`方法作出预测：

```python
keras_reg.fit(X_train, y_train, epochs=100,
              validation_data=(X_valid, y_valid),
              callbacks=[keras.callbacks.EarlyStopping(patience=10)])
```

```python
mse_test = keras_reg.score(X_test, y_test)
```

```python
y_pred = keras_reg.predict(X_new)
```

任何传递给`fit`方法的额外参数都会传递给底层（underlying）Keras模型。因为Scikit-Learn期望得分而不是损失（即越高越好），所以得分是MSE的取反。

> Also note that the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e., higher should be better).

我们不希望像这样训练与评估单个模型，而是训练多个变体并看哪个在验证集上的性能最佳。因为超参数多，所以最好使用随机搜索而不是网格搜索。下面探索隐藏层数量、神经元数量与学习率：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
from scipy.stats import reciprocal
from sklearn.model_selection import RandomizedSearchCV

param_distribs = {
    "n_hidden": [0, 1, 2, 3],
    "n_neurons": np.arange(1, 100)               .tolist(),
    "learning_rate": reciprocal(3e-4, 3e-2)      .rvs(1000).tolist(),
}

rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)
rnd_search_cv.fit(X_train, y_train, epochs=100,
                  validation_data=(X_valid, y_valid),
                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])
```

这与在[机器学习工程清单](#(机器学习工程清单))中所做的一致，只是这里还传递给`fit`方法额外的超参数，它们被转发给底层Keras模型。注意`RandomizedSearchCV`使用K折交叉验证，因此这里不需要使用`X_valid`与`y_valid`，它们只用于早停法。

探索时间取决于硬件、数据集大小、模型复杂度以及`n_iter`与`cv`的值。当探索结束，可以获得最优参数、最优得分以及被训练的Keras模型，并保存、在测试集上评估模型，甚至将其部署到生产环境中（如果对它的性能满意的话）：

```python
rnd_search_cv.best_params_
```

```python
rnd_search_cv.best_score_
```

```python
rnd_search_cv.best_estimator_
```

```python
rnd_search_cv.score(X_test, y_test)
```

```python
model = rnd_search_cv.best_estimator_.model
model
```

```python
model.evaluate(X_test, y_test)
```

# 自定义模型与训练

## TensorFlow数据结构

TensorFlow的API以**张量（tensors）**为核心，张量从一个运算流向另一个运算——所以有了名字Tensor*Flow*。张量与NumPy的`ndarray`非常相似：它通常是一个多维数组，但是也可以保存一个标量。当创建自定义损失函数、自定义指标、自定义层等等时，这些张量非常重要。

### 张量与运算

可以使用`tf.constant`方法创建一个张量。例如，下面是一个张量，它表示一个包含2行3列浮点数的矩阵：

```python
import tensorflow as tf

tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix
```

```python
tf.constant(42) # scalar
```

就像一个`ndarray`一样，一个`tf.Tensor`有一个形状与一个数据类型（`dtype`）：

```python
t = tf.constant([[1., 2., 3.], [4., 5., 6.]])
t
```

```python
t.shape
```

```python
t.dtype
```

索引的工作方式与Numpy非常相似：

```python
t[:, 1:]
```

```python
t[..., 1, tf.newaxis]
```

最重要的是，可以使用各种张量运算：

```python
t + 10
```

```python
tf.square(t)
```

```python
t @ tf.transpose(t)
```

注意`t + 10`等价于调用`tf.add(t, 10)`（实际上，Python调用方法`t.__add__(10)`，后者调用`tf.add(t, 10)`）。其他运算，例如`-`与`*`同样被支持。`@`运算在Python 3.5中被添加，它用于矩阵乘法：它等价于调用`tf.matmul`函数。

所有所需的基本数学运算（`tf.add`、`tf.multiply`、`tf.square`、`tf.exp`、`tf.sqrt`等等）以及大多数Numpy中的运算（例如，`tf.reshape`、`tf.squeeze`、`tf.tile`）都可以找到。一些函数与Numpy中的名字不同，例如：`tf.reduce_mean`、`tf.reduce_sum`、`tf.reduce_max`与`tf.math.log`等价于`np.mean`、`np.sum`、`np.max`与`np.log`。名称不同是有好的理由的。例如，在TensorFlow中，必须书写`tf.transpose(t)`，不能像NumPy中那样只写`t.T`。因为`tf.transpose`函数与NumPy中的`T`属性所做的事不完全相同：在TensorFlow中，一个新的张量被创建，它是转置后的数据的拷贝，但是在NumPy中，`t.T`只是相同数据的转置视图。类似地，`tf.reduce_sum`运算之所以如此命名是因为它的GPU核（即GPU实现）使用一种reduce算法，它不保证元素相加的顺序：因为32位浮点数精度有限，所以每次调用该运算时，结果可能发生微小变化。对于`tf.reduce_mean`也是这样（当然，`tf.reduce_max`是确定的）。

大部分函数与类都有别名。例如，`tf.add`与`tf.math.add`是相同函数。这允许TensorFlow为最常见的运算提供简洁的名称，同时保持包组织良好。一个值得注意的例外是`tf.math.log`，它很常用，但是没有`tf.log`别名（因为可能与logging混淆）。

Keras API有它自己的、位于`keras.backend`的低层API。它包含诸如`square`、`exp`与`sqrt`的函数。在tf.keras中，这些函数通常只是调用对应的TensorFlow运算。如果想要编写可移植到其他Keras实现中的代码，则应该使用这些Keras函数。然而，它们只覆盖了TensorFlow中所有可用函数的子集。以下是`keras.backend`的简单使用示例，它通常简称为`K`：

> so in this book we will use the TensorFlow operations directly.

```python
from tensorflow import keras

K = keras.backend
K.square(K.transpose(t)) + 10
```

### 张量与NumPy

张量与NumPy配合得很好：可以创建从NumPy数组创建一个张量，反之亦然。甚至可以将TensorFlow运算用于NumPy数组，将NumPy运算用于张量：

```python
import numpy as np

a = np.array([2., 4., 5.])
tf.constant(a)
```

```python
t.numpy()
```

```python
np.array(t)
```

```python
tf.square(a)
```

```python
np.square(t)
```

注意，NumPy默认使用64位浮点数，而TensorFlow使用32位。因为对于神经网络来说，32位精度通常已经足够了，并且它运行更快速、使用内存更少。因此当从NumPy数组创建一个张量时，确保设置`dtype=tf.float32`。

### 类型转换

类型转换会显著影响性能，当且当它们自动完成时很容易被忽略。为了避免这点，TensorFlow不自动执行任何类型转换：如果试图对类型不兼容的张量执行运算，则它抛出异常。例如，不能将一个浮点张量与一个整型张量相加，甚至不能将32位浮点数与64位浮点数相加：

```python
try:
    tf.constant(2.0) + tf.constant(40)
except tf.errors.InvalidArgumentError as ex:
    print(ex)
```

```python
try:
    tf.constant(2.0) + tf.constant(40., dtype=tf.float64)
except tf.errors.InvalidArgumentError as ex:
    print(ex)
```

如果确实需要转换类型，可以使用`tf.cast`方法：

> This may be a bit annoying at first, but remember that it’s for a good cause!

```python
t2 = tf.constant(40., dtype=tf.float64)
tf.constant(2.0) + tf.cast(t2, tf.float32)
```

### 字符串

张量可以保存字节字符串（byte strings），后者对自然语言处理尤其有用。

```python
tf.constant(b"hello world")
```

如果尝试使用一个Unicode字符串构建一个张量，则TensorFlow自动将其编码为UTF-8：

```python
tf.constant("café")
```

也可以创建代表Unicode字符串的张量：只需要创建一个32位整型数组，每个整型表示一个Unicode码点（Unicode code point）（参见[https://homl.info/unicode](https://homl.info/unicode)）。

```python
u = tf.constant([ord(c) for c in "café"])
u
```

在类型为tf.string的张量中，字符串长度不是张量形状的一部分。换言之，字符串被当作一个原子值。然而，在一个Unicode字符串张量（即一个int32张量）中，字符串长度是张量形状的一部分。

`tf.strings`包包含若干操纵字符串张量的函数，例如`length`函数统计一个字节字符串的字符数量（或码点数量，如果设置`unit="UTF8_CHAR"`的话），`unicode_encode`函数将一个Unicode字符串张量转换为一个字节字符串张量，以及`unicode_decode`函数执行相反操作：

```python
b = tf.strings.unicode_encode(u, "UTF-8")
tf.strings.length(b, unit="UTF8_CHAR")
```

```python
tf.strings.unicode_decode(b, "UTF-8")
```

还可以操纵包含多个字符串的张量：

```python
p = tf.constant(["Café", "Coffee", "caffè", "咖啡"])
```

```python
tf.strings.length(p, unit="UTF8_CHAR")
```

```python
r = tf.strings.unicode_decode(p, "UTF8")
r
```

```python
print(r)
```

注意解码后的字符串保存在`RaggedTensor`中。

### 不规则张量

**不规则张量（ragged tensor）**是一种特殊张量，它表示不同大小的数组组成的列表。更一般地，它是一个张量，有一个或多个**不规则维度（ragged dimensions）**，即切片长度可能不同的维度。在不规则张量`r`中，第二个维度是一个不规则维度。在所有不规则张量中，第一个维度总是一个规则维度（也被称为**uniform dimension**）。

不规则张量`r`中的所有元素都是规则张量。例如，查看该不规则张量的第二个元素：

```python
print(r[1])
```

```python
print(r[1:3])
```

`tf.ragged`包包含若干创建与操纵不规则张量的函数。这里使用`tf.ragged.constant`函数创建第二个不规则张量，并沿着第0轴将其与第一个不规则张量拼接：

```python
r2 = tf.ragged.constant([[65, 66], [], [67]])
print(tf.concat([r, r2], axis=0))
```

`r2`中的张量沿着第0轴附加到`r`中的张量后。下面沿着第1轴将`r`与另一个不规则张量拼接：

```python
r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])
print(tf.concat([r, r3], axis=1))
```

```python
tf.strings.unicode_encode(r3, "UTF-8")
```

`r`中第$i$个张量与`r3`中第$i$个张量被拼接。

如果调用`to_tensor`方法，它会被转换为一个规则张量，较短的张量以0填充以得到长度相等的张量（可以通过设置`default_value`实参来改变该默认值）：

```python
r.to_tensor()
```

许多TF运算都支持不规则张量。完整列表见`tf.RaggedTensor`类的文档。

### 稀疏张量

TensorFlow可以高效地表示**稀疏张量（sparse tensors）**（即主要包含0的张量）。只需要创建`tf.SparseTensor`，指定非零元素的索引与值以及张量形状。索引必须按照“阅读顺序”列出（从左到右，从上到下）。如果对此不确定，则使用`tf.sparse.reorder`函数。可以使用`tf.sparse.to_dense`函数将一个稀疏张量转换为一个密集张量（dense tensor）（即常规的张量）：

```python
s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],
                    values=[1., 2., 3.],
                    dense_shape=[3, 4])
```

```python
print(s)
```

```python
tf.sparse.to_dense(s)
```

注意稀疏张量支持的运算没有密集张量多。例如，可以将一个稀疏张量乘以任何标量值，并得到一个新的稀疏张量，但是不能将一个标量值加到一个稀疏张量上，这不会返回一个稀疏张量：

```python
s2 = s * 2.0
```

```python
try:
    s3 = s + 1.
except TypeError as ex:
    print(ex)
```

```python
s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])
tf.sparse.sparse_dense_matmul(s, s4)
```

```python
s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],
                     values=[1., 2.],
                     dense_shape=[3, 4])
print(s5)
```

```python
try:
    tf.sparse.to_dense(s5)
except tf.errors.InvalidArgumentError as ex:
    print(ex)
```

```python
s6 = tf.sparse.reorder(s5)
tf.sparse.to_dense(s6)
```

### 集合

TensorFlow支持整型与字符串的集合（但是不支持浮点型的集合）。它使用规则张量表示它们。例如，集合`{1, 5, 9}`表示为张量`[[1, 5, 9]]`。注意张量必须至少有两个维度，集合必须在最后一个维度。例如`[[1, 5, 9], [2, 5, 11]]`是包含两个独立集合`{1, 5, 9}`与`{2, 5, 11}`的张量。如果某些集合比其他集合短，则必须使用填充值填充它们（默认为0，但是可以使用其他任何值）。

`tf.set`包包含若干操纵集合的函数。例如，下面创建两个集合并计算它们的并（结果是一个稀疏张量，因此这里调用`to_dense`函数去展示它）：

```python
a = tf.constant([[1, 5, 9]])
b = tf.constant([[5, 6, 9, 11]])
u = tf.sets.union(a, b)
u
```

```python
tf.sparse.to_dense(u)
```

还可以同时计算的多对集合的并：

```python
a = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]])
b = tf.constant([[4, 5, 6], [9, 10, 0]])
u = tf.sets.union(set1, set2)
tf.sparse.to_dense(u)
```

如果想要使用其他填充值，则在调用`to_dense`函数时必须设置`default_value`：

```python
tf.sparse.to_dense(u, default_value=-1)
```

`default_value`默认为0，因此当处理字符串集合时，必须设置`default_value`（例如，设置为一个空字符串）。

`tf.sets`中的其他函数包含`difference`、`intersection`与`size`。如果想要检查集合是否包含一些给定值，则可以计算集合与该值的；如果想要将一些值加到集合中，则可以计算集合与该值的并。

```python
tf.sparse.to_dense(tf.sets.difference(set1, set2))
```

```python
tf.sparse.to_dense(tf.sets.intersection(set1, set2))
```

### 张量数组

` tf.TensorArray`表示张量组成的列表，这在包含循环的动态模型中可以非常方便地累计结果然后计算一些统计信息。可以读或些数组中任意位置的张量：

```python
array = tf.TensorArray(dtype=tf.float32, size=3)
array = array.write(0, tf.constant([1., 2.]))
array = array.write(1, tf.constant([3., 10.]))
array = array.write(2, tf.constant([5., 7.]))
```

```python
array.read(1)
```

注意读取一项会将其从数组中删除，并被一个相同形状的、全0的张量所替代。

> Notice that reading an item pops it from the array, replacing it with a tensor of the same shape, full of zeros.

当写数组时，必须将输出重新赋值给数组，如上所示。否则，代码在动态图模式下可以很好地工作，但是在图模式下会崩溃。

> When you write to the array, you must assign the output back to the array,

当创建一个`TensorArray`时，必须提供它的`size`，除非是在图模式下。或者，可以不设置`size`，而是设置`dynamic_size=True`，但是这会妨碍性能，因此如果提前知道`size`，则应该设置它。同时还必须指定`dtype`，并且所有元素的形状必须与第一个写入数组的元素的形状相同。

可以调用`stack`方法将所有项堆叠进规则张量中：

```python
array.stack()
```

```python
mean, variance = tf.nn.moments(array.stack(), axes=0)
mean
```

```python
variance
```

### 变量

目前看到的`tf.Tensor`值不可变。这意味着不能将常规的张量作为神经网络中的权重，因为它们不能被反向传播微调。另外，其他参数也可能需要随时间改变（例如，动量优化记录过去的梯度）。因此需要`tf.Variable`：

```python
v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])
```

`tf.Variable`表现与`tf.Tensor`非常相似：可以对其执行相同运算，它也与NumPy配合得很好。但是它可以通过`assign`、`assign_add`或`assign_sub`方法被原地修改。还可以使用单元（或切片）的`assign`方法（不能直接对项赋值）或`scatter_update`、`scatter_nd_updat`方法修改单个单元或切片：

```python
v.assign(2 * v)
```

```python
v[0, 1].assign(42)
```

```python
v[:, 2].assign([0., 1.])
```

```python
try:
    v[1] = [7., 8., 9.]
except TypeError as ex:
    print(ex)
```

```python
v.scatter_nd_update(indices=[[0, 0], [1, 2]],
                    updates=[100., 200.])
```

```python
sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],
                                indices=[1, 0])
v.scatter_update(sparse_delta)
```

在实践中很少需要手动创建变量，因为Keras提供了`add_weight`方法做到这点。另外，模型参数通常直接由优化器更新，因此很少需要手动更新变量。

### 自定义模型与训练算法

#### 自定义损失函数

假设想要训练一个线性模型，但是训练集有一点噪音。当然，首先要尝试通过移除或修正离群值来清洗数据集，但是这还不够，数据集仍然有噪音。此时，如果使用均方误差作为损失函数，则它可能过分惩罚大误差，导致模型不精确；而平均误差则不会对离群值造成如此大的惩罚，但是训练的收敛速度较慢，并且训练好的模型不会很精确。这是可以使用Huber损失。Huber损失目前不是官方Keras API的一部分，但是它可见于tf.keras中（只需要使用`keras.losses.Huber`损失类的一个实例）。这里创建一个函数，它接受标签与预测作为实参，并使用TensorFlow运算去计算每个实例的损失：

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target.reshape(-1, 1), random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.transform(X_test)
```

```python
def huber_fn(y_true, y_pred):
    error = y_true - y_pred
    is_small_error = tf.abs(error) < 1
    squared_loss = tf.square(error) / 2
    linear_loss  = tf.abs(error) - 0.5
    return tf.where(is_small_error, squared_loss, linear_loss)
```

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 3.5))
z = np.linspace(-4, 4, 200)
plt.plot(z, huber_fn(0, z), "b-", linewidth=2, label="huber($z$)")
plt.plot(z, z**2 / 2, "b:", linewidth=1, label=r"$\frac{1}{2}z^2$")
plt.plot([-1, -1], [0, huber_fn(0., -1.)], "r--")
plt.plot([1, 1], [0, huber_fn(0., 1.)], "r--")
plt.gca().axhline(y=0, color='k')
plt.gca().axvline(x=0, color='k')
plt.axis([-4, 4, 0, 4])
plt.grid(True)
plt.xlabel("$z$")
plt.legend(fontsize=14)
plt.title("Huber loss", fontsize=14)
plt.show()
```

为了优化性能，应该使用向量化的实现（vectorized implementation）。另外，如果需要从TensorFLow的图特征中受益，应该只使用TensorFlow运算。

返回一个每个实例包含一个损失的张量，而不是平均误差要更好。这样，Keras就可以在需要时应用类别权重或样本权重。

现在可以在编译Keras模型时使用该损失，然后训练该模型：

```python
input_shape = X_train.shape[1:]

model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1),
])
```

```python
model.compile(loss=huber_fn, optimizer="nadam", metrics=["mae"])
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

对训练过程中的每批，Keras将调用`huber_fn`函数去计算损失，并使用它来执行梯度下降步。另外，它会记录自代开始的总损失，并展示该平均损失。

### 保存与加载包含自定义组件的模型

当保存包含自定义损失函数的模型时，Keras会保存该函数的名字。当加载模型时，需要提供一个将函数名映射到实际函数的字典。更一般地，当加载一个包含自定义对象的模型时，需要将名字映射到对象。

```python
model.save("my_model_with_a_custom_loss.h5")
```

```python
model = keras.models.load_model("my_model_with_a_custom_loss.h5",
                                custom_objects={"huber_fn": huber_fn})
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

在当前实现中，任何介于-1\~1之间的误差被认为是“小的”。如果需要一个不同的阈值，一种解决方法是创建一个函数，它创建一个已配置的（configured）损失函数：

```python
def create_huber(threshold=1.0):
    def huber_fn(y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < threshold
        squared_loss = tf.square(error) / 2
        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    return huber_fn
```

```python
model.compile(loss=create_huber(2.0), optimizer="nadam", metrics=["mae"])
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

不幸的是，当保存该模型时，`threshold`不会被保存。这意味着加载模型时，必须要指定`threshold`值（注意要使用的名字是`huber_fn`，它是给Keras的函数名，而不是创建该函数的名字）：

```python
model.save("my_model_with_a_custom_loss_threshold_2.h5")
```

```python
model = keras.models.load_model("my_model_with_a_custom_loss_threshold_2.h5",
                                custom_objects={"huber_fn": create_huber(2.0)})
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

可以通过创建`keras.losses.Loss`类的子类，然后实现它的`get_config`方法来解决这个问题：

```python
class HuberLoss(keras.losses.Loss):
    def __init__(self, threshold=1.0, **kwargs):
        self.threshold = threshold
        super().__init__(**kwargs)
    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < self.threshold
        squared_loss = tf.square(error) / 2
        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
```

其中：

- 构造器接受`**kwargs`并将它们传递给父构造器，后者处理标准超参数：损失的`name`，用于聚合（aggregate）单个实例损失的`reduction`算法。默认情况下，它是`sum_over_batch_size`，这意味着损失将是每个实例损失之和，由样本权重加权（如果有的话），并除以批大小（不是除以权重大小，因此这不是加权平均：使用加权平均不是一个好主意，如果这么做，则取决于每个批的总权重，权重相同但在不同批中的两个实例将对训练产生不同影响）。其中可能的值是`sum`与`none`。
- `call`方法接受标签与预测，计算所有实例损失，并返回它们。
- `get_config`方法返回一个将每个超参数名映射到它的值的字典。它首先调用父类的`get_config`，然后将新的超参数加到该字典）。

> the reduction algorithm to use to aggregate the individual instance losses. 

> note that the convenient {***x*} syntax was added in Python 3.5

现在当编译模型时，可以使用该类的任何实例：

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1),
])
```

```python
model.compile(loss=HuberLoss(2.), optimizer="nadam", metrics=["mae"])
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

当保存该模型时，阈值也随之被保存；当加载该模型时，需要将类名映射到类本身：

```python
model.save("my_model_with_a_custom_loss_class.h5")
```

```python
model = keras.models.load_model("my_model_with_a_custom_loss_class.h5",
                                custom_objects={"HuberLoss": HuberLoss})
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

```python
model.loss.threshold
```

当保存一个模型时，Keras调用损失实例的`get_config`方法并将该配置（config）保存为HDF5文件中的JSON。当加载该模型时，它调用`HuberLoss`类中的`from_config`类方法：该方法由基类（`Loss`）实现，并创建该类的一个实例，将`**config`传递给构造器。

>  this method is implemented by
>
### 自定义激活函数

大部分Keras功能，例如损失、正则化器、约束、初始化器、指标、激活函数、层甚至整个模型，都可以以非常相似的方式进行自定义。大多数时候，只需要编写一个简单的函数，它接受合适的（appropriate）输入与输出。以下是一个自定义激活函数（等价于`keras.activations.softplus`或`tf.nn.softplus`函数）、一个自定义Glorot初始化器（等价于`keras.initializers.glorot_normal`函数）、一个自定义$\ell_1$正则化器（等价于` keras.regularizers.l1(0.01)`）以及一个保证权重全部非负的自定义约束（等价于`keras.constraints.nonneg`或`tf.nn.relu`）的例子：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
def my_softplus(z): # return value is just tf.nn.softplus(z)
    return tf.math.log(tf.exp(z) + 1.0)

def my_glorot_initializer(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / (shape[0] + shape[1]))
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)

def my_l1_regularizer(weights):
    return tf.reduce_sum(tf.abs(0.01 * weights))

def my_positive_weights(weights): # return value is just tf.nn.relu(weights)
    return tf.where(weights < 0., tf.zeros_like(weights), weights)
```

可以看到，实参取决于自定义函数的类型。这些自定义函数可被正常使用，例如：

```python
layer = keras.layers.Dense(1, activation=my_softplus,
                           kernel_initializer=my_glorot_initializer,
                           kernel_regularizer=my_l1_regularizer,
                           kernel_constraint=my_positive_weights)
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1, activation=my_softplus,
                       kernel_regularizer=my_l1_regularizer,
                       kernel_constraint=my_positive_weights,
                       kernel_initializer=my_glorot_initializer),
])
```

```python
model.compile(loss="mse", optimizer="nadam", metrics=["mae"])
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

```python
model.save("my_model_with_many_custom_parts.h5")
```

```python
model = keras.models.load_model(
    "my_model_with_many_custom_parts.h5",
    custom_objects={
       "my_l1_regularizer": my_l1_regularizer,
       "my_positive_weights": my_positive_weights,
       "my_glorot_initializer": my_glorot_initializer,
       "my_softplus": my_softplus,
    })
```

激活函数被应用到`Dense`层的输出，它的结果被传递给下一层。层的权重使用该初始化器返回的值初始化。在每个训练部，权重被传递给正则化函数，以计算正则化损失，它被加到主损失上以得到用于训练的最终损失。最后，约束函数在每个训练步后被调用，并且层的权重被该约束权重替换。

如果函数具有需要与模型一起保存的超参数，则可以子类化相应的类，例如`keras.regularizers.Regularizer`、`keras.constraints.Constraint`、`keras.initializers.Initializer`或`keras.layers.Layer`（对于每层，包含激活函数）。与自定义损失类似，以下是一个用于$\ell_1$约束的简单的类，它保存约束的`factor`超参数（这里不需要调用父构造器或`get_config`方法，因为它们没有由父类定义）：

```python
class MyL1Regularizer(keras.regularizers.Regularizer):
    def __init__(self, factor):
        self.factor = factor
    def __call__(self, weights):
        return tf.reduce_sum(tf.abs(self.factor * weights))
    def get_config(self):
        return {"factor": self.factor}
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1, activation=my_softplus,
                       kernel_regularizer=MyL1Regularizer(0.01),
                       kernel_constraint=my_positive_weights,
                       kernel_initializer=my_glorot_initializer),
])
```

```python
model.compile(loss="mse", optimizer="nadam", metrics=["mae"])
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

```python
model.save("my_model_with_many_custom_parts.h5")
```

```python
model = keras.models.load_model(
    "my_model_with_many_custom_parts.h5",
    custom_objects={
       "MyL1Regularizer": MyL1Regularizer,
       "my_positive_weights": my_positive_weights,
       "my_glorot_initializer": my_glorot_initializer,
       "my_softplus": my_softplus,
    })
```

注意，必须为损失、层（包括激活函数）与模型实现`call`方法，或为正则化器、初始化器与约束实现`__call__`方法。

### 自定义指标

损失与指标在概念上是不同的：损失（例如交叉熵）被梯度下降用来*训练*一个模型，因此它们必须可微（至少在求值的地方是这样），并且它们的梯度不能处处为0。另外，它们不必容易为人所理解。反之，指标（例如准确率）用于*评估*模型：它们必须更容易被解释，并且它们可以在任何地方不可微分或梯度为0。

尽管如此，在大多数情况下，定义一个自定义指标函数等同于定义一个自定义损失函数。事实上，甚至可以将之前创建的Huber损失函数作为指标（然而，Huber损失很少用作指标，MAE或MSE更好），并且可以按照相同方式持久化（在这种情况下只需要保存函数的名字`huber_fn`）：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1),
])
```

```python
model.compile(loss="mse", optimizer="nadam", metrics=[create_huber(2.0)])
```

```python
model.fit(X_train_scaled, y_train, epochs=2)
```

注意，如果损失与指标为相同函数，则结果可能不同。这通常是因为浮点数精度误差：即使数学公式等价，运算不会以相同顺序运行，导致微小差异。另外，当使用样本权重时，自代开始的损失为目前为止的所有批损失的均值，每个批损失为加权实例损失之和除以批大小；而自代开始的指标等于加权实例损失之和除以目前为止的所有权重之和，它是所有实例损失的加权平均，两者不一样。可以发现，损失 = 指标 * 样本权重的均值（加上一些浮点数精度误差）。

```python
model.compile(loss=create_huber(2.0), optimizer="nadam", metrics=[create_huber(2.0)])
```

```python
sample_weight = np.random.rand(len(y_train))
history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight)
```

```python
history.history["loss"][0], history.history["huber_fn"][0] * sample_weight.mean()
```

对于训练过程中的每批，Keras计算它的指标并记录自该代开始它的均值。大多数时候，它就是想要的。考虑一个二分类器的精确率，假设模型在第一批作了5次预测，其中4次预测正确，精确率为80%；然后模型在第二批作了3次预测，但是它们全部错误，精确率为0%。如果单纯计算这两个精确率的均值，则得到40%。但是它不是模型在两批上的精确率（总精确率为50%，而不是40%）。这里所需要的是一个对象，它可以记录真阳性与假阳性的数量，并在需要时计算它们的比率。这正是`keras.metrics.Precision`类所做的：

```python
precision = keras.metrics.Precision()
precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])
```

```python
precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])
```

该例创建了一个`Precision`对象，然后像一个函数一样使用它，传递给它第一批的标签与预测，然后是第二批的标签与预测（注意也可以传递样本权重）。这里使用了与刚才讨论的示例中相同数量的真阳性与假阳性数量。在第一批后，它返回精确率80%，在第二批后，它返回50%（它是目前为止的总精确率，而不是第二批的精确率）。这被称为**流指标（streaming metric）**或**有状态指标（stateful metric）**，因为它是一批又一批逐步更新的。

在任何时候，都可以调用`result`方法以获得指标的当前值。还可以通过使用`variables`属性查看它的变量（记录真阳性与假阳性的数量），并且可以使用`reset_states`方法重置这些变量：

```python
precision.result()
```

```python
precision.variables
```

```python
precision.reset_states()
```

要创建这样的流指标，则创建`keras.metrics.Metric`类的子类。下面是一个简单的例子，它记录目前为止的总Huber损失与实例数量。当询问结果时，它返回比率，即平均Huber损失：

```python
class HuberMetric(keras.metrics.Metric):
    def __init__(self, threshold=1.0, **kwargs):
        super().__init__(**kwargs) # handles base args (e.g., dtype)
        self.threshold = threshold
        self.huber_fn = create_huber(threshold)
        self.total = self.add_weight("total", initializer="zeros")
        self.count = self.add_weight("count", initializer="zeros")
    def update_state(self, y_true, y_pred, sample_weight=None):
        metric = self.huber_fn(y_true, y_pred)
        self.total.assign_add(tf.reduce_sum(metric))
        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))
    def result(self):
        return self.total / self.count
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
```

```python
m = HuberMetric(2.)

# total = 2 * |10 - 2| - 2²/2 = 14
# count = 1
# result = 14 / 1 = 14
m(tf.constant([[2.]]), tf.constant([[10.]])) 
```

```python
# total = total + (|1 - 0|² / 2) + (2 * |9.25 - 5| - 2² / 2) = 14 + 7 = 21
# count = count + 2 = 3
# result = total / count = 21 / 3 = 7
m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))

m.result()
```

```python
m.variables
```

```python
m.reset_states()
m.variables
```

- 构造器使用`add_weight`方法创建了在多批中跟踪指标状态所需的变量。在该例中，它是所有目前为止的Huber损失之和（`total`）以及实例数量（`count`）。如果喜欢的话，可以手动创建这些变量。Keras记录任何设置为属性的`tf.Variable`（更一般地，任何“可记录的（trackable）”对象，例如层或模型）。
- 当将该类的实例作为函数时（就像对`Precision`对象所做的那样），`update_state`方法被调用。给定一批的标签与预测（以及样本权重，这里忽略），它更新变量。
- `result`方法计算并返回最终结果，在该例中它是所有实例的平均Huber指标。当将该指标作为函数，`update_state`方法首先被调用，然后`result`方法被调用，并返回它的输出。
- 这里还实现了`get_config`方法以确保`threshold`随模型一起被保存。
- `reset_states`的默认实现重置所有变量为0.0（如果需要，可以覆盖它）。

> The constructor uses the add_weight() method to create the variables needed to keep track of the metric’s state over multiple batches

> Keras tracks any tf.Variable that is set as an attribute (and more generally, any “trackable” object, such as layers or models).

下面检验`HuberMetric`是否工作良好：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1),
])
```

```python
model.compile(loss=create_huber(2.0), optimizer="nadam", metrics=[HuberMetric(2.0)])
```

```python
model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)
```

```python
model.save("my_model_with_a_custom_metric.h5")
```

```python
model = keras.models.load_model("my_model_with_a_custom_metric.h5",
                                custom_objects={"huber_fn": create_huber(2.0),
                                                "HuberMetric": HuberMetric})
```

```python
model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)
```

注意，在TF 2.2中，tf.keras在`model.metrics`中0位置处添加了一个额外的首指标（见[TF issue #38150](https://github.com/tensorflow/tensorflow/issues/38150)），这迫使我们使用`model.metrics[-1]`而不是`model.metrics[0]`以获取`HuberMetric`：

```python
model.metrics[-1].threshold
```

当使用一个简单函数定义一个指标，Keras自动为每批调用它，并记录每代过程中的均值，就像上面手动做的那样。因此`HuberMetric`类的唯一好处是保存了`threshold`。但是一些指标，例如精确率，不能简单地对批进行平均。在这些情况下，只能实现流指标。

上面的`HuberMetric`类只用于演示。更简单也更好的实现是直接子类化`keras.metrics.Mean`类：

```python
class HuberMetric(keras.metrics.Mean):
    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):
        self.threshold = threshold
        self.huber_fn = create_huber(threshold)
        super().__init__(name=name, dtype=dtype)
    def update_state(self, y_true, y_pred, sample_weight=None):
        metric = self.huber_fn(y_true, y_pred)
        super(HuberMetric, self).update_state(metric, sample_weight)
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}        
```

该类可以更好地处理形状，并且还支持样本权重。

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1),
])
```

```python
model.compile(loss=keras.losses.Huber(2.0), optimizer="nadam", weighted_metrics=[HuberMetric(2.0)])
```

```python
sample_weight = np.random.rand(len(y_train))
history = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),
                    epochs=2, sample_weight=sample_weight)
```

```python
history.history["loss"][0], history.history["HuberMetric"][0] * sample_weight.mean()
```

```python
model.save("my_model_with_a_custom_metric_v2.h5")
```

```python
model = keras.models.load_model("my_model_with_a_custom_metric_v2.h5",
                                custom_objects={"HuberMetric": HuberMetric})
```

```python
model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)
```

```python
model.metrics[-1].threshold
```

### 自定义层

如果想要构建一个结构，它包含TensorFlow未提供默认实现的层，则需要创建自定义层；如果想构建一个非常重复的结构，它包含重复很多次的相同层块，此时将每个层块看做单一层会很方便。例如，如果模型为层序列：A、B、C、A、B、C、A、B、C，则可以定义一个自定义层D，它包含层A、B、C，因此模型就是D、D、D。

一些层没有权重，例如`keras.layers.Flatten`或`keras.layers.ReLU`。如果要创建一个没有任何权重的自定义层，最简单的选择是编写一个函数并用`keras.layers.Lambda`包装它。例如，以下层对它的输出应用指数函数：

```python
exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))
```

```python
exponential_layer([-1., 0., 1.])
```

这个自定义层可以像其他任何层一样使用，使用顺序API、函数式API或子类化API。还可以将它当做激活函数（或者也可以使用`activation=tf.exp, activation=keras.activations.exponential`，或简单地使用`activation="exponential`）。当预测值的尺度非常不同（例如0.001、10、1000）时，指数层有时用在回归模型的输出层中。

> This custom layer can then be used like any other layer, using the Sequential API, the Functional API, or the Subclassing API.

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=input_shape),
    keras.layers.Dense(1),
    exponential_layer
])
model.compile(loss="mse", optimizer="sgd")
model.fit(X_train_scaled, y_train, epochs=5,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
```

为了构建一个自定义有状态层（有权重的层），需要创建`keras.layers.Layer`类的子类。例如，以下类实现`Dense`层的简化版本：

```python
class MyDense(keras.layers.Layer):
    def __init__(self, units, activation=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, batch_input_shape):
        self.kernel = self.add_weight(
            name="kernel", shape=[batch_input_shape[-1], self.units],
            initializer="glorot_normal")
        self.bias = self.add_weight(
            name="bias", shape=[self.units], initializer="zeros")
        super().build(batch_input_shape) # must be at the end

    def call(self, X):
        return self.activation(X @ self.kernel + self.bias)

    def compute_output_shape(self, batch_input_shape):
        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "units": self.units,
                "activation": keras.activations.serialize(self.activation)}
```

- 构造器接受所有超参数作为实参（在该例子中，是`units`与`activation`），重要的是，它还接受`**kwargs`实参。它调用父构造器，传递给它`kwargs`：这将处理标准实参，例如`input_shape`、`trainable`与`name`。然后它将超参数保存为属性，使用`keras.activations.get`函数（它接受函数、诸如`"relu"`或`"selu"`的标准字符串或简单为`None`，这个函数专属于tf.keras，可以用`keras.layers.Activation`代替之）将`activation`实参转换为合适（appropriate）的激活函数。
- `build`方法的作用是通过调用`add_weight`方法为每个权重创建层变量。`build`方法在层第一次使用时被调用。此时，Keras将知道层的输出的形状，并将其传递给`build`方法（Keras称该实参为`input_shape`，因为它还包含批维度，这里称它为`batch_input_shape`，对于`compute_output_shpae`方法同理），它常常是创建一些权重所必需的。例如，我们需要知道前一层的神经元数量以便创建连接权重矩阵（即`kernel`），它对应输入的最后一个维度的大小。在`build`方法的末尾（并且只在末尾），必须调用父`build`方法，它告诉Keras该层已被构建（它只是设置`self.built=True`）。
- `call`方法执行所需运算。该例计算输入`X`与层的核的矩阵乘法，加上偏置向量，并对结果应用激活函数，这样就得到了该层的输出。
- `compute_output_shape`方法只是返回该层输出的形状。在这里，它与输入的形状一致，只是最后一个维度被该层中的神经元数量所代替。注意在tf.keras中，形状为`tf.TensorShape`类的实例，可以使用`as_list`方法将其转换为Python列表。通常可以忽略`compute_output_shape`方法，因为tf.keras会自动推导输出形状，除了当层是动态的时候。在其他Keras实现中，该方法要么是必需的，要么它的默认实现假定输出形状与输入形状相同。
- `get_config`方法就像前面的自定义类一样。注意这里通过调用`keras.activations.serialize`函数保存激活函数的所有配置。

> The build() method’s role is to create the layer’s variables by calling the add_weight() method for each weight.

> which is often necessary to create some of the weights.

这样就可以像其他任何层一样使用`MyDense`层了。

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential([
    MyDense(30, activation="relu", input_shape=input_shape),
    MyDense(1)
])
```

```python
model.compile(loss="mse", optimizer="nadam")
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
```

```python
model.save("my_model_with_a_custom_layer.h5")
```

```python
model = keras.models.load_model("my_model_with_a_custom_layer.h5",
                                custom_objects={"MyDense": MyDense})
```

为了创建有多个输出的层（例如，`Concatenate`），`call`方法的实参应该是一个包含所有输入的元组，类似地，`compute_output_shape`的实参应该是一个包含每个输入的批形状的元素。为了创建有多个输出的层，`call`方法应该返回输出列表，`compute_output_shape`应该返回批输出形状的列表（每个输出一个）。例如，以下层接受两个输出并返回三个输出：

> a tuple containing each input’s batch shape.

```python
class MyMultiLayer(keras.layers.Layer):
    def call(self, X):
        X1, X2 = X
        print("X1.shape: ", X1.shape ," X2.shape: ", X2.shape) # Debugging of custom layer
        return X1 + X2, X1 * X2

    def compute_output_shape(self, batch_input_shape):
        batch_input_shape1, batch_input_shape2 = batch_input_shape
        return [batch_input_shape1, batch_input_shape2]
```

可以使用函数式API调用该自定义层：

```python
inputs1 = keras.layers.Input(shape=[2])
inputs2 = keras.layers.Input(shape=[2])
outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))
```

注意`call`方法接受符号输入，它的形状只被部分指定（在该阶段，还不知道批大小，这是就是为什么第一个维度为`None`）。

也可以向该自定义层传递实际数据。为了测试它，这里将每个数据集的输入分成两部分，每部分四个特征：

```python
def split_data(data):
    columns_count = data.shape[-1]
    half = columns_count // 2
    return data[:, :half], data[:, half:]

X_train_scaled_A, X_train_scaled_B = split_data(X_train_scaled)
X_valid_scaled_A, X_valid_scaled_B = split_data(X_valid_scaled)
X_test_scaled_A, X_test_scaled_B = split_data(X_test_scaled)

# Printing the splitted data shapes
X_train_scaled_A.shape, X_train_scaled_B.shape
```

注意，形状被完全指定：

```python
outputs1, outputs2 = MyMultiLayer()((X_train_scaled_A, X_train_scaled_B))
```

下面使用函数式API构建一个更具体的模型（只是一个玩具示例，表现未必很好）：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

input_A = keras.layers.Input(shape=X_train_scaled_A.shape[-1])
input_B = keras.layers.Input(shape=X_train_scaled_B.shape[-1])
hidden_A, hidden_B = MyMultiLayer()((input_A, input_B))
hidden_A = keras.layers.Dense(30, activation='selu')(hidden_A)
hidden_B = keras.layers.Dense(30, activation='selu')(hidden_B)
concat = keras.layers.Concatenate()((hidden_A, hidden_B))
output = keras.layers.Dense(1)(concat)
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])
```

```python
model.compile(loss='mse', optimizer='nadam')
```

```python
model.fit((X_train_scaled_A, X_train_scaled_B), y_train, epochs=2,
          validation_data=((X_valid_scaled_A, X_valid_scaled_B), y_valid))
```

如果需要模型在训练过程中与测试过程中的表现不同（例如，如果它使用`Dropout`或`BatchNormalization`层），则必须给`call`方法添加一个`training`实参并使用该实参决定做什么。例如，下面创建一个层，它在训练过程中添加高斯噪音（用于正则化），但是在测试过程中什么也不做（Keras的`keras.layers.GaussianNoise`做同样的事）：

```python
class AddGaussianNoise(keras.layers.Layer):
    def __init__(self, stddev, **kwargs):
        super().__init__(**kwargs)
        self.stddev = stddev

    def call(self, X, training=None):
        if training:
            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)
            return X + noise
        else:
            return X

    def compute_output_shape(self, batch_input_shape):
        return batch_input_shape
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    AddGaussianNoise(stddev=1.0),
    keras.layers.Dense(30, activation="selu"),
    keras.layers.Dense(1)
])
```

```python
model.compile(loss="mse", optimizer="nadam")
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
```

有了这些，就可以构建任意所需的自定义层了。

### 自定义模型

前面已经展示了如何自定义模型：子类化`keras.Model`类，在构造器中创建层与变量，实现`call`方法来执行模型要做的事。假设要构建下图所示的模型。

![自定义模型示例：一个任意模型，它有一个包含一个跳跃连接的自定义ResidualBlock层](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\自定义模型示例.png)

输入经过第一个密集层，然后经过一个由两个密集层与一个额外操作组成的**残差块（residual block）**，然后再通过这个相同的残差块三次，然后经过第二个残差块，最终的结果经过一个密集输出层。这个模型没有多大意义，只是一个例子，说明这样的一个事实：可以轻松地构建任何想要的模型，即使它包含环（loops）或跳跃连接。为了实现这个模型，最好首先创建一个`ResidualBlock`层，因为这里要创建两个相同的块（也因为可能希望在另一个模型中重用它）：

```python
X_new_scaled = X_test_scaled
```

```python
class ResidualBlock(keras.layers.Layer):
    def __init__(self, n_layers, n_neurons, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(n_neurons, activation="elu",
                                          kernel_initializer="he_normal")
                       for _ in range(n_layers)]

    def call(self, inputs):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        return inputs + Z
```

这个层有点特殊，因为它包含其他层。Keras会自动检测到`hidden`属性包含可记录（trackable）对象（在这里是层），因此它们的变量被自动加到该层的变量列表中。下面使用该子类化API定义模型本身：

```python
class ResidualRegressor(keras.models.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = keras.layers.Dense(30, activation="elu",
                                          kernel_initializer="he_normal")
        self.block1 = ResidualBlock(2, 30)
        self.block2 = ResidualBlock(2, 30)
        self.out = keras.layers.Dense(output_dim)

    def call(self, inputs):
        Z = self.hidden1(inputs)
        for _ in range(1 + 3):
            Z = self.block1(Z)
        Z = self.block2(Z)
        return self.out(Z)
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = ResidualRegressor(1)
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train_scaled, y_train, epochs=5)
score = model.evaluate(X_test_scaled, y_test)
y_pred = model.predict(X_new_scaled)
```

```python
model.save("my_custom_model.ckpt")
```

```python
model = keras.models.load_model("my_custom_model.ckpt")
```

```python
history = model.fit(X_train_scaled, y_train, epochs=5)
```

这里在构造器中创建了层并在`call`方法中使用它们。该模型可以像其他任何模型一样使用（编译它、拟合它、评估它、使用它做预测）。如果要求能够使用`save`方法保存模型并使用`keras.models.load_model`函数加载模型，则必须同时在`ResidualBlock`类与`ResidualRegressor`类中实现`get_config`方法。可选地，可以使用`save_weights`与`load_weights`方法保存与加载模型。

也可以使用顺序API定义该模型：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
block1 = ResidualBlock(2, 30)
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="elu", kernel_initializer="he_normal"),
    block1, block1, block1, block1,
    ResidualBlock(2, 30),
    keras.layers.Dense(1)
])
```

```python
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train_scaled, y_train, epochs=5)
score = model.evaluate(X_test_scaled, y_test)
y_pred = model.predict(X_new_scaled)
```

`Model`类是`Layer`类的子类，因此可以像层一样定义与使用模型。但是模型有一些额外的功能，包括它的`complie`、`fit`、`evaluate`、`predict`方法（以及一些变体），`get_layers`方法（可以通过名字或索引返回模型的任何层）与`save`方法（以及对`keras.models.load_model`与`keras.models.clone_model`函数的支持）。

> which can return any of the model’s layers by name or by index

因为模型提供了比层更多的功能，因此技术上来说，可以将所有的层定义为模型。但是通常来说，将模型的内部组件（即层或可重用的层块）与模型本身（即要训练的对象）区分开来要更清晰。前者应该子类化`Layer`类，后者应该子类化`Model`类。

有了这些，就可以使用顺序API、函数式API、子类化API甚至它们的组合来自然简洁地构建论文中找到的*几乎*任何模型。但是仍然有一些需要关注的事情：如何基于模型内部（model internals）定义损失或指标；如何构建一个自定义训练循环（training loop）。

> With that, you can naturally and concisely build almost any model that you find in a paper,

### 基于模型内部的损失与指标

前面定义的自定义损失与指标都基于标签与预测（以及可选的样本权重）。有时需要定义基于模型其他部分的损失，例如权重或其隐藏层的活性（activations）。这对于正则化或监视模型的某些内部方面可能有用。

要根据模型内部定义自定义损失，则根据模型的任何部分计算它，然后将结果传递给`add_loss`方法。例如，下面构建一个自定义回归MLP模型，它由五个隐藏层组成的栈以及一个输出层组成。该自定义模型在上面隐藏层的顶部还有一个辅助输出。与该辅助输出关联的损失被称为**重构损失（reconstruction loss）**：它是重构与输入之间的均方差异（mean squared difference）。通过将该重构损失加到主损失上，可以促进模型在通过隐藏层时尽可能多地保留信息，即使对回归任务本身没有直接用处的信息。在实践中，该损失有时能改善泛化性能（它是正则化损失）。以下代码展示了带有自定义重构损失的自定义模型：

```python
class ReconstructingRegressor(keras.models.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(30, activation="selu",
                                          kernel_initializer="lecun_normal")
                       for _ in range(5)]
        self.out = keras.layers.Dense(output_dim)
        self.reconstruct = keras.layers.Dense(8) # workaround for TF issue #46858
        self.reconstruction_mean = keras.metrics.Mean(name="reconstruction_error")

    #Commented out due to TF issue #46858, see the note above
    #def build(self, batch_input_shape):
    #    n_inputs = batch_input_shape[-1]
    #    self.reconstruct = keras.layers.Dense(n_inputs)
    #    super().build(batch_input_shape)

    def call(self, inputs, training=None):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        reconstruction = self.reconstruct(Z)
        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        self.add_loss(0.05 * recon_loss)
        if training:
            result = self.reconstruction_mean(recon_loss)
            self.add_metric(result)
        return self.out(Z)
```

- 构造器创建了有5个隐藏层与1个密集输出层的DNN，以及一个额外的密集层。注意，因为TF 2.2[#46858](https://github.com/tensorflow/tensorflow/issues/46858)中引入的一个问题，目前无法与`build`方法一起使用`add_loss`。因此这里在构造器中而不是在`build`方法内创建重构层。不幸的是，这意味着该层中的单元数量必须被硬编码（或者可以将其作为一个实参传递给构造器）。
- `call`方法处理通过所有五个隐藏层处理输入，然后通过将结果传递给重构层，它产生重构。
- `call`方法计算重构损失（重构与输出之间的均方差异），并使用`add_loss`方法将它加到模型的损失列表上（还可以在模型内的任意层上调用`add_loss`方法，因为模型递归地从它的所有层中收集损失）。注意这里将重构损失乘以0.05来缩小它（这是可以调整的超参数）。这保证了重构损失不会主导主损失。
- 最后，`call`方法将隐藏层的输出传递给输出层并返回输出层的输出。

类似地，可以通过任何想要的方式计算自定义指标，从而基于模型内部添加它，只是结果是指标对象的输出。例如，可以在构造器中创建一个`keras.metrics.Mean`对象，然后在`call`方法中调用它，传递给它`recon_loss`，最后通过调用模型的`add_metric`方法将其添加到模型中。这样，当训练模型时，Keras会同时展示每代的平均损失（该损失是主损失加上0.05乘以重构损失）以及每代的平均重构误差。在训练过程中两者都会下降：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = ReconstructingRegressor(1)
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train_scaled, y_train, epochs=2)
y_pred = model.predict(X_test_scaled)
```

在超过99%的情况下，目前讨论的知识已足够实现想要构建的任何模型，即使它有复杂的结构、损失与指标。但是某些罕见情况下，可能需要自定义训练循环本身。

## 使用自动微分计算梯度

考虑一个简单的函数：

```python
def f(w1, w2):
    return 3 * w1 ** 2 + 2 * w1 * w2
```

该函数对`w1`的偏导为`6 * w1 + 2 * w2`，对`w2`的偏导为`w * w1`。但是神经网络的函数复杂得多，通常有数万参数，手动分析去找到偏导几乎不可能。一种方法是通过度量当微调对应参数后函数的输出变化多大来计算每个偏导的近似值：

> But if this were a neural network, the function would be much more complex,

```python
w1, w2 = 5, 3
eps = 1e-6
(f(w1 + eps, w2) - f(w1, w2)) / eps
```

```python
(f(w1, w2 + eps) - f(w1, w2)) / eps
```

该方法工作得相当好并且很容易实现，但是它只是一种近似，并且至少需要为每个参数调用一次`f`函数（不是两次，因为可以只计算一次`f(w1, w2)`）。至少需要为每次参数调用一次`f`函数使得该方法难以处理大型神经网络。因此，应该使用自动微分。TensorFlow计算如下：

```python
w1, w2 = tf.Variable(5.), tf.Variable(3.)
with tf.GradientTape() as tape:
    z = f(w1, w2)

gradients = tape.gradient(z, [w1, w2])
```

这里首先定义两个变量`w1`与`w2`，然后创建一个`tf.GradientTape`上下文，它自动记录涉及变量的每个运算，最后询问该带（tape）去计算结果`z`关于两个变量`[w1, w2]`的梯度。下面查看TensorFlow计算得到的梯度：

> that will automatically record every operation that involves a variable,

```python
gradients
```

结果不仅准确（精确仅受浮点误差的限制），并且`gradient`方法只（反向）经过记录的计算一次，不管有多少变量，因此它非常高效。

> but the gradient() method only goes through the recorded computations once

为了节省内存，只需将严格最小部分放在`tf.GradientTape()`块中，或者通过在`tf.GradientTape()`块内创建`with tape.stop_recording()`块来停止记录。

当调用带的`gradident`方法后，带被自动清除，因此如果尝试两次调用`gradient`方法会得到一个异常：

```python
with tf.GradientTape() as tape:
    z = f(w1, w2)

dz_dw1 = tape.gradient(z, w1)
try:
    dz_dw2 = tape.gradient(z, w2)
except RuntimeError as ex:
    print(ex)
```

如果需要多次调用`gradient`方法，必须让带持久化，并且每次完成后必须删除它以节省内存（如果带超出作用域，Python垃圾收集器会自动删除它）：

```python
with tf.GradientTape(persistent=True) as tape:
    z = f(w1, w2)

dz_dw1 = tape.gradient(z, w1)
dz_dw2 = tape.gradient(z, w2) # works now!
del tape
```

```python
dz_dw1, dz_dw2
```

默认情况下，带只记录涉及变量的运算，因此如果尝试计算`z`关于任何不是变量的事物的梯度，则结果为`None`：

```python
c1, c2 = tf.constant(5.), tf.constant(3.)
with tf.GradientTape() as tape:
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2])
```

```python
gradients
```

但是可以迫使带去监视任何想要的张量，去记录任何涉及它们的运算。然后就可以计算梯度关于这些张量的梯度，就好像它们是变量一样：

```python
with tf.GradientTape() as tape:
    tape.watch(c1)
    tape.watch(c2)
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2])
```

```python
gradients
```

这在某些情况下有用。例如，如果想要实现一个正则化损失，它惩罚当输入变化很小时变化很大的活性，此时损失将基于活性关于输入的梯度。因此输入不是变量，所以需要告诉带去监视它们。

大多数时候，一个梯度带用于计算单个值（通常是损失）关于一组值（通常是模型参数）的梯度。这是反向自动微分擅长的地方，因为它只需要一次前向传递（forward pass ）与一次反向传递（reverse pass）就可以一次性得到所有梯度。如果尝试计算一个向量的梯度，例如一个包含多个损失的向量，则TensorFlow将计算向量和的梯度。因此如果需要获得每个梯度（例如，每个损失关于模型参数的梯度），则必须调用带的`jacobian`方法，它会为向量中的每个损失执行反向自动微分（默认情况下所有操作都是并行的）。甚至可以计算二阶偏导（Hessians，即偏导的偏导），但是在实践中很少需要这么做。

```python
with tf.GradientTape() as tape:
    z1 = f(w1, w2 + 2.)
    z2 = f(w1, w2 + 5.)
    z3 = f(w1, w2 + 7.)

tape.gradient([z1, z2, z3], [w1, w2])
```

```python
with tf.GradientTape(persistent=True) as tape:
    z1 = f(w1, w2 + 2.)
    z2 = f(w1, w2 + 5.)
    z3 = f(w1, w2 + 7.)

tf.reduce_sum(tf.stack([tape.gradient(z, [w1, w2]) for z in (z1, z2, z3)]), axis=0)
del tape
```

```python
with tf.GradientTape(persistent=True) as hessian_tape:
    with tf.GradientTape() as jacobian_tape:
        z = f(w1, w2)
    jacobians = jacobian_tape.gradient(z, [w1, w2])
hessians = [hessian_tape.gradient(jacobian, [w1, w2])
            for jacobian in jacobians]
del hessian_tape
```

```python
jacobians
```

```python
hessians
```

在一些情况下可能想要阻止梯度反向传播通过神经网络的某些部分，为此必须使用`tf.stop_gradient`函数。该函数在前向传递过程中返回它的输入（就像`tf.identity`函数），但是在反向传播过程中它不允许梯度通过（它就像一个常量一样）：

```python
def f(w1, w2):
    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)

with tf.GradientTape() as tape:
    z = f(w1, w2)

tape.gradient(z, [w1, w2])
```

最后，当计算梯度时偶尔会遇到一些数值问题。例如，如果为大的输入计算`my_softplus`函数的梯度，则结果为NaN：

```python
x = tf.Variable(100.)
with tf.GradientTape() as tape:
    z = my_softplus(x)

tape.gradient(z, [x])
```

```python
tf.math.log(tf.exp(tf.constant(30., dtype=tf.float32)) + 1.)
```

```python
x = tf.Variable([100.])
with tf.GradientTape() as tape:
    z = my_softplus(x)

tape.gradient(z, [x])
```

这是因为使用自动微分计算该函数的梯度导致一些数值困难：因为浮点精度误差，自动微分最终计算无穷大除以无穷大（返回NaN）。幸运的是，可以分析得到该softplus函数的梯度为$1/(1+1/\exp(x))$，它是数值稳定的。接下来，可以告诉TensorFlow，当计算`my_softplus`函数的梯度时使用该稳定函数，方法是使用`@tf.custom_gradient`装饰它并令其同时返回它的正常输出与计算该梯度的函数（注意它接受目前反向传播到softplus函数的梯度作为输入，根据链式法则，应该将它们乘以该函数的梯度）：

```python
@tf.custom_gradient
def my_better_softplus(z):
    exp = tf.exp(z)
    def my_softplus_gradients(grad):
        return grad / (1 + 1 / exp)
    return tf.math.log(exp + 1), my_softplus_gradients
```

现在当计算`my_better_softplus`函数的梯度时，将得到正确的结果，即使输入值很大（然而，由于指数的存在，主输出仍然会爆炸，一种变通方法是在输入很大时使用`tf.where`函数返回输入）。

```python
def my_better_softplus(z):
    return tf.where(z > 30., z, tf.math.log(tf.exp(z) + 1.))
```

```python
x = tf.Variable([1000.])
with tf.GradientTape() as tape:
    z = my_better_softplus(x)

z, tape.gradient(z, [x])
```

### 自定义训练循环

在某些罕见的情况下，`fit`方法可能不够灵活，无法满足需要。例如，[Wide & Deep paper](https://homl.info/widedeep)网络使用不同的优化器：一个用于宽度路径，一个用于深度路径。因为`fit`方法只使用一个优化器（当编译模型时指定的优化器），实现这篇论文需要编写自定义循环。

编写自定义训练循环有时只是为了更确认它们做我们希望做的事情（可能不确定`fit`方法的某些细节）。有时让任何事情清楚明白（explicit）会让人觉得更安全。但是编写自定义训练循环也会使得代码更长，更易出错，并且更难维护。除非真的需要额外的灵活性，最好使用`fit`方法而不是实现自定义训练循环，尤其是在团队中工作时。

下面构建一个简单的模型。这里不需要编译它，因为后面会手动处理训练循环：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
l2_reg = keras.regularizers.l2(0.05)
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="elu", kernel_initializer="he_normal",
                       kernel_regularizer=l2_reg),
    keras.layers.Dense(1, kernel_regularizer=l2_reg)
])
```

接下来创建一个小函数，它随机从训练集中采样一批实例（Data API提供了好得多的选择）：

```python
def random_batch(X, y, batch_size=32):
    idx = np.random.randint(len(X), size=batch_size)
    return X[idx], y[idx]
```

同时定义一个函数用于展示训练状态，包括步数、总步数、自代开始的平均损失（即使用`Mean`指标计算它），以及其他指标：

```python
def print_status_bar(iteration, total, loss, metrics=None):
    metrics = " - ".join(["{}: {:.4f}".format(m.name, m.result())
                         for m in [loss] + (metrics or [])])
    end = "" if iteration < total else "\n"
    print("\r{}/{} - ".format(iteration, total) + metrics,
          end=end)
```

```python
import time

mean_loss = keras.metrics.Mean(name="loss")
mean_square = keras.metrics.Mean(name="mean_square")
for i in range(1, 50 + 1):
    loss = 1 / i
    mean_loss(loss)
    mean_square(i ** 2)
    print_status_bar(i, 50, mean_loss, [mean_square])
    time.sleep(0.05)
```

一个更高级（fancier）的版本如下，它包含一个进度条（也可以使用`tdqm`库）：

```python
def progress_bar(iteration, total, size=30):
    running = iteration < total
    c = ">" if running else "="
    p = (size - 1) * iteration // total
    fmt = "{{:-{}d}}/{{}} [{{}}]".format(len(str(total)))
    params = [iteration, total, "=" * p + c + "." * (size - p - 1)]
    return fmt.format(*params)
```

```python
progress_bar(3500, 10000, size=6)
```

```python
def print_status_bar(iteration, total, loss, metrics=None, size=30):
    metrics = " - ".join(["{}: {:.4f}".format(m.name, m.result())
                         for m in [loss] + (metrics or [])])
    end = "" if iteration < total else "\n"
    print("\r{} - {}".format(progress_bar(iteration, total), metrics), end=end)
```

```python
mean_loss = keras.metrics.Mean(name="loss")
mean_square = keras.metrics.Mean(name="mean_square")
for i in range(1, 50 + 1):
    loss = 1 / i
    mean_loss(loss)
    mean_square(i ** 2)
    print_status_bar(i, 50, mean_loss, [mean_square])
    time.sleep(0.05)
```

下面需要定义一些超参数并选择优化器、损失函数与指标：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
n_epochs = 5
batch_size = 32
n_steps = len(X_train) // batch_size
optimizer = keras.optimizers.Nadam(lr=0.01)
loss_fn = keras.losses.mean_squared_error
mean_loss = keras.metrics.Mean()
metrics = [keras.metrics.MeanAbsoluteError()]
```

现在可以构建自定义循环了：

```python
for epoch in range(1, n_epochs + 1):
    print("Epoch {}/{}".format(epoch, n_epochs))
    for step in range(1, n_steps + 1):
        X_batch, y_batch = random_batch(X_train_scaled, y_train)
        with tf.GradientTape() as tape:
            y_pred = model(X_batch)
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
            loss = tf.add_n([main_loss] + model.losses)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        for variable in model.variables:
            if variable.constraint is not None:
                variable.assign(variable.constraint(variable))
        mean_loss(loss)
        for metric in metrics:
            metric(y_batch, y_pred)
        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)
    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)
    for metric in [mean_loss] + metrics:
        metric.reset_states()
```

- 这里首先创建了两个嵌套循环：一个用于代，一个用于一代内的批。
- 然后从训练集中采样一个随机批。
- 在`tf.GradientTape()`块内，我们将模型用作函数，为一批做出预测，并计算损失，它等于主损失加上其他损失（在该模型中，每层有一个正则化损失）。因为`mean_squared_error`函数为每个实例返回一个损失，这里使用`tf.reduce_mean`计算批的损失均值（如果想要对不同实例应用不同权重，则可以在这里做）。正则化损失已经规约（reduce）为单个标量，因此只需要把它们加起来（使用`tf.add_n`函数，它将形状与数据类型相同的多个张量加起来）。
- 接下来，我们请求带去计算损失关于每个可训练变量（不是所有变量）的梯度，并将它们应用于优化器，以执行梯度下降步。
- 然后更新（当前代的）平均损失与指标，并展示状态条。
- 在每代的末尾，我们再次展示状态条，使其看起来完整（这里没有处理训练集中的每个实例，因为实例被随机采样：一些实例被多次处理，一些实例根本没被处理。类似地，如果训练集大小不是批大小的倍数，也会遗漏一些实例。在实践中，这没问题。），并打印换行，然后重置平均损失与指标的状态。

如果设置优化器的`clipnorm`或`clipvalue`超参数，它会自动处理好。如果想要将其他任何转换应用于优化器，则只需在调用`apply_gradients`前执行此操作。

如果将权重约束加到模型中（例如通过在创建层时设置`kernel_constraint`或`bias_constraint`），则应该在（just after）`apply_gradients`方法后应用这些约束（如上）。

最重要的是，这个训练循环没有处理在训练与测试过程中表现不同的层（例如，`BatchNormalization`或`Dropout`）。为了处理这些，需要使用`training=True`调用该模型并确保它将其传播到每个需要它的层。

使用`tdqm`库的版本如下：

```python
try:
    from tqdm.notebook import trange
    from collections import OrderedDict
    with trange(1, n_epochs + 1, desc="All epochs") as epochs:
        for epoch in epochs:
            with trange(1, n_steps + 1, desc="Epoch {}/{}".format(epoch, n_epochs)) as steps:
                for step in steps:
                    X_batch, y_batch = random_batch(X_train_scaled, y_train)
                    with tf.GradientTape() as tape:
                        y_pred = model(X_batch)
                        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
                        loss = tf.add_n([main_loss] + model.losses)
                    gradients = tape.gradient(loss, model.trainable_variables)
                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
                    for variable in model.variables:
                        if variable.constraint is not None:
                            variable.assign(variable.constraint(variable))                    
                    status = OrderedDict()
                    mean_loss(loss)
                    status["loss"] = mean_loss.result().numpy()
                    for metric in metrics:
                        metric(y_batch, y_pred)
                        status[metric.name] = metric.result().numpy()
                    steps.set_postfix(status)
            for metric in [mean_loss] + metrics:
                metric.reset_states()
except ImportError as ex:
    print("To run this cell, please install tqdm, ipywidgets and restart Jupyter")
```

可以看到，当自定义训练循环时，要确保很多事情正确，并很容易犯错。好的一面是，可以完全控制训练过程。

这样就知道了如何去自定义模型与训练算法的任何部分了（除了优化器，因为极少有人定制这些）。

## TensorFlow函数与图

在TensorFlow 1中，图（以及随之而来的复杂性）是不可避免的，因为它们是TensorFlow API的核心部分。在TensorFlow中，它们仍然存在，但是不是核心，并且容易使用得多。为了展示它有多简单，下面以一个计算其输入的立方的函数开始：

```python
def cube(x):
    return x ** 3
```

可以使用一个Python值（例如一个int或一个float）或一个张量来调用该函数：

```python
cube(2)
```

```python
cube(tf.constant(2.0))
```

下面使用`tf.function`函数将该Python函数转换为一个**TensorFlow函数（TensorFlow Function）**：

```python
tf_cube = tf.function(cube)
tf_cube
```

这个TF函数可以像原始Python函数一样使用，并且它会返回相同结果（但作为张量返回）：

```python
tf_cube(2)
```

```python
tf_cube(tf.constant(2.0))
```

`tf.function`函数分析`cube`函数执行的计算并生成一个等价的计算图。或者也可以将`tf.function`用作装饰器，这实际上更常见：

```python
@tf.function
def tf_cube(x):
    return x ** 3
```

原始Python函数仍然可以通过TF函数的`python_function`属性获得：

```python
tf_cube.python_function(2)
```

TensorFlow优化计算图，修剪掉未使用的结点，简化表达式（例如，用$3$代替$1+2$），等等。一旦优化图准备好，TF函数以适当顺序（并行，如果可以的话）高效执行图中的运算。因此TF函数的运行速度通常比原始Python函数快得多，尤其当它执行复杂运算时（然而，在这个简单例子中，计算图太小，没有需要优化的地方，因此`tf_cube`函数的运行速度实际上比`cube`函数慢得多）。

然而，当编写一个自定义损失函数、自定义指标、自定义层或任何其他自定义函数，并将它用于Keras模型中时，Keras自动将该函数转换为一个TF函数，不需要使用`tf.function`函数。

可以通过在创建一个自定义层或自定义模型时设置`dynamic=True`来告诉Keras不要将Python函数转换为TF函数。或者，可以在调用模型的`compile`方法时设置`run_eagerly=True`。

TF函数是多态的，意味着它们支持不同类型与形状的输入。

默认情况下，TF函数为每组的输入形状与数据类型生成一个新图，并将其缓存以供后续调用。每次使用新的输入类型与形状的组合来调用一个TF函数时，它会生成一个新的**具体函数（concrete function）**，它有其自己的专门用于这个组合的图。这种实参类型与形状的组合被称为**输入签名（input signature）**。如果使用一个TF函数已经见过的输入签名去调用该TF函数，则它会重用它之前生成过的具体函数。例如，如果调用`tf_cube(tf.constant(3.0))`，则TF函数会重用它用于`tf_cube(tf.constant(2.0))`（float32标量张量）的具体函数。但是如果调用`tf_cube(tf.constant([2.0]))`或`tf_cube(tf.constant([3.0]))`（形状为[1]的float32张量），则它会生成一个新的具体函数；对于`tf_cube(tf.constant([[1.0, 2.0], [3.0, 4.0]]))`（形状为[2, 2]的float32张量），则又会生成一个。这就是TF函数处理多态（polymorphism）（即可变（varying）实参类型与形状）的方式。然而，这只适用于张量实参：如果将数值Python值（numerical Python values）传递给TF函数，则会为每个不同的值生成一个新图。例如，调用`tf_cube(10)`与`tf_cube(20)`将生成两个图。

如果使用不同数值Python值调用一个TF函数许多次，则会产生许多图，降低程序速度并使用大量内存（必须删除TF函数以释放它）。Python值应该只用于很少有唯一值的实参，例如像每层的神经元的数量这样的超参数。这使得TensorFlow可以更好地优化模型的每个变体（variant）。

> Python values should be reserved for arguments that will have few unique values, such as hyperparameters like the number of neurons per layer.

可以通过调用TF函数的`get_concrete_function`方法获得具有特定输入组合的具体函数。然后它可以像常规函数一样被调用，但是它只支持一个输入签名（在这个例子中为float32标量张量）。

```python
concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))
concrete_function.graph
```

```python
concrete_function(tf.constant(2.0))
```

```python
concrete_function is tf_cube.get_concrete_function(tf.constant(2.0))
```

下图展示了当调用`tf_cube(2)`与`tf_cube(tf.constant(2.0))`后的`tf_cube` TF函数：生成了两个具体函数，每个签名一个，每个都有自己的优化后的**函数图（function graph）**（`FuncGraph`），以及自己的**函数定义（function definition）**（`FunctionDef`）。一个函数定义指向图中与函数输入与输出对应的部分。在每个`FuncGraph`中，结点（椭圆形）代表运算（例如，幂、常量或实参占位符，如`x`），而边（运算间的实线）表示流经图的张量。左边的具体函数专用于`x = 2`，因此TensorFlow将其简化为始终只输出8（注意该函数定义甚至没有输入）。右边的具体函数专用于float32标量张量，它无法被简化。如果调用`tf_cube(tf.constant(5.0))`，则第二个具体函数将被调用，`x`的占位符运算将输出5.0，然后幂运算将计算`5.0 ** 3`，因此输入为125.0。

![tf_cube TF函数与它的具体函数与函数图](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\tf_cube TF函数.png)

图中的张量为**符号张量（symbolic tensors）**，意味着它们没有实际值，只有一个数据类型、一个形状与一个名字。它们代表一旦实际值馈送给占位符`x`其图执行时的流经图的未来张量。符号张量使得提前指定如何连接运算称为可能，它们还允许TensorFlow递归地推断所有张量的数据类型与形状，只要给定它们的输入的数据类型与形状。

## 函数定义与图

可以使用`graph`属性获取一个具体函数的计算图，通过调用`get_operations`方法得到它的运算列表：

```python
concrete_function.graph
```

```python
ops = concrete_function.graph.get_operations()
ops
```

在这个例子中，第一个运算代表输入实参`x`（它被称为占位符），第二个“运算”代表常量3，第三个运算代表幂运算（`**`），最后一个运算代表函数的输出（它是恒等运算，意味着它只拷贝该额外运算的输出（可以安全地忽略它，它出现在这里只是出于技术原因，以保证TF函数不泄露内部结构）。

> it is an identity operation, meaning it will do nothing more than copy the output of the addition operation

每个运算有一个输入与输出张量列表，可以使用运算的`inputs`与`output`属性获取它们。例如，下面获取幂运算的输入与输出列表：

```python
pow_op = ops[2]
list(pow_op.inputs)
```

```python
pow_op.outputs
```

下图显示了这个计算图：

![一个计算图示例](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个计算图示例.png)



注意每个运算都有一个名字，它默认为运算的名字（例如`"pow"`），但是可以在调用运算时手动定义它（例如，`tf.pow(x, 3, name="other_name")`。如果名字已存在，则TensorFlow自动添加一个唯一索引（例如`pow_1`，`pow_2`，等等）。每个张量也有一个唯一的名字，它总是输出该张量的运算的名字，加上一个`:0`（如果它是运算的第一个输出的话）或`:1`（如果它是第二个输出的话），等等。可以使用图的`get_operation_by_name`或`get_tensor_by_name`方法来通过名字获取一个运算或一个张量：

```python
concrete_function.graph.get_operation_by_name('x')
```

```python
concrete_function.graph.get_tensor_by_name('Identity:0')
```

具体函数还包含函数定义（表示为一个protocol buffer），它包含函数签名。该签名使得具体函数可以知道哪个占位符可以与输入值一起被馈送，以及返回哪个张量：

```python
concrete_function.function_def.signature
```

### AutoGraph与tracing

// 应该先介绍AutoGraph与tracing生成图的方式，然后介绍图的细节。

下面介绍TensorFlow生成图的方式。

TensorFlow首先分析Python函数的源代码以捕获所有控制流语句，例如`for`循环、`while`循环、`if`语句，以及`break`、`continue`与`return`语句。这个第一步被称为**AutoGraph**。TensorFlow要分析源代码的原因在于Python没有提供任何其他方法去捕获控制流语句：它提供了魔术方法例如`__add__`与`__mul__`去捕获诸如`+`与`*`的运算，但是没有`__while__`或`__if__`魔术方法。分析完函数的代码后，AutoGraph输出一个该函数的升级版本，其中所有控制流被合适的（appropriate）TensorFlow运算所代替，例如`tf.while_loop`用于for循环、`tf.cond`用于`if`语句。例如，在下图中，AutoGraph分析`sum_squares`Python函数的源代码，并生成`tf__sum_squares`函数。在该函数中，`for`循环被`loop_body`函数的定义所代替（包括原始`for`循环的主体），并跟随一个对`for_stmt`函数的调用。这个调用会在计算图中构建合适的`tf.while_loop`运算。

![使用AutoGraph与tracing生成TensorFlow图的方法](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用AutoGraph与tracing生成TensorFlow图的方法.png)

接下来，TensorFlow调用该“升级的”函数，但是它传递的不是一个实参，而是一个**符号张量（symbolic tensor）**——一个没有任何实际值，而是只有一个名字、一个数据类型以及一个形状的张量。例如，如果调用`sum_squares(tf.constant(10))`，则`tf__sum_squares`函数将通过一个类型为int32、形状为[]的符号张量被调用。这个函数运行在**图模式（graph mode）**中，这意味着每个TensorFlow运算会在图中加一个结点来代表它本身与它的输出张量（与常规模式不同，常规模式被称为**动态图执行（eager execution）**或**动态图模式（eager mode）**）。在图模式中，TF运算不执行任何计算。在上图中，可以看到`tf__sum_squares`函数通过将一个符号张量作为它的实参（在这个例子中是一个形状为[]的int32张量）而被调用，最终的图在tracing过程中被生成。结点代表运算，箭头代表张量（生成的函数与图都被简化了）。

要想查看生成的函数的源代码，可以调用`tf.autograph.to_code(sum_squares.python_function)`。代码不友好，但是有时对调试有帮助。

## tracing

下面调整`tf_cube`函数，使其打印它的输入：

```python
@tf.function
def tf_cube(x):
    print("print:", x)
    return x ** 3
```

下面调用它：

```python
result = tf_cube(tf.constant(2.0))
```

```python
result
```

`result`看上去没有问题，但是它打印出来`x`是符号张量。它有一个形状与一个数据类型，但是没有值。另外，它有一个名字（`"x:0"`）。这是因为`print`函数不是一个TensorFlow运算，因此它只在Python函数被trace时运行，这发生在图模式中，其中实参被符号张量所替代（同样的类型与形状，但是没有值）。因此`print`函数没有捕获到图中，因此下一次使用float32标量张量调用`tf_cube`时，什么也不会打印：

```python
result = tf_cube(tf.constant(3.0))
result = tf_cube(tf.constant(4.0))
```

但是如果使用一个类型或形状不同的张量，或使用一个新的Python值调用`tf_cube`函数时，函数会再次被trace，因此`print`函数将被调用：

```python
result = tf_cube(2)
result = tf_cube(3)
result = tf_cube(tf.constant([[1., 2.]])) # New shape: trace!
result = tf_cube(tf.constant([[3., 4.], [5., 6.]])) # New shape: trace!
result = tf_cube(tf.constant([[7., 8.], [9., 10.], [11., 12.]])) # New shape: trace!
```

如果函数有Python副作用的话（例如，它将一些日志保存到磁盘上），要小心该代码只在函数被trace时运行（即每次使用一个新的输入签名调用TF函数）。

在某些情况下，需要限定TF函数的输入签名。例如，假设已经知道只调用具有$28\times28$像素图像的批的TF函数，但是批的形状大小非常不同。此时可能不希望TensorFlow为每个批大小生成一个不同的具体函数，或指望它去决定何时使用`None`。此时可以像下面这样指定输入签名：

```python
@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])
def shrink(images):
    print("Tracing", images)
    return images[:, ::2, ::2] # drop half the rows and columns
```

这个TF函数将接受任何形状为[*, 28, 28]的float32张量，并且每次会重用相同的具体函数：

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
img_batch_1 = tf.random.uniform(shape=[100, 28, 28])
img_batch_2 = tf.random.uniform(shape=[50, 28, 28])
preprocessed_images = shrink(img_batch_1) # Traces the function.
preprocessed_images = shrink(img_batch_2) # Reuses the same concrete function.
```

但是如果尝试使用一个Python值，或一个具有非期望的数据类型或形状的张量调用该TF函数，则得到一个异常：

```python
img_batch_3 = tf.random.uniform(shape=[2, 2, 2])
try:
    preprocessed_images = shrink(img_batch_3)  # rejects unexpected types or shapes
except ValueError as ex:
    print(ex)
```

## 使用AutoGraph去捕获控制流

如果函数包含一个简单的`for`循环，例如下面的函数通过连续10次加1来将10加到它的输入上：

```python
@tf.function
def add_10(x):
    for i in range(10):
        x += 1
    return x
```

它工作得很好，但是它的图不包含循环，只包含10个加法运算：

```python
add_10(tf.constant(5))
```

```python
add_10.get_concrete_function(tf.constant(5)).graph.get_operations()
```

这么做是有意义的。当函数被trace时，循环运行了10次，因此`x += 1`运算运行了10次，因此它处于图模式中，它在图中记录了这一操作10次。可以认为这个`for`循环为一个“静态”循环，它在创建图时被展开。

如果想要图包含一个“动态”循环（当图被执行时运行的循环），则可以使用`tf.while_loop`运算手动创建一个，但是这不够直观。使用TensorFlow的AutoGraph特征要简单得多。实际上，默认情况下AutoGraph是激活的（如果想要关闭它，可以将`autograph=False`传递给`tf.function`函数）。但是，它只能捕获在`tf.range`而不是`range`函数上的`for`循环，因此：

> one that runs when the graph is executed

- 如果使用`range`函数，则`for`循环将是静态的，这意味着它只在函数被trance时执行。循环将针对每次迭代，被“展开”为一组运算。
- 如果使用`tf.range`函数，循环将是动态的，这意味着它会包含在图本身中（但是不会在trancing过程中运行）。

使用`tf.while_loop`函数的“动态”循环如下：

```python
@tf.function
def add_10(x):
    condition = lambda i, x: tf.less(i, 10)
    body = lambda i, x: (tf.add(i, 1), tf.add(x, 1))
    final_i, final_x = tf.while_loop(condition, body, [tf.constant(0), x])
    return final_x
```

```python
add_10(tf.constant(5))
```

```python
add_10.get_concrete_function(tf.constant(5)).graph.get_operations()
```

下面查看将`add_10`函数中的`range`函数替换为`tf.range`函数后产生的图：

```python
@tf.function
def add_10(x):
    for i in tf.range(10):
        x = x + 1
    return x
```

```python
add_10.get_concrete_function(tf.constant(0)).graph.get_operations()
```

可以看到，该图现在包含一个`While`循环运算，就像调用了`tf.while_loop`函数一样。

## 处理TF函数中的变量与其他资源

在TensorFlow中，变量与其他有状态变量，例如queues或datasets，被称为**资源（resources）**。任何读取或更新一个资源的运算被认为是有状态的，TF函数确保有状态运算按其出现顺序执行（与无状态运算相反，后者可以并行运行，因此它们的执行顺序不定）。另外，当将一个资源作为实参传递给一个TF函数时，它通过引用传递，因此函数可能修改它。例如：

```python
counter = tf.Variable(0)

@tf.function
def increment(counter, c=1):
    return counter.assign_add(c)
```

```python
increment(counter)
increment(counter)
```

如果查看函数定义，可以看到第一个实参被标记为一个资源：

```python
function_def = increment.get_concrete_function(counter).function_def
function_def.signature.input_arg[0]
```

也可以使用定义在函数外的`tf.Variable`，而不用显式将其作为实参传递：

```python
counter = tf.Variable(0)

@tf.function
def increment(c=1):
    return counter.assign_add(c)
```

```python
increment()
increment()
```

```python
function_def = increment.get_concrete_function().function_def
function_def.signature.input_arg[0]
```

TF函数会隐式地将其视为第一个实参，因此它实际上最终会有相同的签名（除了实参的名字）。然而，使用全局变量可能很快变得混乱，因此一般应该在类内包装变量（以及其他资源）。`@tf.function`也适用于方法：

```python
class Counter:
    def __init__(self):
        self.counter = tf.Variable(0)

    @tf.function
    def increment(self, c=1):
        return self.counter.assign_add(c)
```

```python
c = Counter()
c.increment()
c.increment()
```

注意不要对TF变量使用`=`、`+=`、`-=`或其他任何Python赋值运算符，而必须使用`assign`、`assign_add`或`assign_sub`方法。如果尝试使用一个Python赋值运算符，则调用方法时会得到一个异常。

```python
@tf.function
def add_10(x):
    for i in tf.range(10):
        x += 1
    return x

print(tf.autograph.to_code(add_10.python_function))
```

```python
def display_tf_code(func):
    from IPython.display import display, Markdown
    if hasattr(func, "python_function"):
        func = func.python_function
    code = tf.autograph.to_code(func)
    display(Markdown('```python\n{}\n```'.format(code)))
```

```python
display_tf_code(add_10)
```

## 将（或不将）TF函数与tf.keras一起使用

默认情况下，与tf.keras使用的任何自定义的函数、层或模型都会自动转换为TF函数，而不需要做任何事。然而，在某些情况下，可能希望停用自动自动转换。例如，如果自定义代码不能转换为TF函数时，或只想要调试代码时（这在动态图模式下会容易得多）。此时，只要在创建模型或它的任何层时传递`dynamic=True`即可。

```python
# Custom loss function
def my_mse(y_true, y_pred):
    print("Tracing loss my_mse()")
    return tf.reduce_mean(tf.square(y_pred - y_true))
```

```python
# Custom metric function
def my_mae(y_true, y_pred):
    print("Tracing metric my_mae()")
    return tf.reduce_mean(tf.abs(y_pred - y_true))
```

```python
# Custom layer
class MyDense(keras.layers.Layer):
    def __init__(self, units, activation=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, input_shape):
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[1], self.units),
                                      initializer='uniform',
                                      trainable=True)
        self.biases = self.add_weight(name='bias', 
                                      shape=(self.units,),
                                      initializer='zeros',
                                      trainable=True)
        super().build(input_shape)

    def call(self, X):
        print("Tracing MyDense.call()")
        return self.activation(X @ self.kernel + self.biases)
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
# Custom model
class MyModel(keras.models.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = MyDense(30, activation="relu")
        self.hidden2 = MyDense(30, activation="relu")
        self.output_ = MyDense(1)

    def call(self, input):
        print("Tracing MyModel.call()")
        hidden1 = self.hidden1(input)
        hidden2 = self.hidden2(hidden1)
        concat = keras.layers.concatenate([input, hidden2])
        output = self.output_(concat)
        return output

model = MyModel()
```

```python
model.compile(loss=my_mse, optimizer="nadam", metrics=[my_mae])
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = MyModel(dynamic=True)
```

```python
model.compile(loss=my_mse, optimizer="nadam", metrics=[my_mae])
```

```python
model.fit(X_train_scaled[:64], y_train[:64], epochs=1,
          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)
model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = MyModel()
```

```python
model.compile(loss=my_mse, optimizer="nadam", metrics=[my_mae], run_eagerly=True)
```

```python
model.fit(X_train_scaled[:64], y_train[:64], epochs=1,
          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)
model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)
```

如果自定义模型或层总是动态的，则可以使用`dynamic=True`调用基类的构造器。

```python
class MyLayer(keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(dynamic=True, **kwargs)
        [...]
```

还可以在调用`compile`方法时传递`run_eagerly=True`：

```python
model.compile(loss=my_mse, optimizer="nadam", metrics=[my_mae],
              run_eagerly=True)
```

### TF函数规则

大多数时候，将一个执行TensorFlow运算的Python函数转换为一个TF函数很简单：使用`@tf.function`装饰它或让Keras来处理。然而，有一些规则需要遵守：

- 如果调用任何外部库，包括Numpy，甚至是标准库，则该调用只在tracing过程中运行，它不属于图的一部分。实际上，TensorFlow图只能包含TensorFlow结构（TensorFlow constructs）（张量、运算、变量、datasets等等）。因此，要确保使用`tf.reduce_sum`函数而不是`np.sum`函数，`tf.sort`函数而不是内置的`sorted`函数，等等（除非真的想要代码只在tracing过程中运行）。这带来一些额外的影响：
  - 如果定义一个只是返回`np.random.rand()`的TF函数`f(x)`，则只有在函数被trace时才会生成一个随机数，因此`f(tf.constant(2.))`与`f(tf.constant(3.))`会返回不同值。
  - 如果非TensorFlow代码有副作用（例如记录一些东西或更新一个Python计数器），则不要期望这些副作用会在每次调用TF函数时发生，因为它们只在函数被trace时发生。
  - 可以使用`tf.py_function`运算装饰任何Python代码，但是这么做会妨碍性能，因为TensorFlow不能在此代码上执行图优化。这也会降低可移植性，因为图只能在Python可用的平台上运行（并且要保证安装了正确的库）。
- 可以调用其他Python函数或TF函数，但是它们也应该遵循同样的规则，因为TensorFlow将在计算图中捕获它们的操作。注意这些其他函数不需要使用`@tf.function`装饰。
- 如果函数创建了一个TensorFlow变量（或其他任何有状态的TensorFlow对象，例如dataset或queue），则它必须在第一次调用时就这么做，否则将得到一个异常。通常最好在TF函数外创建变量（例如，在自定义层的`build`方法中）。如果想要为该变量赋一个新值，则要确保调用它的`assign`方法，而不是使用`=`运算符。
- Python的源代码应该对TensorFlow可见。如果无法获得源代码（例如，在Python shell中定义函数，它不允许访问源代码，或只将编译后的*.pyc Python文件部署到生产环境，则图生成过程将失败或功能有限。
- TensorFlow只能捕获在一个张量或一个datasets上迭代的`for`循环。因此，确保使用`for i in tf.range(x)`而不是`for i in range(x)`，否则循环在图中不会被捕获。相反，它只在tracing过程中运行（如果`for`训练的目的是构建图，例如创建神经网络中的每层，则这可能正是想要的）。
- 一如往常，为了性能，如果可以的话，最好使用向量化时间而不是循环。

> This has a few additional implications:

> If you define a TF Function f(*x*) that just returns np.random.rand(), a random number will only be generated when the function is traced,

# 训练深度神经网络

## 梯度消失/爆炸问题

在反向传播过程中，随着算法向下推进到较低层，梯度常常越来越小。因此，该梯度下降更新使得较低层的连接权重几乎不变，训练永远不会收敛到好的解。这被称为**梯度消失（vanishing gradients）**问题。在某些情况下，梯度可能越来越大，直到层得到巨大的权重更新，算法发散。这被称为**梯度爆炸（exploding gradients）**问题，它出现在循环神经网络中。更一般地，深度神经网络遭受不稳定梯度的困扰，不同的层的学习速度可能非常不同。

这个不幸的行为很早就被经验观察到，它是深度神经网络在21世纪初几乎被抛弃的原因之一。虽然不清楚是什么原因导致训练DNN时梯度如此不稳定，但是Xavier Glorot与Yoshua Bengio在[2010年的一篇论文](https://homl.info/47)提出一些见解。作者发现了包括当时流行的logistic sigmoid激活函数与最受欢迎的权重初始化技术（即均值为0、标准差为1的正态分布）的结合在内的一些疑点。简言之，它们展示了使用该激活函数与该初始化方案，每层输出的方差远大于该层输入的方差。随着在网络中前进，方差在每层后不断增加，直到激活函数在顶层饱和。由于logistic函数的均值为0.5，而不是0，这使得饱和更严重（双曲正切函数均值为0，因此在深度网络中的表现稍好于logistic函数）。

查看logistic激活函数（如图），可以看到当输入（绝对值）很大时，函数在0或1处饱和，它的导数及其接近0。因此，当反向传播开始时，它几乎没有导数去反向传播通过网络，并且当反向传播从顶层向下传播时，存在的微小梯度被不断稀释，因此低层没有任何梯度。

```python
import numpy as np

def logit(z):
    return 1 / (1 + np.exp(-z))
```

```python
import matplotlib.pyplot as plt

z = np.linspace(-5, 5, 200)

plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [1, 1], 'k--')
plt.plot([0, 0], [-0.2, 1.2], 'k-')
plt.plot([-5, 5], [-3/4, 7/4], 'g--')
plt.plot(z, logit(z), "b-", linewidth=2)
props = dict(facecolor='black', shrink=0.1)
plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha="center")
plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha="center")
plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha="center")
plt.grid(True)
plt.title("Sigmoid activation function", fontsize=14)
plt.axis([-5, 5, -0.2, 1.2])

plt.show()
```

### Glorot与He初始化

Glorot与Bengio在他们的论文中提出了一种显著缓解不稳定梯度问题的方法。他们指出我们需要信号在两个方向上合适地（properly）流动，即当进行预测时的前向方向，与当反向传播梯度时的反向方向。我们不希望信号消失，也不希望它爆炸或饱和。为了让信号合适地流动，作者认为需要让每层输出的方差等于它的输入的方差，并且梯度在反向流过一层前与后具有相等的方差。实际上不能同时保证两点，除非该层输入与神经元的数量相同（这些数量被称为该层的**扇入（fan-in)**与**扇出（fan-out）**，但是Glorot与Bengio提出一个好的折中，它被证明在实践中工作得非常好：每层的连接权重必须按照如下方式随机初始化（当使用logistic激活函数时），其中$fan_{avg}=(fan_{in}+fan_{out})/2$，该初始化策略被称为**Xavier初始化（Xavier initialization）**或**Glorot初始化（Glorot initialization）**（根据该论文的第一作者命名）：

- 正态分布，均值为$0$，方差为$\sigma^2=\frac{1}{fan_{avg}}$。
- 或者：介于$-r$到$r$之间的均匀分布，其中$r=\sqrt{\frac{3}{fan_{avg}}}$。

如果上面的$fan_{avg}$替换为$fan_{in}$，则得到Yann LeCun在20世纪90年代提出的初始化策略。他称它为**LeCun初始化（LeCun initialization）**。Genevieve Orr与Klaus-Robert Müller甚至在它们1998年的书《Neural Networks: Tricks of the Trade》推荐了它。当$fan_{in}=fan_{out}$时，LeCun初始化等价于Glorot初始化。研究人员用了超过十年的时间才意识到该技巧的重要性。使用Glorot初始化可以显著加速训练，它是导致深度学习成功的技巧之一。

> Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book *Neural Networks: Tricks of the Trade* (Springer).

一些论文（例如[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://homl.info/48)）为不同的激活函数提出了类似的策略。这些策略的区别仅在于方差的尺度与（scale）以及使用$fan_{avg}$还是$fan_{in}$（对于均匀分布，只需要计算$r=\sqrt{3\sigma^2}$）。用于ReLU激活函数（以及它的变体，包括ELU激活）的[初始化策略](https://homl.info/48)有时被称为**He初始化（He initialization）**（根据该论文的第一作者命名）。SELU激活函数则应该使用LeCun初始化（最好使用正态分布）。

> These strategies differ only by the scale of the variance and whether they use $fan_{avg}$ or $fan_{in}$, as shown in Table 11-1 (for the uniform distribution, just compute $r=\sqrt{3\sigma^2}$).

| 初始化 | 激活函数                    | $\sigma^2$（正态分布） |
| ------ | --------------------------- | ---------------------- |
| Glorot | 无，tanh，logistic，softmax | $1/fan_{avg}$          |
| He     | ReLU及其变体                | $2/fan_{in}$           |
| LeCun  | SELU                        | $1/fan_{in}$           |

下面列出keras中可用的初始化器（initializer）：

```python
from tensorflow import keras

[name for name in dir(keras.initializers) if not name.startswith("_")]
```

默认情况下，Keras使用具有均匀分布的Glorot初始化。当创建一层时，可以通过设置`kernel_initializer="he_uniform"`或`kernel_initializer="he_normal`将其改变为He初始化：

```python
keras.layers.Dense(10, activation="relu", kernel_initializer="he_normal")
```

如果要使用具有均匀分布的He初始化，但是基于$fan_{avg}$而不是$fan_{in}$，则可以按照如下方式使用`VarianceScaling`初始化器（initializer）：

```python
init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',
                                          distribution='uniform')
keras.layers.Dense(10, activation="relu", kernel_initializer=init)
```

### 非饱和激活函数

Glorot与Bengio在2010年的论文中提出的一个见解是：梯度不稳定问题的部分原因在于激活函数选择不当。在那之前，大部分人认为如果大自然母亲选择在生物神经元中使用大致的sigmoid激活函数，则它一定是一个优秀的选择。但事实证明，在深度学习中，其他激活函数的表现要好得多，尤其是ReLU激活函数，主要是因为它对正值不会饱和（也因为它计算起来很快）。

但是ReLU函数不完美，它遭受被称为**死亡ReLU（dying ReLUs）**问题的困扰：在训练过程中，一些神经元停止输出0意外的任何东西。在某些情况下，一半的网络神经元都死亡，尤其当使用大的学习率的时候。当训练集中所有实例的输入的加权和为负时，该神经元死亡。当这发生时，它只会不断输出0，梯度下降不会影响到它，因为当它的输入为负时，ReLU函数的梯度为0。

为了解决这个问题，可以使用ReLU函数的变体，例如**leaky ReLU**。该函数定位为$LeakyReLU_\alpha(z)=\max(\alpha z,z)$（如下列代码所示）。超参数$\alpha$定义了函数“泄露（leaks）”的程度：它是$z<0$时函数的斜率，通常设为0.01。这个小斜率保证了leaky ReLU永远不会死亡，它们可能陷入长期昏迷（coma），但是有机会最终醒来。[2015年的一篇论文](https://homl.info/49)比较了ReLU激活函数的一些变体，它的结论之一是：leaky变体的表现总是比严格的ReLU激活函数好。事实上，设置$\alpha=0.2$（一个大的泄露）比设置$\alpha=0.01$（一个小的泄露）能产生更好的表现。这篇论文还评估了**随机化leaky ReLU（randomized leaky ReLU，RReLU）**，其中$\alpha$在训练过程中在给定范围内被随机选取，且在测试过程中被固定为一个平均值。RReLU同样表现得相当好，并且看起来充当一个正则化器（regularizer）（减少过拟合训练集的风险）。最后，该论文评估了**参数ReLU（parametric leaky ReLU，PReLU）**，其中$\alpha$在训练过程中被学习（它成为一个参数，可以像任何其他参数一样被反向传播修改，而不是一个超参数）。据报告，PReLU在大型图像数据集上的表现明显优于ReLU，但是在较小的数据集上，它有过拟合训练集的风险。

```python
def leaky_relu(z, alpha=0.01):
    return np.maximum(alpha*z, z)
```

```python
plt.plot(z, leaky_relu(z, 0.05), "b-", linewidth=2)
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([0, 0], [-0.5, 4.2], 'k-')
plt.grid(True)
props = dict(facecolor='black', shrink=0.1)
plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha="center")
plt.title("Leaky ReLU activation function", fontsize=14)
plt.axis([-5, 5, -0.5, 4.2])

plt.show()
```

下面列出keras中可用的激活函数以及ReLU及其变体：

```python
[m for m in dir(keras.activations) if not m.startswith("_")]
```

```python
[m for m in dir(keras.layers) if "relu" in m.lower()]
```

为了使用leaky ReLU激活函数，需要创建一个`LeakyReLU`层并将其加到模型上恰好位于想要应用的层后。

下面使用Leaky ReLU在Fashion MNIST上训练一个神经网络：

```python
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full / 255.0
X_test = X_test / 255.0
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
```

```python
import tensorflow as tf

tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, kernel_initializer="he_normal"),
    keras.layers.LeakyReLU(),
    keras.layers.Dense(100, kernel_initializer="he_normal"),
    keras.layers.LeakyReLU(),
    keras.layers.Dense(10, activation="softmax")
])
```

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(learning_rate=1e-3),
              metrics=["accuracy"])
```

```python
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
```

下面试试PReLU（将`LeakyReLU(alpha=...)`替换为`PReLU`）：

```python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, kernel_initializer="he_normal"),
    keras.layers.PReLU(),
    keras.layers.Dense(100, kernel_initializer="he_normal"),
    keras.layers.PReLU(),
    keras.layers.Dense(10, activation="softmax")
])
```

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(learning_rate=1e-3),
              metrics=["accuracy"])
```

```python
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
```

Keras目前未提供RReLU的官方实现，但是自己实现它相当容易。

Djork-Arné Clevert等人在[2015年的一篇论文中](https://homl.info/50)提出了一种新的激活函数，被称为**指数线性单元（exponential linear unit，ELU）**，在作者的实验中，它的表现超过所有ReLU变体：训练时间减少，网络在测试集上的表现更好。ELU定义如下：
$$
ELU_\alpha(z)=\left\{
\begin{aligned}
\alpha(\exp(z)-1)\ if\ z<0\\
z\ if\ z>0
\end{aligned}
\right.
$$

下面是它的图形化表示：


```python
def elu(z, alpha=1):
    return np.where(z < 0, alpha * (np.exp(z) - 1), z)
```

```python
plt.plot(z, elu(z), "b-", linewidth=2)
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [-1, -1], 'k--')
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.title(r"ELU activation function ($\alpha=1$)", fontsize=14)
plt.axis([-5, 5, -2.2, 3.2])

plt.show()
```

ELU激活函数看起来很像ReLU函数，有一些主要区别：

- 当$z<0$时，它取赋值，这使得单元的平均输出接近0，有助于缓解梯度消失问题。超参数$\alpha$定义了当$z$为一个大的负数时，ELU函数接近的值。它通常设置为1，但是可以像任何其他超参数一样微调它。
- 当$z<0$时，它的梯度非零，这避免了死亡神经元问题。
- 如果$\alpha=1$，则函数处处平滑，包括在$z=0$附近，这有助于加速梯度下降，因为它不会在$z=0$左右跳跃太多。

> which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem.

> which helps speed up Gradient Descent since it does not bounce as much to the left and right of *z* = 0.

ELU激活函数的主要缺点是，它比ReLU函数及其变体计算起来更慢（因为使用了指数函数）。它在训练过程中更快的收敛速度弥补了计算上的缓慢，但是在测试时，ELU网络将比ReLU网络慢。

在TensorFlow中，实现ELU很简单，只需要在构建每层时指定该激活函数：

```
keras.layers.Dense(10, activation="elu")
```

Günter Klambauer等人在[2017年的一篇论文中](https://homl.info/selu)引入了**Scaled ELU（SELU）**激活函数。顾名思义，它是ELU激活函数的一个缩放变体。作者展示了如果构建一个只由密集层堆叠而成的神经网络，并且所有的隐藏层使用SELU激活函数，则该网络将**自归一化（self-normalize）**：在训练过程中，每层的输出将倾向于保持0的均值与1的标准差，这解决了梯度消失/爆炸问题。因此，对于这样的神经网络（尤其是深的），SELU激活函数的表现常常显著优于其他激活函数。但是，自归一化（self-normalization）有一些条件：

- 输入特征必须被标准化（standardized）（均值为0，标准差为1）。
- 每个隐藏层的权重使用LeCun正态初始化。在Keras中，这意味着设置` kernel_initializer="lecun_normal"`。
- 网络结构必须是顺序的（sequential）。不幸的是，如果在非顺序结构（例如循环（recurrent）网络）中或有跳跃连接的网络（例如Wide & Deep网络）中使用SELU，自归一化不保证发生，因此SELU的表现未必优于其他激活函数。
- 该论文只保证当所有层都是密集层时才会发生自归一化，但是一些研究人员注意到SELU激活函数同样能改善卷积神经网络的表现。

下面是SELU的图形化表示：

```python
from scipy.special import erfc

# alpha and scale to self normalize with mean 0 and standard deviation 1
# (see equation 14 in the paper):
alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)
scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)
```

```python
def selu(z, scale=scale_0_1, alpha=alpha_0_1):
    return scale * elu(z, alpha)
```

```python
plt.plot(z, selu(z), "b-", linewidth=2)
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [-1.758, -1.758], 'k--')
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.title("SELU activation function", fontsize=14)
plt.axis([-5, 5, -2.2, 3.2])

plt.show()
```

下面验证SELU可以保证自归一化：

```python
np.random.seed(42)
Z = np.random.normal(size=(500, 100)) # standardized inputs
for layer in range(1000):
    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization
    Z = selu(np.dot(Z, W))
    means = np.mean(Z, axis=0).mean()
    stds = np.std(Z, axis=0).mean()
    if layer % 100 == 0:
        print("Layer {}: mean {:.2f}, std deviation {:.2f}".format(layer, means, stds))
```

使用SELU非常简单，只需要在创建层时，设置`activation="selu" `与`kernel_initializer="lecun_normal"`：

```python
keras.layers.Dense(10, activation="selu",
                   kernel_initializer="lecun_normal")
```

下面使用SELU激活函数，为Fashion MNIST创建一个有100个隐藏层的网络：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="selu",
                             kernel_initializer="lecun_normal"))
for layer in range(99):
    model.add(keras.layers.Dense(100, activation="selu",
                                 kernel_initializer="lecun_normal"))
model.add(keras.layers.Dense(10, activation="softmax"))
```

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(learning_rate=1e-3),
              metrics=["accuracy"])
```

```python
pixel_means = X_train.mean(axis=0, keepdims=True)
pixel_stds = X_train.std(axis=0, keepdims=True)
X_train_scaled = (X_train - pixel_means) / pixel_stds
X_valid_scaled = (X_valid - pixel_means) / pixel_stds
X_test_scaled = (X_test - pixel_means) / pixel_stds
```

```python
history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))
```

下面查看如果尝试使用ReLU会发生什么：

```python
np.random.seed(42)
tf.random.set_seed(42)
```

```python
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="relu", kernel_initializer="he_normal"))
for layer in range(99):
    model.add(keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"))
model.add(keras.layers.Dense(10, activation="softmax"))
```

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(learning_rate=1e-3),
              metrics=["accuracy"])
```

```python
history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))
```

可以看到效果一点也不好，我们遭受梯度消失/爆炸问题的困扰。

下面是为深度神经网络的隐藏层选择激活函数的一些建议：

- 通常SELU>ELU>leaky ReLU（以及它的变体）>ReLU>tanh>logistic。
- 如果网络结构阻止其自归一化，则ELU的表现可能优于SELU（因为SELU在$z=0$处不平滑）。
- 如果对运行时延迟十分敏感，则可以优先考虑leaky ReLU。如果不想微调超参数，则可以使用Keras使用的默认值$\alpha$（例如，对于leaky ReLU为0.3）。
- 如果有空闲的时间与算力，则可以使用交叉验证去评估其他激活函数，例如RReLU（如果网络过拟合的话）或PReLU（如果训练集过大的话）。
- 尽管如此，因为ReLU是（目前）最常用的激活函数，许多库与硬件加速器提供了特定于ReLU的优化，因此优先考虑速度，则ReLU可能是最佳选择。

### 批规范化

虽然使用He初始化并配合ELU（或其他任何ReLU变体）可以在训练开始显著降低梯度消失/爆炸问题的危险，但是它不能保证该问题在训练过程中不会回来。

Sergey Ioffe与Christian Szegedy在[2015年的一篇论文](https://homl.info/51)中提出一个被称为**批规范化（Batch Normalization，BN）**的技术来解决这些问题。该技术包括在模型中每个隐藏层激活函数前或后添加一个操作。这个操作只是将每个输入零均值化（zero-centers）并规范化，然后对每层使用两个新的参数向量将结果放缩（scale）并平移（shift）：一个用于放缩，一个用于平移。换言之，该操作让模型学习每个层的输入的最优尺度与均值。在很多情况下，如果将BN层添加为神经网络的第一层，则不需要标准化（standardize）训练集（例如，使用`StandardScaler`），BN层将做到这一点（只是近似，因为它一次只查看一批，并且它还会重缩放与平移每个输入特征）。

为了零均值化并规范化输入，算法需要估计每个输入的均值与标准差，它通过评估当前小批的输入的均值与标准差做大这点（因此被称为“批规范化”）。整个操作如下：
$$
\pmb{\mu}_B=\frac{1}{m_B}\sum_{i=1}^{m_B}\pmb{x}^{(i)}
$$

$$
\pmb{\sigma}_B^2=\frac{1}{m_B}\sum_{i=1}^{m_B}(\pmb{x}^{(i)}-\pmb{\mu}_B)^2
$$

$$
\widehat{\pmb{x}}^{(i)}=\frac{\pmb{x}^{(i)}-\pmb{\mu}_B}{\sqrt{\pmb{\sigma}_B^2+\epsilon}}
$$

$$
\pmb{z}^{(i)}=\pmb{\gamma}\otimes\widehat{\pmb{x}}^{(i)}+\pmb{\beta}
$$

其中：

- $\pmb{\mu}_B$为输入均值向量，通过在整个小批$B$上评估得到（每个输入一个均值）。
- $\pmb{\sigma}_B$为输入标准差向量，同样通过在整个小批上评估得到（每个输入一个标准差）。
- $m_B$为小批的实例数量。
- $\widehat{\pmb{x}}^{(i)}$为实例$i$的零均值化与标准化的输入的向量。
- $\pmb{\gamma}$为每层的输出放缩参数向量（每个输入一个放缩参数）。
- $\otimes$代表逐元素乘法（element-wise multiplication）（每个输入与其对应的输出放缩参数相乘）。
- $\pmb{\beta}$为该层的输出平移（shift）（偏移（offset））参数向量（每个输入一个偏移参数）。每个输入偏移与其对应的平移参数。
- $\epsilon$为一个很小的值，用于避免被零除（通常为$10^{-5}$），这被称为**平滑项（smoothing term）**。
- $\pmb{z}^{(i)}$为BN操作的输入。它是输入的重缩放与平移后的版本。

所以在训练过程中，BN标准化它的输入，然后对它们重缩放并偏移。但是在测试期间，可能需要为单个实例而不是实例批作出预测。在这种情况下，没有办法去计算每个输入的均值与标准差。另外，即使有实例批，它可能太小，或实例可能不是独立的与均匀分布的，因此计算该批实例的统计数据是不可靠的。一种方法是等到训练结束后，在整个神经网络上运行训练集，计算BN层每个输入的均值与标准差，当进行预测时，可以使用这个“最终的”输入均值与标准差而不是批输入均值与标准差。但是大多数批规范化的实现通过使用层的输入均值与标准差的移动平均来估计这些训练过程中的最终统计数据。这是当使用`BatchNormalization`层时，Keras自动执行的操作。总而言之，每个批规范化（batch-normalized）层中有4个要学习的参数向量：$\pmb{\gamma}$（输出放缩向量）与$\pmb{\beta}$（输出偏移向量）通过常规的反向传播学习，$\mu$（最终的输入均值向量）与$\pmb{\sigma}$（最终的输入标准差向量）通过使用指数移动平均（exponential moving average）估计得到。注意，$\pmb{\mu}$与$\pmb{\sigma}$在训练过程中被估计，但是在训练后被使用（去代替上式中的批输入均值与标准差）。

Ioffe与Szegedy证明了批规范化显著改善了他们实验的所有深度神经网络，并在ImageNet分类任务（ImageNet为一个大型图像数据集，其中图像被分类为许多类别，它通常用于评估计算机视觉系统）中取得了巨大改善。梯度消失问题大幅减少，以至于他们可以使用饱和激活函数，例如tanh甚至是logistic激活函数。同时，网络对权重初始化的敏感度大大减小。作者可以大得多的学习率，显著加速训练过程。

最后，批规范化充当一个正则化器，它降低了对其他正则化技术（例如dropout）的需求。

然而，批规范化给模型增加了一些复杂性（虽然可以不用对输入数据标准化，如前所述）。另外它带来一个运行时惩罚：由于每层所需的额外的计算量，神经网络的预测速度更慢。幸运的是，常常可以在训练后将BN层与前一层融合，从而避免运行时惩罚。这是通过更新前一层的权重与偏置做到的，以便它直接产生合适尺度与偏移的输出。例如，如果前一层计算$\pmb{X}\pmb{W}+\pmb{b}$，则BN层将计算$\pmb{\gamma}\otimes(\pmb{X}\pmb{W}+\pmb{b}-\pmb{\mu})/\pmb{\sigma}+\pmb{\beta}$（忽略分母中的平滑项$\epsilon$）。如果定义$\pmb{W}'=\pmb{\gamma}\otimes\pmb{W}/\pmb{\sigma}$、$\pmb{b}'=\pmb{\gamma}\otimes(\pmb{b}-\pmb{mu})/\pmb{\sigma}+\pmb{\beta}$，则该公式简化为$\pmb{X}\pmb{W}'+\pmb{b}'$。因此如果将前一层的权重与偏置（$\pmb{W}$与$\pmb{b}$）替换为更新后的权重与偏置（$\pmb{W}'$与$\pmb{b}'$），则可以去除BN层（TFLite的优化器自动执行此操作）。

模型加上BN层后，训练会相当慢，因为当使用批规范化时每代需要更多时间。但是这通常被BN的快速收敛所抵消，因此它需要更少的代数达到相同的性能。总的来说，总时间通常更短。

使用Keras实现批规范化简单直观：只需要在模型的每个隐藏层的激活函数前或后添加一个`BatchNormalization`层，并可选地添加一个BN层以及第一层。以下模型在每个隐藏层后应用BN，并将其作为模型的第一层（在展平输入图像后）：

```
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
```

该例只有两个隐藏层，批规范化产生非常积极的影响，但是对于更深的网络，它可以带来巨大的不同。

下面展示模型的摘要：

```python
model.summary()
```

每个BN层为每个输入添加4个参数：$\pmb{\gamma}$、$\pmb{\beta}$、$\pmb{\mu}$与$\pmb{\sigma}$。最后两个参数：$\pmb{\mu}$与$\pmb{\sigma}$为移动平均，它们不受反向传播的影响，因此Keras称它们“不可训练（non-trainable）”（然而它们在训练过程中基于训练数据被估计，因此按理来说它们是可训练的。在Keras中，“non-trainable”实际上意味着“不受反向传播影响”）。如果计算BN参数的总数并除以2：$(3136+1200+400)/2=2368$，结果正是模型中不可训练的参数的总数。

下面查看第一个BN层的参数，两个可（被反向传播）训练，两个不可：

```python
bn1 = model.layers[1]
[(var.name, var.trainable) for var in bn1.variables]
```

下面训练模型：

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(learning_rate=1e-3),
              metrics=["accuracy"])
```

```python
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
```

BN论文的作者主张在激活函数前而不是后添加BN层。对此有一些争论，因为哪一个更好似乎取决于任务。可以对此进行实验，查看哪个选项在数据集上工作得最好。要在激活函数前添加BN层，必须从隐藏层中移除激活函数，并将它们作为单独的层添加到BN层后。另外，因为批规范化为每个输入包含一个偏移参数，可以从前一层中移除偏置项（只需要在创建它时传递`use_bias=False`）：

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, use_bias=False),
    keras.layers.BatchNormalization(),
    keras.layers.Activation("relu"),
    keras.layers.Dense(100, use_bias=False),
    keras.layers.BatchNormalization(),
    keras.layers.Activation("relu"),
    keras.layers.Dense(10, activation="softmax")
])
```

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(learning_rate=1e-3),
              metrics=["accuracy"])
```

```python
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
```

`BatchNormalization`类有不少的超参数可以微调。默认值通常很好，但是偶尔需要微调`momentum`。`BatchNormalization`在更新指数移动平均时使用它。给定一个新值$\pmb{v}$（即在当前批上计算的输入均值或标准差新向量），该层使用下式更新该移动平均（running average）：

$$
\widehat{\pmb{v}}\leftarrow\widehat{\pmb{v}}\times momentum+\pmb{v}\times(1-momentum)
$$

良好的momentum值通常接近1，例如0.9、0.99或0.999（对于较大的数据集与较小的小批，需要更多的9）。

另一个重要的超参数为`axis`：它决定哪个轴应被规范化。它默认为-1，意味着默认情况下它将规范化最后一个轴（使用其他轴上计算的均值与标准差）。当输入批为2维（即批形状为$[batch\ size,features]$），这意味着每个输入特征将基于在批中所有实例上计算的均值与标准差进行规范化。例如，前面代码示例中的第一个BN层将独立正则化（且重缩放与平移）784个输入特征中的每个。如果将第一个BN层移到`Flatten`前，则输入批将会是3维，形状为$[batch\ size,height, width]$。因此，BN层将计算28个均值与28个标准差（每列像素一个，在批中所有实例与列中所有行上计算得到），并且它将使用同样的均值与标准差规范化给定列的所有像素。还有28个缩放参数与28个平移参数。如果想要独立地对待784个像素中的每个，则应该设置`axis=[1, 2]`。

> 1 per column of pixels, computed across all instances in the batch and across all rows in the column

注意BN层不会在训练期间与训练后执行相同计算：它使用训练期间的批统计数据与训练后的“最终”统计数据（即移动平均的最终值）。下面窥探该类的源码以查看这是如何处理的：

```python
class BatchNormalization(keras.layers.Layer):
    [...]
    def call(self, inputs, training=None):
        [...]
```

`call`方法是执行该计算的方法。它有一个额外的`training`实参，默认设置为`None`，但是在训练期间，`fit`方法将其设置为1。如果需要编写一个自定义层，且它必须在训练与测试期间表现不同，则像`call`方法添加一个`training`实参，并在该方法中使用该实参以决定计算什么。

`BatchNormalization`已经成为深度神经网络中最常用的层之一，以至于它常常在图表中被忽略，因为假定每层后都添加BN。但是Hongyi Zhang等人的[论文](https://homl.info/fixup)可能改变这个假定：通过使用一个新的**fixed-update（fixup）**权重初始化技术，在不使用BN的情况下作者成功地训练了一个非常深的神经网络（10000层），在复杂图像分类任务上实现了最先进的性能。由于这是最前沿的研究，在丢弃批规范化前，需要其他研究来确认该发现的有效性。

### 梯度裁剪

另一个缓解梯度爆炸问题的流行技术是在反向传播过程中裁剪梯度以便它们永远不会超过某个阈值。这被称为**[梯度裁剪（Gradient Clipping）](https://arxiv.org/abs/1211.5063)**。该技术最常用于循环神经网络，因为批规范化很难在RNN中使用。对于其他类型的网络，BN通常足够。

在Keras中，实现梯度裁剪只需要在创建一个优化器时设置`clipvalue`或`clipnorm`实参：

```python
optimizer = keras.optimizers.SGD(clipvalue=1.0)
model.compile(loss="mse", optimizer=optimizer)
```

该优化器将把梯度向量的每个分量裁剪为一个介于$-1.0$\~$1.0$之间的一个值。这意味着损失（相对于每个可训练参数）的偏导将被裁剪为$-1.0$到$1.0$之间。该阈值是一个可以调整的超参数。注意，这可能改变梯度向量的方向。例如，如果原始梯度向量为$[0.9,100.0]$，它主要指向第二个轴的方向，但是一旦按值裁剪它，则得到$[0.9,1.0]$，它大致指向两个轴之间的对角线。在实践中，该方法工作得很好。如果要保证梯度裁剪不改变梯度向量的方向，则应该通过设置`clipnorm`而不是`clipvalue`，按范数（norm）来裁剪：

```python
optimizer = keras.optimizers.SGD(clipnorm=1.0)
```

如果整个梯度的$\ell_2$范数大于选定的阈值，则它将裁剪整个梯度。例如，该优化器设置`clipnorm=1.0=1.0`，则向量$[0.9,100.0]$将被裁剪为$[0.00899964, 0.9999595]$，这保留它的方向但是几乎消除了第一个分量。如果在训练过程中观察到梯度爆炸（可以使用TensorBoard跟踪梯度的大小），则可以使用尝试不同的阈值按值裁剪与按范数裁剪，并查看哪个选项在验证集上表现得最好。

> This will clip the whole gradient if its ℓ2 norm is greater than the threshold you picked.

## 重用预训练的层

通常，从头训练一个非常大的DNN不是一个好主意。相反应该始终尝试寻找现有的神经网络，它可以完成与要处理的任务相似的任务，然后重用该网络的低层。该技术被称为**迁移学习（transfer learning）**。它不仅能显著加速训练，而且需要的训练数据也会显著减少。

> you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle

假设现在可以访问一个DNN，该DNN已被训练将图片分类为100个不同范畴（categories），包括动物、植物、交通工具与日常对象等。现在想要训练一个DNN对特定类型的交通工具进行分类。这两个任务非常相似，甚至部分重叠，因此应该尝试重用第一个网络的一部分（如图）。

![重用训练过的层](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\重用训练过的层.png)

如果新任务的输入尺寸与原始任务的输入尺寸不等，则通常必须添加一个预处理步骤以将其调整为原始模型预期的大小。更一般地，当输入的低层特征相似时，迁移学习工作得最好。

原始模型的输出通常应该被替换，因为它很可能对新任务毫无用处，它甚至可能没有对新任务的正确数量的输出。

类似地，原始模型的高隐藏层不太可能与低层一样有用，因为对新任务最有用高层特征的可能显著不同于对原始任务最有用的特征。

任务越相似，则重用的层越多（从低层开始）。对于非常相似的任务，尝试保留所有隐藏层，只替换输出层。

首先尝试冻结（freeze）所有重用层（即令它们的权重不可训练以使得梯度下降不会改变它们），然后训练模型，查看它的表现。然后尝试解冻（unfreeze）一个或两个顶部隐藏层，以使得反向传播微调它们并查看表现是否改善。训练数据越多，可解冻的层越多。当解冻重用层时，降低学习率也很有用，这会防止破坏这些层的微调过的权重。

如果表现还是不好，并且训练数据很少，则尝试丢弃顶部隐藏层（layer(s)）并再次冻结其他隐藏层。可以如此迭代下去直到找到正确数量的层以供重用。如果训练数据很多，则可以尝试替换顶部隐藏层而不是丢弃它们，甚至添加更多的隐藏层。

下面展示一个例子。假设Fashion MNIST数据集只包含8个类别，例如它包含除sandal与shirt外的所有类别。某人在该数据集上构建并训练了一个Keras模型，且模型表现很好（准确率$>90%$）。我们称该模型为A。现在要处理一个不同的任务：你有sandal与shirt的图像，且想要训练一个二分类器（正类表示shirt，反类表示sandal）。该数据集相当小，只有200个有标签图像。当为该任务训练一个新的模型（我们称其为B），且它的结构与模型A相同，它的表现相当好（准确率为$97.2%$）。

```python
def split_dataset(X, y):
    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts
    y_A = y[~y_5_or_6]
    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7
    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?
    return ((X[~y_5_or_6], y_A),
            (X[y_5_or_6], y_B))

(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)
(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)
(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)
X_train_B = X_train_B[:200]
y_train_B = y_train_B[:200]
```

```python
X_train_A.shape
```

```python
X_train_B.shape
```

```python
y_train_A[:30]
```

```python
y_train_B[:30]
```

```python
tf.random.set_seed(42)
np.random.seed(42)
```

```python
model_A = keras.models.Sequential()
model_A.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_hidden in (300, 100, 50, 50, 50):
    model_A.add(keras.layers.Dense(n_hidden, activation="selu"))
model_A.add(keras.layers.Dense(8, activation="softmax"))
```

```python
model_A.compile(loss="sparse_categorical_crossentropy",
                optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                metrics=["accuracy"])
```

```python
history = model_A.fit(X_train_A, y_train_A, epochs=20,
                    validation_data=(X_valid_A, y_valid_A))
```

```python
model_B = keras.models.Sequential()
model_B.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_hidden in (300, 100, 50, 50, 50):
    model_B.add(keras.layers.Dense(n_hidden, activation="selu"))
model_B.add(keras.layers.Dense(1, activation="sigmoid"))
```

```python
model_B.compile(loss="binary_crossentropy",
                optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                metrics=["accuracy"])
```

```python
history = model_B.fit(X_train_B, y_train_B, epochs=20,
                      validation_data=(X_valid_B, y_valid_B))
```

```python
model_B.summary()
```

但是因为它是一个简单得多的任务（只有两个类别），你希望它的表现更好。下面查看迁移学习能否有所帮助。

首先需要加载模型A并基于该模型的层创建一个新的模型。下面重用除了输出层外的所有层：

```python
model_A = keras.models.load_model("my_model_A.h5")
model_B_on_A = keras.models.Sequential(model_A.layers[:-1])
model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid"))
```

注意`model_A`与`model_B_on_A`现在共享一些层。当训练`model_B_on_A`，它同时会影响`model_A`。如果要避免这点，需要在重用`model_A`的层前*克隆（clone）*`model_A`。为此，使用`clone_model`方法克隆模型A的结构，然后复制它的权重（因为`clone_model`方法不会克隆权重）：

```python
model_A_clone = keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())
```

现在可以为任务B训练`model_B_on_A`，但是因为新的输出层被随机初始化，因此它将犯大错误（至少在最初几代），因此将有大的误差梯度，它可能会破坏重用权重。为了避免这点，一种方法是在最初几代冻结重用层，给新层一些时间去学习合理的权重。为此，设置每层的`trainable`属性为`False`并编译模型：

```python
for layer in model_B_on_A.layers[:-1]:
    layer.trainable = False

model_B_on_A.compile(loss="binary_crossentropy",
                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                     metrics=["accuracy"])
```

注意，每次冻结或解冻模型后必须编译模型。

现在，可以训练模型几代了，然后解冻重用层（需要再次编译模型），并继续训练，为任务B微调重用层。解冻重用层后，降低学习率通常是一个好主意，同样是为了避免破坏重用权重。

```python
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
                           validation_data=(X_valid_B, y_valid_B))

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = True

model_B_on_A.compile(loss="binary_crossentropy",
                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                     metrics=["accuracy"])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
                           validation_data=(X_valid_B, y_valid_B))
```

模型的测试准确率为99.25%，意味着迁移学习将错误率从2.8%降到接近0.7%。

```python
model_B.evaluate(X_test_B, y_test_B)
```

```python
model_B_on_A.evaluate(X_test_B, y_test_B)
```

当然，这是在尝试许多配置直到找到一个显示显著改进的配置后得到的结果。如果尝试去改变类别或随机种子，则将发现改善程度通常会下降，甚至消失或相反。

之所以要这样，是因为事实证明迁移学习在小密集网络中的作用不太大，可能是因为小网络学习很少的模式，并且密集网络学习非常特定的模式，这在其他任务中不太可能有用。迁移学习最适用于深度卷积神经网络，该网络倾向于学习更通用的特征检测器（尤其在低层中）。

## 无监督预训练

假定要处理一个复杂任务，但是没有很多有标签训练数据，并且很不幸，也无法找到在相似任务上训练过的模型。则首先应该尝试收集更多的有标签训练数据，但是如果不能的话，则可以执行**无监督预训练（unsupervised pretraining）**（如图）。实际上，收集无标签示例常常很便宜，但是为它们打标签代价很大。如果可以收集大量的无标签训练数据，则可以尝试使用它们去训练一个无监督模型，例如自编码器或生成对抗网络。然后重用自编码器的低层或GAN的判别器（discriminator）的低层，为任务在顶部添加输出层，然后使用监督学习微调最终的网络（即使用有标签训练示例）。

正是Geoffrey Hinton与他的团队在2006使用的这项技术导致了神经网络的复兴（revival）与深度学习的成功。直到2010年，无监督预训练——通常使用受限Boltzmann及其（RBMs）是深度网络的标准，只有当梯度消失问题缓解后，只使用监督学习训练DNN才常见得多。当有复杂任务需要解决，没有可重用的相似模型，并且有标签数据很少而无标签数据很多时，无监督预训练（如今通常使用自编码器或GANs而不是RBMs）仍然是好的选择。

注意在深度学习的早期很难训练深度模型，因此人们使用一个被称为**贪婪逐层预训练（greedy layer-wise pretraining）**（如图所示）的技术。它们首先使用一个层训练一个无监督模型，通常是RBM，然后它们冻结该层并在其顶部添加另一个层，然后再次训练该模型，然后冻结这个新层并在其顶部添加另一个层，再次训练该模型，如此下去。如今，事情要简单得多：人们通常一次性训练整个无监督模型（即，在图中，直接从第三步开始）并使用自编码器或GANs而不是RBMs。

![在无监督训练中，使用无监督学习技术在无标签数据（或整个数据）上训练模型，然后使用无监督学习技术在无标签数据上为最终任务微调它，无监督部分可能一次训练一层，如图所示，也可能直接训练整个模型。](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\无监督预训练.png)

## 在辅助任务上预训练

如果没有很多有标签数据，最后一个选择是在辅助任务上训练第一个神经网络，该辅助任务的有标签数据很容易获得或生成，然后重用该网络的低层用于实际任务。第一个神经网络的低层将学习特征检测器，它可能被第二个神经网络重用。



例如，如果想要构建一个系统去识别人脸，且每个人的照片可能只有几张，显然这不够训练一个好的分类器。为每个人收集数百张图片可能不现实。此时可以在web上收集很多人的随机照片，然后训练第一个神经网络去检测两张不同的照片是否是同一人。这样的网络将为人脸学到好的特征检测器，所以重用它的低层将允许使用很少的训练数据去训练一个好的人脸分类器。

在**自然语言处理（natural language processing，NLP）**应用中，可以下载包含数百万个文本文档的语料库，并从中自动生成有标签数据。例如，可以随机屏蔽一些单词并训练一个模型去预测缺失的单词是什么（例如，它应该预测句子“What ___ you saying?”中的缺失单词可能为“are”或“were”）。如果如果可以训练一个模型在该任务上取得好的表现，则它已经对语言有相当多的了解，当然可以重用它用于实际任务并在有标签数据上微调它。

上面是**自我监督学习（self-supervised learning）**的一个例子。它指的是自动从数据中生成标签，这样就可以使用监督学习技术在最终的“有标签”数据集上训练一个新的模型。因为该方法不需要人工标记，最好将它分类为一种无监督学习形式。

## 快速优化器

训练一个非常大的深度神经网络可能非常慢。目前讨论了四种方法可以加速训练：为权重应用好的初始化策略、使用好的激活函数、使用批规范化与重用预训练模型的部分（可能构建于一个辅助任务上或使用无监督学习）。另一个巨大的速度提升来自使用比常规的梯度下降优化器更快的优化器。

### 动量优化

想象一个保龄球在光滑表面上沿着缓坡滚下：它首先很慢，然后会迅速加速直到到达最终速度（如果存在摩擦或空气阻力的话）。这是**动量优化（momentum optimization）**背后的非常简单的思想，它[由Boris Polyak发表于1964年](https://homl.info/54)。相反，常规的梯度下降只会沿着斜坡走一小段有规律的步，因此算法需要花费多得多的时间来到达底部。

梯度下降通过直接减去损失函数$J(\pmb{\theta})$关于权重的梯度$\nabla_{\pmb{\theta}} J(\pmb{\theta})$与学习率$\eta$的乘积来更新权重$\pmb{\theta}$。公式为：$\pmb{\theta}\leftarrow\pmb{\theta}-\eta\nabla_{\pmb{\theta}} J(\pmb{\theta})$。它不关心先前的梯度时什么。如果局部梯度很小，它进展得很慢。

动量优化非常关心先前的梯度：每次迭代，它从**动量向量（momentum vector）**$\pmb{m}$（乘以学习率$\eta$）中减去局部梯度，并通过加上该动量向量来更新权重（公式如下）。换言之，梯度用于加速，而不是速度。为了模拟某种摩擦机制并避免动量过大，算法引入新的超参数$\beta$，被称为**动量（momentum）**，它必须介于为0（高摩擦）与1（没有摩擦）之间。典型的动量值为0.9。
$$
\begin{aligned}
&1.\ \pmb{m}\leftarrow\beta\pmb{m}-\eta\nabla_{\pmb{\theta}}J(\pmb{\theta})\\
&2.\ \pmb{\theta}\leftarrow\pmb{\theta}+\pmb{m}
\end{aligned}
$$


可以验证如果梯度保持不变，则最终速度（即权重更新的最大大小）等于该梯度乘以学习率$\eta$乘以$1/(1-\beta)$（忽略符号）。例如，如果$\beta=0.9$，则动量优化最终比梯度下降快10倍。这使得相比于梯度下降，动量优化可以以快得多的速度逃离高原。[模型训练](#(模型训练))一章中提到，如果输入有着非常不同的尺度，则损失函数看起来像一个细长的碗（an elongated bowl）（见[图1](#(模型训练)(梯度下降)(1))）。梯度下降沿陡坡下降得非常快，但是然后它要花费非常长的时间去通过山谷。相反，动量优化将以越来越快的速度沿着山谷滚动，直到到达底部（最优）。在不使用批规范化的深度神经网络中，上层常常最终有非常不同的尺度，因此使用动量优化帮助很大。它有助于滚动通过局部最优。

由于动量的存在，优化器可能会稍微过冲（overshoot），然后回来，再次过冲，并在稳定到最小值之前多次像这样振荡。这是系统中要有一点摩擦力的原因之一，它摆脱了这些振荡，从而加速收敛。

在Keras中实现动量优化很简单，只需要使用`SGD`优化器并设置它的`momentum`超参数：

```python
optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
```

动量优化的一个缺点是它增加了另一个需要调整的超参数。但是动量值0.9在实践中通常工作得很好，并且几乎总是快于常规的梯度下降。

### Nesterov加速梯度

[Yurii Nesterov在1983年](https://homl.info/55)提出了动量优化的一个小变体：**Nesterov加速梯度（Nesterov Accelerated Gradient，NAG）**方法，也被称为**Nesterov动量优化（Nesterov momentum optimization）**，它几乎总是比普通的动量优化快。它不是在局部位置$\pmb{\theta}$，而是在动量方向上稍微向前的$\pmb{\theta}+\beta\pmb{m}$处度量损失函数的梯度：
$$
\begin{aligned}
&1.\ \pmb{m}\leftarrow\beta\pmb{m}-\eta\nabla_{\pmb{\theta}}J(\pmb{\theta}+\beta\pmb{m})\\
&2.\ \pmb{\theta}\leftarrow\pmb{\theta}+\pmb{m}
\end{aligned}
$$

这个小的微调之所以能起到作用，是因为通常情况下动量向量将指向正确方向（向着最优），因此使用该方向上稍微远一点的地方，而不是使用原始位置的梯度度量该梯度，会稍微更准确一点，如图所示（其中$\nabla_1$表示在起点$\pmb{\theta}$处度量的损失函数梯度，$\nabla_2$表示在点$\pmb{\theta}+\beta\pmb{m}$处度量的梯度值）。



可见，Nesterov更新最终稍微更接近最优值。一段时间后，这些小的改善积累，NAG最终显著快于常规的动量优化。另外，注意当动量迫使权重通过山谷，$\nabla_1$继续向山谷深度推进，而$\nabla_2$会反向向山谷底部推进。这有助于降低振荡，从而NAG收敛得更快。


![常规的与Nesterov动量优化：前者应用在动量步前计算的梯度，后者应用在动量步后计算的梯度](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\常规的与Nesterov动量优化.png)

NAG通常快于常规的动量优化。要使用它，只需要在创建SGD优化器时设置`nesterov=True`。

```python
optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)
```

### AdaGrad

再次考虑细长的碗问题：梯度下降首先快速沿着最陡的坡下降，它没有直接指向全局最优，然后它非常慢地下降到山谷底部。如果算法能够提前纠正它的方向，使其更接近全局最优就好了。[**AdaGrad**算法](https://homl.info/56)通过缩小沿着最陡维度缩小梯度向量实现了该纠正：


$$
\begin{aligned}
&1.\ \pmb{s}\leftarrow\pmb{s}+\nabla_{\pmb{\theta}}J(\pmb{\theta})\otimes\nabla_{\pmb{\theta}}J(\pmb{\theta})\\
&2.\ \pmb{\theta}\leftarrow\pmb{\theta}-\eta\nabla_{\pmb{\theta}}J(\pmb{\theta})\oslash\sqrt{\pmb{s}+\epsilon}
\end{aligned}
$$

第一步将梯度的平方累加到向量$\pmb{s}$中。这个向量化形式等价于为向量$\pmb{s}$的每个元素$s_i$计算$s_i\leftarrow s_i+(\partial J(\pmb{\theta})/\partial\theta_i)^2$。如果损失函数沿着第$i$个维度很陡，则$s_i$每次迭代会越来越大。


第二步几乎与梯度下降相同，但是有一个很大不同：梯度向量通过因子$\sqrt{\pmb{s}+\epsilon}$缩减（$\oslash$符号表示逐元素相除，$\epsilon$为平滑因子，防止除以0，通常设置为$10^{-10}$）。这个向量化形式等价于同时为所有参数$\theta_i$计算$\theta_i\leftarrow\theta_i-\eta\partial J(\pmb{\theta})/\partial\theta_i/\sqrt{s_i+\epsilon}$。

简言之，该算法会衰减学习速率，但是陡峭维度的衰减速度快于坡度较缓的维度。这被称为**自适应学习率（adaptive learning rate）**。它有助于让得到的更新更直接地指向全局最优（如图）。一个额外的好处是它需要少得多的学习率超参数$\eta$的调整。

![AdaGrad与梯度下降：前者可以更早地纠正其方向以指向最优值](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\AdaGrad与梯度下降.png)

对于简单的二次问题，AdaGrad常常（frequently）表现很好，但是当训练神经网络时，它常常过早停止。学习率缩减地太多以至于算法在到达全局最优值前最终完全停止。因此即使Keras有`Adagrad`优化器，也不应该用它去训练深度神经网络（然而对于简单任务，如线性回归，它可能足够有效）。不过，了解AdaGrad有助于掌握其他自适应学习率优化器。

以下是AdaGrad的Keras实现：

```python
optimizer = keras.optimizers.Adagrad(learning_rate=0.001)
```

### RMSProp

AdaGrad有缩减过快、永远不会收敛到全局最优值的风险。**RMSProp**算法通过只积累最近迭代的梯度（而不是从训练开始的所有梯度）。它通过在第一步使用指数衰减（ exponential decay）来做到这点：

$$
\begin{aligned}
&1.\ \pmb{s}\leftarrow\beta\pmb{s}+(1-\beta)\nabla_{\pmb{\theta}}J(\pmb{\theta})\otimes\nabla_{\pmb{\theta}}J(\pmb{\theta})\\
&2.\ \pmb{\theta}\leftarrow\pmb{\theta}-\eta\nabla_{\pmb{\theta}}\oslash\sqrt{\pmb{s}+\epsilon}
\end{aligned}
$$

衰减率$\beta$通常设置为0.9，它又是一个新的超参数，但是默认值常常工作得很好，所有可能完全不需要微调它。

Keras提供了`RMSprop`优化器：

```python
optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

`rho`实参对应上式中的$\beta$。除了在非常简单的问题上，该优化器的表现几乎总是好于AdaGrad。实际上，在Adam优化出现前，它是许多研究人员的首选优化算法。

### Adam与Nadam优化

[**Adam**](https://homl.info/59)代表**自适应矩估计（adaptive moment estimation）**，它结合了动量优化与RMSProp的思想：与动量优化一样，它记录过去梯度的指数衰减平均（an exponentially decaying average of past gradients）；与RMSProp一样，它记录过去平方梯度的指数衰减平均：
$$
\begin{aligned}
&1.\ \pmb{m}\leftarrow\beta_1\pmb{m}-(1-\beta_1)\nabla_{\pmb{\theta}}J(\pmb{\theta})\\
&2.\ \pmb{s}\leftarrow\beta_2\pmb{s}+(1-\beta_2)\nabla_{\pmb{\theta}}J(\pmb{\theta})\otimes\nabla_{\pmb{\theta}}J(\pmb{\theta})\\
&3.\ \widehat{\pmb{m}}\leftarrow\frac{\pmb{m}}{1-\beta_1^t}\\
&4.\ \widehat{\pmb{s}}\leftarrow\frac{\pmb{s}}{1-\beta_2^t}\\
&5.\ \pmb{\theta}\leftarrow\pmb{\theta}+\eta\widehat{\pmb{m}}\oslash\sqrt{\widehat{\pmb{s}}+\epsilon}
\end{aligned}
$$

这些是对梯度的均值与（非中心，uncentered）方差估计。该均值常常被称为**一阶矩（first moment）**，而方差常常被称为**二阶矩（second moment）**，因此算法得名。

式中的$t$表示迭代次数（从1开始）。

如果只看步1、2与5，可以注意到Adam与动量优化与RMSProp都非常相似。唯一的区别在于步1计算指数衰减平均而不是指数衰减和。但是除了常数因子外，它们实际上是等价的（衰减平均就是$1-\beta_1$乘以衰减和）。步3与步4涉及一些技术细节：因为$\pmb{m}$与$\pmb{s}$被初始化为0，它们在训练开始时会偏向0，因此这两步有助于在训练开始时加速$\pmb{m}$与$\pmb{s}$。

动量衰减超参数$\beta_1$通常设置为0.9，而缩放衰减因子$\beta_2$常常设置为0.999。如前，平滑项$\epsilon$通常初始化为一个很小的值例如$10^{-7}$。这些是`Adam`类的默认值（更精确地说，`epsilon`默认为`None`，它告诉Keras去使用`keras.backend.epsilon()`，它默认为$10^{-7}$，可以使用`keras.backend.set_epsilon()`改变它）。下面展示了如何使用Keras创建一个Adam优化器：

```python
optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
```

因为Adam是自适应学习率算法（就像AdaGrad与RMSProp一样），它需要较少的学习率超参数$\eta$的调整。常常可以使用默认值$\eta=0.001$，这使得Adam甚至比梯度下降更容易使用。

最后，Adam的两个变体值得一提：

**AdaMax**：在上式的步2，Adam将梯度的平方累加为$\pmb{s}$（对更近（recent）的梯度有更大的权重）。在步5，如果忽略$\epsilon$与步3与步4（毕竟它们是技术细节），则Adam按$\pmb{s}$的平方根缩减参数更新。简言之，Adam通过随时间衰减的梯度的$\ell_2$范数缩减参数更新。AdaMax与Adam在同一篇论文中被引入，它将$\ell_2$范数替换为$\ell_\infty$范数。具体来说，它将上式步2替换为$\pmb{s}\leftarrow \max(\beta_2\pmb{s},\nabla_\theta J(\pmb{\theta}))$，它丢弃步4，在步5它通过因子$\pmb{s}$缩减梯度更新，该因子就是随时间衰减的梯度的最大值。在实践中，这使得AdaMax比Adam更稳定，但这实际上取决于数据集，并且通常Adam表现得更好。因此，这只是当在某些任务上使用Adam遇到问题时可以尝试的另一种优化器。

**Nadam**：Nadam优化是Adam优化加上Nesterov技巧，因此它常常比Adam收敛得稍快。在[他的介绍该技术的报告](https://homl.info/nadam)中，研究人员Timothy Dozat在各种任务上比较了许多不同的优化器，并发现的表现通常好于Adam，但是有时表现不如RMSProp。

自适应优化方法（包括RMSProp、Adam与Nadam优化）常常很好，能快速收敛到好的解。然而Ashia C. Wilson等人在[2017年一篇论文](https://homl.info/60)中展示了在某些数据集上，它们可能产生泛化很差的解。因此如果对模型的表现很失望，则尝试使用普通的Nesterov加速梯度，因为数据集可能只是对自适应梯度敏感。还可以看看最新研究，因为该研究进展很快。

目前讨论的所有优化技术依赖于*一阶偏导（first-order partial derivatives，Jacobians）*。优化文献（literature）同样包含基于*二阶偏导（second-order partial derivatives，Hessians）*的算法。不幸的是，这些算法很难用于深度神经网络，因为每个输出有$n^2$个Hessian（$n$为参数数量），而不是每个输出只有$n$个Jacobians。因此DNN通常包含数万个参数，二阶优化算法常常不能放入内存，即使可以，计算Hessians也太慢。

下表总结了目前讨论的所有优化器（“\*”表示不好，“\*\*”表示一般，“\*\*\*”表示好）：

<table>
    <tr>
        <th>类别</th>
        <th>收敛速度</th>
        <th>收敛质量</th>
    </tr>
    <tr>
        <td>SGD</td>
        <td>*</td>
        <td>***</td>
    </tr>
    <tr>
        <td>SGD(momentum=...)</td>
        <td>**</td>
        <td>***</td>
    </tr>
    <tr>
        <td>SGD(momentum=..., nesterov=True)</td>
        <td>**</td>
        <td>***</td>
    </tr>
    <tr>
        <td>Adagrad</td>
        <td>***</td>
        <td>*（过早停止）</td>
    </tr>
    <tr>
        <td>RMSprop</td>
        <td>***</td>
        <td>**或***</td>
    </tr>
    <tr>
        <td>Adam</td>
        <td>***</td>
        <td>**或***</td>
    </tr>
    <tr>
        <td>Nadam</td>
        <td>***</td>
        <td>**或***</td>
    </tr>
    <tr>
        <td>AdaMax</td>
        <td>***</td>
        <td>**或***</td>
    </tr>
</table>

以上展示的所有优化算法都产生密集模型（dense models），这意味着大部分参数都为非零。如果需要一个在运行时极快的模型，或如果需要它占用更少的内存，则可以使用稀疏模型。

为此，一个简单的方法是像往常一样训练模型，然后去除很小的权重（将它们设置为0），注意这通常不会导致一个非常稀疏的模型，并且它可能降低模型性能。

更好的选择是在训练过程中应用强$\ell_1$正则化，这会迫使优化器尽可能多得将权重归零。

如果这些技术仍然不足，则查看[TensorFlow Model Optimization Toolkit（TF-MOT）](https://homl.info/tfmot)，它提供了一个剪枝API（pruning API），它能够根据连接大小迭代地移除连接。

## 学习率调度

找到一个好的学习率非常重要。如果将它设置太大，则训练可能发散；如果将它设置太小，则训练最终会收敛到最优解，但是需要很长时间。如果将它设置得稍微过高，则它一开始进展非常快，但是最终在最优值附近振荡，永远不会真正稳定下来。如果计算预算有限，则可能必须在训练正确收敛前中断它，从而产生次优解（如图）。

![不同学习率下的学习曲线](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\不同学习率下的学习曲线.png)



如前所述，可以通过训练模型几百次，将学习率从一个非常小的值指数增长到一个非常大的值，然后查看学习曲线并选择一个比学习曲线开始反弹时的学习率稍小的学习率来找到好的学习率。然后重新初始化模型并使用该学习率训练它。

但是，如果开始于一个大的学习率然后降低它直到训练不再取得快速进展，则可以比使用最优的恒定学习率更快地到达好的解。有许多不同策略可以在训练期间降低学习率。也可以开始于一个低学习率，然后增加它，然后再次降低它。这些策略被称为**学习率调度（learning schedules）**。下面是最常用的学习率调度：

- **幂调度（power scheduling）**设置学习率为迭代次数$t$的函数：$\eta(t)=\eta_0/(1+t/s)^c$。初始学习率$\eta_0$、幂$c$（通常设置为1）与步$s$为超参数。学习率会在每步下降。当然，幂调度要求调整$\eta_0$与$s$（可能还有$c$）。
- **指数调度（exponential scheduling）**设置学习率为$\eta_t=\eta_0\ 0.1^{t/s}$。学习率每$s$步逐渐下降10倍。幂调度降低学习率的速度越来越慢，而指数调度则持续每$s$步将其削减10倍。
- **分段恒定调度（piecewise constant scheduling）**使用在若干代内使用恒定学习率（例如，5代内使用$\eta_0=0.1$），然后在另一个若干代内使用较小的学习率（例如，50代内使用$\eta_1=0.001$），如此下去。虽然该解决方法可以很好地工作，但是这要求找到正确的学习率顺序以及每种学习率使用多长。
- **性能调度（performance scheduling）**每$N$步度量验证误差（就像早停法一样），然后当学习率不再下降时将学习率下降$\lambda$倍。
- **1周期调度（1cycle scheduling）**由Leslie Smith在[2018年的一篇论文](https://homl.info/1cycle)中引入。与其他方法相反，它首先增加初始学习率$\eta_0$，在训练中途将其线性增长至$\eta_1$。然后在训练的后半部分，它再线性将降低学习率到$\eta_0$，最后将学习率线性降低若干数量级完成最后几代。最大学习率$\eta_1$的选择方法与寻找最佳学习率的方法相同，初始学习率$\eta_0$被选择为大约低10倍的值。当使用动量（momentum）时，它开始于一个大的动量值（例如0.95），然后在训练的前半部分将其减低到一个较低的动量值（例如，线性降低到0.85），然后在训练的后半部分将其恢复到最大值（例如0.95），最后使用该最大值完成最后几代。Smith做了很多实验，并展示该方法常常可以显著提升训练并得到更好表现。例如，在流行的CIFAR10图像数据集上，该方法仅在100代内就达到了91.9%的验证准确率，而通过标准方法（使用相同的神经网络结构）在800代内达到90.3%的准确率。

Andrew Senior等人在[2013年的一篇论文](https://homl.info/63)中比较了在使用动量优化训练为语音识别深度神经网络时，一些最受欢迎的学习调度的性能。作者的结论是：在该设置下，性能调度与指数调度的表现很好。他们更偏向于指数调度，因为它容易调整，并且时候收敛到最优解稍快一点（他们还指出实现性能调整更简单，但是在Keras中两个选项都很简单）。1周期方法似乎表现得更好。

在Keras中实现幂调度是最简单的：只需要在创建优化器时设置`decay`：

```python
optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)
```

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
```

```python
n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
```

```python
import math

learning_rate = 0.01
decay = 1e-4
batch_size = 32
n_steps_per_epoch = math.ceil(len(X_train) / batch_size)
epochs = np.arange(n_epochs)
lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)

plt.plot(epochs, lrs,  "o-")
plt.axis([0, n_epochs - 1, 0, 0.01])
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
plt.title("Power Scheduling", fontsize=14)
plt.grid(True)
plt.show()
```

`decay`为$s$的倒数，Keras假设$c$等于1。

指数调度与分段调度同样相当简单。首先需要定义一个函数，它接收当前代，并返回学习率：

```python
def exponential_decay_fn(epoch):
    return 0.01 * 0.1**(epoch / 20)
```

如果不想硬编码$\eta_0$与$s$，可以创建一个函数，它返回一个已配置的（configured）函数：

```python
def exponential_decay(lr0, s):
    def exponential_decay_fn(epoch):
        return lr0 * 0.1**(epoch / s)
    return exponential_decay_fn

exponential_decay_fn = exponential_decay(lr0=0.01, s=20)
```

下面可以创建一个`LearningRateScheduler`回调，为其提供调度函数，然后将该回调传递给`fit`方法：

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 25
```

```python
lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[lr_scheduler])
```

```python
plt.plot(history.epoch, history.history["lr"], "o-")
plt.axis([0, n_epochs - 1, 0, 0.011])
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
plt.title("Exponential Scheduling", fontsize=14)
plt.grid(True)
plt.show()
```

`LearningRateScheduler`会在每代开始更新优化器的`learning_rate`属性。每代更新一次学习率通常已足够，但是如果想要更频繁地更新它，例如在每步更新它，则总是可以自定义回调。如果每代有很多步，则在每步更新学习率是有意义的。

```python
K = keras.backend

class ExponentialDecay(keras.callbacks.Callback):
    def __init__(self, s=40000):
        super().__init__()
        self.s = s

    def on_batch_begin(self, batch, logs=None):
        # Note: the `batch` argument is reset at each epoch
        lr = K.get_value(self.model.optimizer.lr)
        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
lr0 = 0.01
optimizer = keras.optimizers.Nadam(lr=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 25

s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
exp_decay = ExponentialDecay(s)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[exp_decay])
```

```python
n_steps = n_epochs * len(X_train) // 32
steps = np.arange(n_steps)
lrs = lr0 * 0.1**(steps / s)
```

```python
plt.plot(steps, lrs, "-", linewidth=2)
plt.axis([0, n_steps - 1, 0, lr0 * 1.1])
plt.xlabel("Batch")
plt.ylabel("Learning Rate")
plt.title("Exponential Scheduling (per batch)", fontsize=14)
plt.grid(True)
plt.show()
```

调度函数可选地接受当前学习率作为第二个实参。例如，以下调度将前一学习率乘以$0.1^{1/20}$，这会导致相同的指数衰减（只是现在衰减开始于代0而不是1的开始）：

```python
def exponential_decay_fn(epoch, lr):
    return lr * 0.1**(1 / 20)
```

该实现依赖于优化器的初始学习率（与前一实现相反），所以要确保它被适当（appropriately）设置。

当保存一个模型时，它的学习率同时被保存。这意味着使用这个新的调度函数，可以加载一个已训练的模型，然后在它停止的地方继续训练。但是如果调度函数使用`epoch`实参，则代（epoch）不会被保存，并且每次调用`fit`方法时它被重置为0。如果在模型停止的地方继续训练它，可能导致非常大的学习率，这可能会破坏模型权重。一种解决方法是手动设置`fit`方法的`initial_epoch`实参，使`epoch`从正确的值开始。

对于分段恒定调度，可以使用类似如下的调度函数，然后使用它创建一个`LearningRateScheduler`并将其传递给`fit`方法，就像为指数调度所做的那样：

```python
def piecewise_constant_fn(epoch):
    if epoch < 5:
        return 0.01
    elif epoch < 15:
        return 0.005
    else:
        return 0.001
```

```python
# 如果不想硬编码的话。
def piecewise_constant(boundaries, values):
    boundaries = np.array([0] + boundaries)
    values = np.array(values)
    def piecewise_constant_fn(epoch):
        return values[np.argmax(boundaries > epoch) - 1]
    return piecewise_constant_fn

piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])
```

```python
lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[lr_scheduler])
```

```python
plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], "o-")
plt.axis([0, n_epochs - 1, 0, 0.011])
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
plt.title("Piecewise Constant Scheduling", fontsize=14)
plt.grid(True)
plt.show()
```

对于性能调度，使用`ReduceLROnPlateau`。每当最佳验证损失连续5代不改善时，以下代码会将学习率乘以0.5（其它选项也是可行的，查看文档以了解更多细节）：

```python
tf.random.set_seed(42)
np.random.seed(42)
```

```python
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[lr_scheduler])
```

```python
plt.plot(history.epoch, history.history["lr"], "bo-")
plt.xlabel("Epoch")
plt.ylabel("Learning Rate", color='b')
plt.tick_params('y', colors='b')
plt.gca().set_xlim(0, n_epochs - 1)
plt.grid(True)

ax2 = plt.gca().twinx()
ax2.plot(history.epoch, history.history["val_loss"], "r^-")
ax2.set_ylabel('Validation Loss', color='r')
ax2.tick_params('y', colors='r')

plt.title("Reduce LR on Plateau", fontsize=14)
plt.show()
```

最后，tf.keras提供了可选的方法实现学习率调度：使用`keras.optimizers.schedules`中的一个调度来定义学习率，然后将该学习率传递给任意优化器。该方法会在每步而不是每代更新学习率。例如，以下diamante实现了与之前定义的` exponential_decay_fn`函数一样的指数调度：

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)
optimizer = keras.optimizers.SGD(learning_rate)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
```

使用这种方法，当保存模型时，学习率与它的调度（包括它的状态）也被保存。但是这种方法不是Keras API的一部分，它是tf.keras独有的。

对于分段恒定调度，尝试以下代码：

```python
learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],
    values=[0.01, 0.005, 0.001])
```

对于1周期方法，只需要创建一个自定义的回调，它在每次迭代时修改学习率（可以通过改变`self.model.optimizer.lr`来更新优化器的学习率）：

```python
K = keras.backend

class ExponentialLearningRate(keras.callbacks.Callback):
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []
    def on_batch_end(self, batch, logs):
        self.rates.append(K.get_value(self.model.optimizer.lr))
        self.losses.append(logs["loss"])
        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)

def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):
    init_weights = model.get_weights()
    iterations = math.ceil(len(X) / batch_size) * epochs
    factor = np.exp(np.log(max_rate / min_rate) / iterations)
    init_lr = K.get_value(model.optimizer.lr)
    K.set_value(model.optimizer.lr, min_rate)
    exp_lr = ExponentialLearningRate(factor)
    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,
                        callbacks=[exp_lr])
    K.set_value(model.optimizer.lr, init_lr)
    model.set_weights(init_weights)
    return exp_lr.rates, exp_lr.losses

def plot_lr_vs_loss(rates, losses):
    plt.plot(rates, losses)
    plt.gca().set_xscale('log')
    plt.hlines(min(losses), min(rates), max(rates))
    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])
    plt.xlabel("Learning rate")
    plt.ylabel("Loss")
```

```python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(lr=1e-3),
              metrics=["accuracy"])
```

```python
batch_size = 128
rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)
plot_lr_vs_loss(rates, losses)
```

```python
class OneCycleScheduler(keras.callbacks.Callback):
    def __init__(self, iterations, max_rate, start_rate=None,
                 last_iterations=None, last_rate=None):
        self.iterations = iterations
        self.max_rate = max_rate
        self.start_rate = start_rate or max_rate / 10
        self.last_iterations = last_iterations or iterations // 10 + 1
        self.half_iteration = (iterations - self.last_iterations) // 2
        self.last_rate = last_rate or self.start_rate / 1000
        self.iteration = 0
    def _interpolate(self, iter1, iter2, rate1, rate2):
        return ((rate2 - rate1) * (self.iteration - iter1)
                / (iter2 - iter1) + rate1)
    def on_batch_begin(self, batch, logs):
        if self.iteration < self.half_iteration:
            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)
        elif self.iteration < 2 * self.half_iteration:
            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,
                                     self.max_rate, self.start_rate)
        else:
            rate = self._interpolate(2 * self.half_iteration, self.iterations,
                                     self.start_rate, self.last_rate)
        self.iteration += 1
        K.set_value(self.model.optimizer.lr, rate)
```

```python
n_epochs = 25
onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[onecycle])
```

需要注意的是，`on_batch_end`方法与`logs["loss"]`之前用来包含批损失，但是在TensorFlow 2.2.0中它被替换为（从代的开始的）平均损失。这意味着上图更平滑，也意味着从批损失开始爆炸到爆炸在图中开始清晰的那一刻之间，存在延迟。因此应该选择一个比本来在“嘈杂”图中选择的学习率稍小的学习率。当然也可以微调上述的`ExponentialLearningRate`以让它（基于当前均值与前一均值）计算批损失：

```python
class ExponentialLearningRate(keras.callbacks.Callback):
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []
    def on_epoch_begin(self, epoch, logs=None):
        self.prev_loss = 0
    def on_batch_end(self, batch, logs=None):
        batch_loss = logs["loss"] * (batch + 1) - self.prev_loss * batch
        self.prev_loss = logs["loss"]
        self.rates.append(K.get_value(self.model.optimizer.lr))
        self.losses.append(batch_loss)
        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)
```

总之，指数调度、性能调度与1周期能显著加速收敛。

### 通过正则化避免过拟合

深度神经网络通常有数万甚至数百万参数，这给了它们巨大的自由度，并意味着它们可以拟合各种复杂数据集。但是大的自由度也使得网络容易过拟合数据集，因此需要正则化。

之前实现的早停法是最好的正则化技术之一。另外，虽然批规范化是为了解决不稳定梯度问题而设计的，它也充当一个相当好的正则化器。

### $\ell_1$与$\ell_2$正则化

就像为简单线性模型所做的那样，可以使用$\ell_2$正则化去约束神经网络的连接权重，且/或使用$\ell_1$正则化，如果想要一个稀疏模型（很多权重等于0）的话。以下展示了如何对Keras层的连接权重应用$\ell_2$正则化，它使用0.01的正则化因子：

```python
layer = keras.layers.Dense(100, activation="elu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=keras.regularizers.l2(0.01))
```

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="elu",
                       kernel_initializer="he_normal",
                       kernel_regularizer=keras.regularizers.l2(0.01)),
    keras.layers.Dense(100, activation="elu",
                       kernel_initializer="he_normal",
                       kernel_regularizer=keras.regularizers.l2(0.01)),
    keras.layers.Dense(10, activation="softmax",
                       kernel_regularizer=keras.regularizers.l2(0.01))
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
```

$l2$函数返回一个正则化器，它会在训练中每步被调用，来计算正则化损失。然后这被加到最终损失上。如果想要$\ell_1$正则化，则使用`keras.regularizers.l1`方法；如果同时想要$\ell_1$与$\ell_2$正则化，则使用`keras.regularizers.l1_l2`方法并指定两个正则化因子。

因为通常想要对网络中的所有层应用相同的正则化器，并对所有隐藏层使用相同的激活函数与相同的初始化策略，这样就会重复相同的实参。这使得代码很丑并且很容易出错。为了避免这点，可以重构代码去使用循环。另一个选择是使用Python的`functools.partial`函数，它可以使用一些默认实参值，为任何可调用对象创建瘦包装器（thin wrapper）：

```python
from functools import partial

RegularizedDense = partial(keras.layers.Dense,
                           activation="elu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=keras.regularizers.l2(0.01))

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    RegularizedDense(300),
    RegularizedDense(100),
    RegularizedDense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
```

### dropout

**dropout**是深度神经网络中最受欢迎的正则化技术之一。它是Geoffrey Hinton 2012年在[一篇论文](https://homl.info/64)中提出，并由Nitish Srivastava等人在[2014年的一篇论文](https://homl.info/65)中被进一步详述，并且它被证明非常成功：即使是最先进的神经网络，简单地加了dropout，也能获得1\~2%的准确率提升。听起来可能不多，但是当一个模型已经得到95%的准确率，2%的准确率提升意味着错误率降低近40%（从5%到大约3%）。

它是一个相当简单的算法：在每一个训练步，每个神经元（包括输入神经元，但是总是不包括输出神经元）有$p$的概率被临时“丢弃（dropped out）”，意味着它在该训练步被完全忽略，但是可能在下一步中处于激活状态（如图）。超参数*p*被称为**dropout率（dropout rate）**，它通常设置为10%\~50%：在循环神经网络中接近20%\~30%，在卷积神经网络中它接近40%\~50%。训练后，神经元不会再被丢弃。

![使用dropout正则化，每次训练迭代，一层或多层（不包括输出层）中所有神经元的一个随机子集被“丢弃”，它们在该次迭代输出0（由虚线箭头表示）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\dropout正则化.png)

这种破坏性的技术是奏效的，刚开始知道这点令人感到惊讶。类比一家公司：如果它的员工被告知每天早上投硬币去决定是否去上班，则这公司说不定表现得更好。公司将被迫调整它的组织结构。它不能依赖任何个人执行关键人物，所以专业技能必须在若干人间传播。员工必须学习与许多同事、而不是少数几个同事协作。公司的适应力将大大提升。一个人辞职不会造成很大影响。虽然不清楚这种思想是否适用于公司，但是它肯定适用于神经网络。使用dropout训练的神经元不能与相邻神经元共同适应，它们必须让自己尽可能有用。它们也不能过分依赖少数几个输入神经元，它们必须关注它们的每个输入神经元。它们最终对输入中的微小变化敏感度降低。最终，得到一个更健壮的网络，它的泛化能力更好。

理解dropout威力的另一种方式是要意识到在训练过程中的每一步都会产生一个特有的神经网络。每个神经元可以存在或缺失，因此一共有$2^N$个可能的网络（$N$为可丢弃的神经元的总数）。这是一个巨大的数，几乎不可能对同一神经网络采样两次。一旦运行10000个训练步，实际上就训练了10000个不同的神经网络（每个只有一个训练实例）。这些网络显然不相互独立，因为它们共享很多相同权重，但它们都是不同的。得到的神经网络可以看做所有这些较小神经网络的一个平均集成。

在实践中，通常只能对顶部1\~3层（不包括输出层）的神经元应用dropout。

一个小的但是重要的细节在于：在测试过程中，一个神经元连接到的输入神经元的数量将是训练过程中的$1/(1-p)$倍（平均而言）。为此必须在训练后将每个神经元的输入连接权重乘以**保持概率（keep probability）**$1-p$，否则每个神经元的总输入信号将大约是训练时的$1/(1-p)$倍，它不太可能会表现好。也可以在训练过程中将每个神经元的输出除以保持概率。这两种方法不完全等价，但是它们表现得同样好。

为了使用Keras实现dropout，可以使用`keras.layers.Dropout`层。在训练过程中，它随机丢弃一些输入（设置它们为0）并将剩余输入除以保持概率。训练后，它什么也不做：它只是将输入传递给下一层。下列代码在每个`Dense`层前应用dropout正则化，dropout率为0.2。

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
```

因为dropout只在训练过程中有效，比较训练损失与验证损失可能具有误导性。特别地，模型可能在训练集上过拟合，但是仍然有相似的训练与验证误差。所以要确保不使用dropout评估训练损失。

如果观察到模型过拟合，则可以增加dropout率。反之，如果模型欠拟合训练集，则应该尝试降低dropout率。对大的层增加dropout率；对小的层减小dropout率也会有帮助。另外，许多最先进的结构只在最后一个隐藏层后使用dropout，因此dropout过强，则可以尝试这样。

dropout容易显著降低收敛速度，但是如果调整得当，它通常会产生好得多的模型。因此，这通常是值得付出额外时间与努力的。

如果想要正则化基于SELU激活函数的自规范化（self-normalizing）网络，则应该使用**alpha dropout**：它是dropout的变体，它保持了它的输入的均值与标准差（它与SELU在同一篇论文中被引入，因为常规的dropout会破坏自规范化）。以下代码演示了alpha dropout的使用：

```python
tf.random.set_seed(42)
np.random.seed(42)
```

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 20
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
```

```python
model.evaluate(X_test_scaled, y_test)
```

```python
model.evaluate(X_train_scaled, y_train)
```

```python
history = model.fit(X_train_scaled, y_train)
```

#### Monte Carlo（MC） Dropout

2016年，Yarin Gal与Zoubin Ghahramani的一篇[论文](https://homl.info/mcdropout)又增加了一些使用dropout的好理由：

- 第一，该论文在dropout网络间建立了深刻的（profound）连接（即在每个权重层前包含一个`Dropout`层的神经网络），并近似Bayesian推断，这给了dropout一个牢固的数学证明。
- 第二，作者引入了一个强大的技术，被称为**MC dropout**，它可以提升任意一个已训练的dropout模型的性能，而无需重新训练它，甚至完全不需要修改它。它提供了一个好得多的度量模型不确定性的指标（measure），并且实现起来也非常简单。

以下是MC Dropout的完整实现，它提升了前面的dropout模型的性能，且模型未被重新训练：

```python
tf.random.set_seed(42)
np.random.seed(42)
```

```python
y_probas = np.stack([model(X_test_scaled, training=True)
                     for sample in range(100)])
y_proba = y_probas.mean(axis=0)
y_std = y_probas.std(axis=0)
```

这里只是在测试集上作了100次预测，并且设置了`training=True`以保证`Dropout`层被激活，并堆叠预测结果。因为dropout被激活，所有预测将不同。`predict`方法返回一个矩阵，每个实例一行，每个类别一列。因为测试集中有10000个实例，类别有10个，所以这个矩阵的形状为$[10000,10]$。这里堆叠了100个这样的矩阵，因此`y_probas`为一个数组，它的形状为$[100,10000,10]$。一旦沿第一个维度（`axis=0`）求平均，就得到了`y_proba`，这是一个数组，形状为$[10000,10]$，就像只做了一个预测一样。使用dropout，对多个结果平均，得到的Monte Carlo估计通常比不使用dropout的单个预测的结果更可靠。例如，查看不使用dropout时，模型对Fashion MNIST测试集中第一个实例的预测：

```python
np.round(model.predict(X_test_scaled[:1]), 2)
```

模型似乎几乎肯定这张图片属于类别9（ankle boot）。当dropout被激活，预测如下：

```python
np.round(y_probas[:, :1], 2)
```

显然，当dropout被激活，模型不再确定了。它似乎还是偏向于类别9，但是有时它会对类别5（sandal）与类别7（sneaker）犹豫，这是有道理的，因为它们都是鞋（footwear）。一旦沿着第一个维度求平均，就得到如下的MC Dropout预测：

```python
np.round(y_proba[:1], 2)
```

模型仍然认为该图像属于类别9，但是只有62%的置信度，看上去比99%合理得多。另外，准确地知道它认为可能的其他类别也是有用的。还可以查看[概率估计的标准差](https://xkcd.com/2110/)：

```python
y_std = y_probas.std(axis=0)
np.round(y_std[:1], 2)
```

显然，概率估计的方差相当大。如果要建立一个风险敏感系统（例如，医疗或金融系统），则可能应该极其谨慎地对待这种不确定的预测。这里当然不能将其看作有99%置信度的预测。另外，模型的准确率从86.8略微提升到86.9：

```python
y_pred = np.argmax(y_proba, axis=1)
```

```python
accuracy = np.sum(y_pred == y_test) / len(y_test)
accuracy
```

Monte Carlo样本数（本例中是100）是一个可以微调的超参数。它越大，预测以及它们的不确定性估计越准确。但是，如果将其加倍，则推断时间也加倍。另外，样本数超过一定值，改善也很小。因此要根据应用，在延迟与准确率间找到正确的折中。

如果模型包含其他在训练过程中表现特殊的层（例如`BatchNormalization`层），则不应该像之前这样强制使用训练模式。而应该使用如下`MCDropout`类替换`Dropout`层：

```python
class MCDropout(keras.layers.Dropout):
    def call(self, inputs):
        return super().call(inputs, training=True)

class MCAlphaDropout(keras.layers.AlphaDropout):
    def call(self, inputs):
        return super().call(inputs, training=True)
```

```python
tf.random.set_seed(42)
np.random.seed(42)
```

```python
mc_model = keras.models.Sequential([
    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer
    for layer in model.layers
])
```

```python
mc_model.summary()
```

```python
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)
mc_model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
```

```python
mc_model.set_weights(model.get_weights())
```

```python
np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)
```

这里只是子类化（subclass）`Dropout`层并覆盖了`call`方法以强制其`training`实参为`True`。同样地，这里通过子类化`AlphaDropout`定义了一个`MCAlphaDropout`类。如果从头创建一个模型，则只需要使用`MCDropout`而不是`Dropout`即可。但是如果模型已经使用`Dropout`训练过，则需要创建一个新的模型，它与已有模型相同，只是它使用`MCDropout`替换了`Dropout`层；然后将已有模型的权重拷贝到新模型中。

该`MCDropout`类适用于所有Keras API，包括顺序API。如果只关心函数式API或子类化API，则不需要创建`MCDropout`类，而是可以创建一个常规的`Dropout`层并使用`training=True`来调用它。

简言之，MC Dropout是一个好（fantastic）技术，它增强dropout模型，并提供更好的不确定性估计。当然，因为训练过程中它只是一个常规的dropout，它同时充当一个正则化器。

### 最大范数正则化

另一种用于神经网络中的正则化技术被称为**最大范数正则化（max-norm regularization）**：对于每个神经元，它约束传入（incoming）连接的权重，以使得$||\pmb{w}||_2\le r$，其中$r$为最大范数超参数，$||·||_2$为$\ell_2$范数。

最大范数正则化不会将正则化损失项加到总损失函数上。它通常通过在每个训练步后计算$||\pmb{w}||_2$并在需要时重缩放$||\pmb{w}||$来实现（$\pmb{w}\leftarrow\pmb{w}r||\pmb{w}/||_2$）。

减小$r$增加正则化量，有助于降低过拟合风险。最大范数正则化还可以减缓不稳定梯度问题（如果没有使用批规范化的话）。

要在Keras中实现最大范数正则化，需要设置每个隐藏层的`kernel_constraint`实参为具有适当最大值的`max_norm()`，如下：

```
layer = keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal",
                           kernel_constraint=keras.constraints.max_norm(1.))
```

```python
MaxNormDense = partial(keras.layers.Dense,
                       activation="selu", kernel_initializer="lecun_normal",
                       kernel_constraint=keras.constraints.max_norm(1.))

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    MaxNormDense(300),
    MaxNormDense(100),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
```

每次训练迭代后，模型的`fit`方法将调用`max_norm`方法返回的对象，将层的权重传递给它并得到返回的重缩放后的权重，它会替换层的权重。如果需要的话，还可以自定义约束函数并将其用作`kernel_constraint`。还可以通过设置` bias_constraint`实参来约束偏置项。

`max_norm`函数有一个默认为`0`的`axis`实参。一个`Dense`层的权重形状通常为$[number\ of\ inputs,number\ of\ neurons]$，所以使用`axis=0`意味着最大范数约束将独立应用于每个神经元的权重向量。如果要在卷积层中使用最大范数，则确保正确（appropriately）设置`max_norm`约束的`axis`实参（通常为`axis=[0, 1, 2]`）。

## 总结

选择使用哪一个技术取决于实际任务，目前还没有明确的共识，但是以下配置在大多数情况下工作得很好，而不需要很多超参数调整（但是不要将这些默认设置当成硬性标准）：

| 超参数                           | 默认值                                         |
| -------------------------------- | ---------------------------------------------- |
| 核初始化器（kernel initializer） | He初始化                                       |
| 激活函数                         | ELU                                            |
| 规范化                           | 如果网络浅，不需要；如果网络深，则使用批规范化 |
| 正则化                           | 早停法（+$\ell_2$正则化，如果需要的话）        |
| 优化器                           | 动量优化（或RMSProp或Nadam）                   |
| 学习率调度                       | 1cycle                                         |

如果网络是密集层组成的简单栈，则它可以自规范化（self-normalize），则应该使用以下配置：

| 超参数                           | 默认值                                 |
| -------------------------------- | -------------------------------------- |
| 核初始化器（kernel initializer） | LeCun初始化                            |
| 激活函数                         | SELU                                   |
| 规范化                           | 不需要（自规范化，self-normalization） |
| 正则化                           | Alpha dropout，如果需要的话            |
| 优化器                           | 动量优化（或RMSProp或Nadam）           |
| 学习率调度                       | 1cycle                                 |

不要忘记规范化输入特征。另外，如果能找到一个解决相似问题的预训练的神经网络，应该尝试重用它的一部分，或者如果有许多无标签数据，则使用无监督预训练，或者如果有许多用于相似任务的有标签数据，则在辅助任务上使用预训练。

前面的指导可以覆盖大多数情况，一些例外如下：

- 如果需要一个稀疏模型，则可以使用$\ell_1$正则化（或者在训练后将小权重归零）。如果需要一个更稀疏的模型，则可以使用TensorFlow Model Optimization Toolkit。
- 如果需要一个低延迟模型（执行快速预测），则可能需要更少的层，将批规范化折叠到前面层中，并可能需要使用更快的激活函数，例如leaky ReLU或ReLU。稀疏模型也能有所帮助。最后，可能需要将浮点数精度从32位降低到16甚至8位。另外，查看TFMOT。
- 如果要构建风险敏感应用，或推断延迟（inference latency）在应用中不是非常重要，则可以使用MC Dropout去提升性能并得到更可靠的概率估计，以及不确定性估计。

有了这些指导，就可以准备好训练非常深的网络了。

# 卷积神经网络

## 网络结构

### 卷积层

**卷积层（convolutional layer）**是**卷积神经网络（convolutional neural network，CNN）**最重要的构建层（building block）。第一个卷积层的神经元不是与输入的每个特征（如图像像素）相连，而是仅与感受域（receptive fields）内的特征相连（如图）；类似地，第二个卷积层中的每个神经元仅与位于第一层小矩形内的神经元相连。该结构允许网络聚焦第一个隐藏层的小的低层特征，然后在下一个隐藏层中将它们组合成更大的高层特征，以此类推。这种层状结构在现实世界图像中很常见，这也是CNN在图像识别中表现出色的原因之一。

![具有矩形局部感受域的CNN层](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\具有矩形局部感受域的CNN层.png)

注意，在CNN中每层表示为2维（考虑到多个过滤器，实际上是3维），这样就更容易将神经元与其对应的输入相匹配。

位于给定层第$i$行、第$j$列的神经元连接到前一层位于第$i$到$i+f_h-1$行、第$j$到第$j+f_w-1$列的神经元的输出相连，其中$f_h$与$f_w$为感受域的高度与宽度（如图）。为了使层与前一层有相同的高度与宽度，常常在输入周围添加一些零。这被称为**零填充（zero padding）**。

![层间连接与零填充](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\层间连接与零填充.png)

也可以通过分隔感受域将大的输入层连接到小的输出层（如图），这将大幅降低模型的计算复杂度。从一个感受域到下一个感受域的转移被称为**步长（stride）**。在下图中，通过使用$3\times 3$的感受域与2的步长，一个$5\times 7$的输入层（加上零填充）与一个$3\times 4$的层连接（在这个例子中，两个方向上的步长相同，但并非必须如此）。位于上层的第$i$行、第$j$列的神经元连接到前一层位于第$i\times s_h$到第$i\times s_h+f_h-1$行、第$j\times s_w$到第$j\times s_w+f_w-1$列的神经元，其中$s_h$与$s_w$为水平与垂直步长。

![使用2的步长降维](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用2的步长降维.png)

#### 过滤器

一个神经元的权重可被表示为感受域大小的小图像。例如，下图展示了两组可能的权重，称为**过滤器（filters）**或**卷积核（convolution kernels）**。第一个过滤器表示为一个黑色正方形，中间有一条垂直白线（它是一个$7\times 7$的矩阵，中间列全1，其余部分全部为0），使用该权重的神经元会忽略感受域内除中间垂直线外的一切（因为除了位于中间垂直线外的输入都被乘以0）。第二个过滤器为黑色正方形，中间有一条水平白线，使用该权重的神经元会忽略感受域内除中间水平线外的一切。

如果一层的所有神经元使用相同的水平线过滤器（以及相同的偏置项），并且将下图所示的输入图片输入网络（底部图片），该层输出左上角的图片。注意水平白线得到增强，而其余部分变得模糊。类似地，如果所有神经元使用相同的垂直线过滤器，则得到右上角的图片。注意垂直白线得到增强而其余部分变得模糊。因此，所有神经元使用相同过滤器的一层输出一个**特征图（feature map）**，它会突出图像中最能激活过滤器的部分。当然，不需要手动定义这些过滤器，在训练过程中，卷积层会自动学习对任务最有用的过滤器，上层（the layers above）会学习将它们组合成更复杂的模式。

<a name="(卷积神经网络)(网络结构)(卷积层)(过滤器)(1)">![应用两个不同的过滤器得到两个特征图](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\应用两个不同的过滤器得到两个特征图.png)</a>

实际上每个卷积层包含多个过滤器，并且对于每个过滤器都输出一个特征图，因此每个卷积层被表示为3维（如图）。在每个特征图中，每个像素有一个神经元，一个给定特征图中的所有神经元共享相同参数（即相同的权重与偏置项），不同特征图中的神经元使用不同权重。一个神经元的感受域会延伸所有前层的特征图。简言之，一个卷积层同时对它的输入应用多个可训练的过滤器，这使得它可以检测输入中任意位置的多个特征。

由于一个特征图的所有神经元共享相同参数，模型的参数数量得以大幅度减少。一旦CNN学会识别一个位置的模式，它可以识别任意其他位置的模式。相反，一个普通的DNN学会识别一个位置的模式，它就只能识别该特定位置的模式。

输入，例如图像，也由多个子层组成，一个颜色通道（color channel）一个，通常有三个：红、绿与蓝（RGB）。灰度图像只有一个通道，而另一些图像有更多通道，例如捕获额外光频率的卫星图像。

![卷积层，包含多个特征图，以及有三个颜色通道的图像](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\卷积层.png)

具体来说，位于给定卷积层$l$中的特征图$k$的第$i$行、第$j$列的神经元连接到前一层$l-1$中的所有特征图的位于第$i\times s_h$到$i\times s_h+f_h-1$行、第$j\times s_w$到第$j\times s_w+f_w-1$列的神经元的输出。注意所有位于相同行$i$与列$j$但是不同的特征图中的神经元连接到前一层的完全相同神经元的输出。

一个卷积层中的给定神经元的输出计算如下：
$$
z_{i,j,k}=b_k+\sum_{u=0}^{f_h-1}\sum_{v=0}^{f_w-1}\sum_{k'=0}^{f_{n'}-1}x_{i',j',k'}\cdot w_{u,v,k',k}
$$

$$
\left\{
\begin{aligned}
i'=i\times s_h+u\\
j'=j\times s_w+v
\end{aligned}
\right.
$$

其中：

- $z_{i,j,k}$为卷积层（层$l$）的特征图$k$中的的第$i$行、第$j$列的神经元的输出。
- $s_h$与$s_w$为垂直与水平步长，$f_h$与$f_w$为感受域的高度与宽度，$f_{n'}$为前一层（层$l-1$）中的特征图的数量。
- $x_{i',j',k'}$为位于层$l-1$、行$l'$、列$j'$、特征图$k'$（如果前一层为输入层，则为通道$k'$）的神经元的输出。
- $b_k$为层$l$中的特征图$k$的偏置项。可以将它想象为一个旋钮（knob），可用来调整特征图$k$的整体亮度。
- $w_{u,v,k',k}$为层$l$的特征图$k$中的任意神经元与它的位于行$u$、列$v$（相对于神经元的感受域）与特征图$k'$的输入之间的连接权重。

下面在TensorFlow中实现卷积层。

在TensorFlow中，每张图片通常表示为3维张量，形状为$[height,width,channels]$。一个小批表示为3维张量，形状为$[mini-batch size,height,width,channels]$。卷积层的权重表示为4维张量，形状为$[f_h,f_w,f_n',f_n]$。卷积层的偏置项简单表示为1维张量，形状为$[f_n]$。

以下代码使用Scikit-Learn的`load_sample_image`加载两张样本图像（两个彩色图像（color images），一个为中国寺庙，一个为花），然后创建两个过滤器并将它们应用到两张图片，最后展示结果。注意，如果要使用`load_sample_image`，必须安装`Pillow`包。

```python
def plot_image(image):
    plt.imshow(image, cmap="gray", interpolation="nearest")
    plt.axis("off")

def plot_color_image(image):
    plt.imshow(image, interpolation="nearest")
    plt.axis("off")
```

```python
import numpy as np
from sklearn.datasets import load_sample_image
import tensorflow as tf
import matplotlib.pyplot as plt

# Load sample images
china = load_sample_image("china.jpg") / 255
flower = load_sample_image("flower.jpg") / 255
images = np.array([china, flower])
batch_size, height, width, channels = images.shape

# Create 2 filters
filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
filters[:, 3, :, 0] = 1  # vertical line
filters[3, :, :, 1] = 1  # horizontal line

outputs = tf.nn.conv2d(images, filters, strides=1, padding="SAME")

plt.imshow(outputs[0, :, :, 1], cmap="gray") # plot 1st image's 2nd feature map
plt.axis("off")
plt.show()
```

```python
for image_index in (0, 1):
    for feature_map_index in (0, 1):
        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)
        plot_image(outputs[image_index, :, :, feature_map_index])

plt.show()
```

```python
def crop(images):
    return images[150:220, 130:250]
```

```python
plot_image(crop(images[0, :, :, 0]))
plt.show()

for feature_map_index, filename in enumerate(["china_vertical", "china_horizontal"]):
    plot_image(crop(outputs[0, :, :, feature_map_index]))
    plt.show()
```

```python
plot_image(filters[:, :, 0, 0])
plt.show()
plot_image(filters[:, :, 0, 1])
plt.show()
```

- 每个颜色通道的像素灰度（intensity）表示为一个0\~255之间的字节，因此需要通过除以255放缩特征，以得到0\~1之间的浮点数。
- 然后创建两个$7\times 7$的过滤器（一个中间有一条垂直白线，另一个中间有一条水平白线）。
- 使用`tf.nn.conv2d`函数（TensorFlow低层深度学习API的一部分）将它们应用到两张图像，然后使用零填充（`paddiing="SAME"`）、步长1。
- 最后绘制得到的特征图，它与[图1](#(卷积神经网络)(网络结构)(卷积层)(过滤器)(1))右上角的图像相似。

对于`tf.nn.conv2d`函数：

- `image`为输入小批（一个4维张量，如前所述）。
- `filter`为要应用的一组过滤器（一个4维张量，如前所述）。
- `stride`也可以为一个1维数组，包含4个元素，中间两个元素表示垂直与水平步长（$s_h$与$s_w$），第一个与最后一个元素目前必须为1，但是未来可用来指定批步长（以跳过一些实例）与通道步长（以跳过前一层的一些特征图或通道）。
- `padding`必须是`SAME`或`VALID`。
  - 如果它被设置为`SAME`，则卷积层会在必要时使用零填充。输出大小等于输入神经元数量除以步长（向上取整），如图所示。然后根据需要在输入周围尽可能均匀地添加零。当`strides=1`，层的输出将具有与其输入相同的空间维度（宽度与高度），因此被称为“same”。
  - 如果它被设置为`VALID`，则卷积层不使用零填充，并且可能忽略输入图片底部与右部的一些行与列（取决于步长），如图所示（只显示了水平维度，但是相同的逻辑适用于垂直维度）。这意味着每个神经元的感受域严格位于输入有效位置内，因此被称为“valid”。

![padding="SAME"或"VALID"，其中输入宽度为13，过滤器宽度为6，步长为5](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\填充类型.png)

在该例中，过滤器被手动定义，但在实际的CNN中通常会将过滤器定义为可训练变量，以使得神经网络可以学习哪个过滤器工作得最好。与其手动创建变量，可以使用`keras.layers.Conv2D`层：

```python
from tensorflow import keras

np.random.seed(42)
tf.random.set_seed(42)

conv = keras.layers.Conv2D(filters=2, kernel_size=7, strides=1,
                           padding="SAME", activation="relu", input_shape=outputs.shape)
```

它创建了一个`Conv2D`层，有2个过滤器，每个都是$7\times 7$，水平与垂直步长都为1，并使用`same`填充，然后对它的输出应用ReLU激活函数。可以看到，卷积层有不少超参数：过滤器数量以及过滤器的高度与宽度、步长、填充类型。一如往常，可以使用交叉验证去寻找合适的（right）超参数值，但是这通常很耗时。

给它传递两张图片来调用它：

```python
conv_outputs = conv(images)
conv_outputs.shape 
```

因为过滤器被随机初始化，所以它们初始只能检测随机模式。下面查看每张图片的两个输出特征图：

```python
plt.figure(figsize=(10,6))
for image_index in (0, 1):
    for feature_map_index in (0, 1):
        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)
        plot_image(crop(conv_outputs[image_index, :, :, feature_map_index]))
plt.show()
```

可以看到虽然过滤器被随机初始化，但是第二个过滤器恰好表现得像一个边缘检测器。随机初始化的过滤器常常表现如此，而检测边缘在图像处理中相当有用。

下面设置过滤器为之前手动定义的过滤器，并设置偏置为0，然后再次在相同图像上调用该层并查看输出特征图是否如之前那样分别突出（highlight）了垂直线与水平线：

```python
conv.set_weights([filters, np.zeros(2)])
```

```python
conv_outputs = conv(images)
conv_outputs.shape 
```

```python
plt.figure(figsize=(10,6))
for image_index in (0, 1):
    for feature_map_index in (0, 1):
        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)
        plot_image(crop(conv_outputs[image_index, :, :, feature_map_index]))
plt.show()
```

CNN的一个问题在于卷积层需要大量的内存，尤其在训练过程中，因为反向传播的逆向传递需要在前向传递过程中计算得到的所有中间变量。

例如，一个卷积层有$5\times 5$过滤器，它输出200个大小为$150\times 100$的特征图，步长为1，并使用`same`填充。如果输入为$150\times 100$的RGB图片（三个通道），则参数数量为$(5\times 5\times 3+1)\times 200=15200$，相比于全连接层（有$150\times 100$个神经元，每个连接到所有的$150\times 100\times3$个输入，一共$6.75\times10^{8}$个参数），这相当少。但是，每个特征图包含$150\times 100$个神经元，每个神经元需要计算它的$5\times 5\times3=75$个输入的加权和，因此一共需要$2.25\times10^8$次浮点乘法，比全连接层好，但是计算量仍然相当大。如果特征图使用32位浮点数表示，则卷积层的输出将占用$200\times 150\times 100\times 32=9.6\times 10^7$位（$12$MB）。这只考虑了一个实例，如果训练批包含$100$个实例，则该层将的使用$1.2$GB内存。

在推断（即为新实例做出预测）过程中，一层占用的内存可以在计算完下一层后被立即释放，因此此时只需要两个连续层所需的内存量。但是在训练过程中，所有在前向传递过程中计算的内容都需要为反向传递保留，因此所需的内存至少是所有层需要的内存总量。

如果因为内存不足（out-of-memory）错误而导致训练崩溃，则可以降低小批大小，或使用一个步长降维，或移除一些层，或使用16位浮点数而不是32位浮点数，或将CNN分发到多个设备上。

### 池化层

**池化层（pooling layer）**的目的是**子采样（subsample）**（即收缩（shrink））输入特征（如图像）以降低计算负载、内存使用以及参数数量（从而降低过拟合的风险）。

与卷积层一样，池化层的每个神经元也连接到前一层的有限数量的神经元的输出。与卷积层类似，必须定义池化层的大小、步长与填充模式。但是，一个池化神经元没有权重，它所做的只是使用一个聚合函数（aggregation function），例如“max”或“mean”去聚合输入。如图展示了一个**最大池化层（max pooling layer）**，它是最常见的池化层类型。该示例使用了$2\times 2$的**池化核（pooling kernel）**（它没有权重，只是无状态的滑动窗口），步长为2，没有填充。只有每个感受域内的最大输入值才能进入下一层，其他输入被丢弃。因为步长为2，所以输出图像的高度与宽度都是输入图像的一半（向下取整）。

![最大池化层（2 × 2的池化核，步长为2，没有填充）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\最大池化层.png)

池化层通常在每个输入通道上独立工作，因此输出深度等于输入深度。

最大池化层除了可以降低计算量、内存使用与参数数量外，还为小平移（small translations）引入了某种程度的**不变性（invariance）**，如图所示。可以看到，图片A与B的最大池化层的输出相同。这就是平移不变性（translation invariance）的含义。对于图片C，输出则向右移动了一个像素，但是仍然有75%的不变性。通过在CNN中每隔几层插入一个最大池化层，可以在更大范围内获得某种程度的平移不变性。除此以外，最大池化层提供了少量的旋转不变性（rotational invariance）与轻微的尺度不变性（scale invariance）。当预测不应该依赖于这些细节时（例如在分类任务中），这样的不变性（即使有限）是有用的。

![对小平移的不变性](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\对小平移的不变性.png)

但是最大池化层也有一些缺点。首先，它的破坏性非常大，即使一个$2\times 2$的小核且步长为2，输出在两个方向上都减小两倍（因此它的面积减小了四倍），它只是简单丢弃了75%的值。在一些应用中，不变性是不可取的（not desirable）。例如在语义分割中，如果输入图片向右平移一个像素，则输出也应该向右平移一个像素，它的目标是**等变性（equivariance）**，而不是不变性：输入发生小变化应该导致输出发生对应的小变化。

> the output will be two times smaller in both directions (so its area will be four times smaller), simply dropping 75% of the input values. 

> (the task of classifying each pixel in an image according to the object that pixel belongs to, which we’ll explore later in this chapter):

下面创建一个使用$2\times 2$核的最大池化层。步长默认为核大小，所有该层使用2的（水平与垂直）步长。默认情况下使用`valid`填充。

```python
from tensorflow import keras

max_pool = keras.layers.MaxPool2D(pool_size=2)
```

```python
cropped_images = np.array([crop(image) for image in images], dtype=np.float32)
output = max_pool(cropped_images)
```

```python
import matplotlib as mpl

fig = plt.figure(figsize=(12, 8))
gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])

ax1 = fig.add_subplot(gs[0, 0])
ax1.set_title("Input", fontsize=14)
ax1.imshow(cropped_images[0])  # plot the 1st image
ax1.axis("off")
ax2 = fig.add_subplot(gs[0, 1])
ax2.set_title("Output", fontsize=14)
ax2.imshow(output[0])  # plot the output for the 1st image
ax2.axis("off")
plt.show()
```

如果要创建**平均池化层（average pooling layer）**，则使用`AvgPool2D`，它与最大池化层工作方式相同，只是计算的是平均值而不是最大值。平均池化层曾经很受欢迎，但是现在人们大多使用最大池化层，因为它们通常表现更好。因为虽然计算平均值通常比计算最大值丢失更少的信息，但是最大池化只保留了最强的特征并摆脱了所有无意义的特征，所以接下来的层（layers）获得了更干净的信号。另外，最大池化层提供了更强的平移不变性，且所需的计算量要略少。

```
avg_pool = keras.layers.AvgPool2D(pool_size=2)
```

```python
output_avg = avg_pool(cropped_images)
```

```python
fig = plt.figure(figsize=(12, 8))
gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])

ax1 = fig.add_subplot(gs[0, 0])
ax1.set_title("Input", fontsize=14)
ax1.imshow(cropped_images[0])  # plot the 1st image
ax1.axis("off")
ax2 = fig.add_subplot(gs[0, 1])
ax2.set_title("Output", fontsize=14)
ax2.imshow(output_avg[0])  # plot the output for the 1st image
ax2.axis("off")
plt.show()
```

也可以沿着深度维度（depth dimension）而不是空间维度（spatial dimensions）执行最大池化与平均池化，虽然这不常见。这使得CNN可以学会对各种特征保持不变性。例如，它可学习多个过滤器，每个过滤器检测相同模式的不同旋转,并且沿深度最大池化能保证无论旋转如何，输出是相同的。类似地，CNN可以学会对其他任何事物保持不变性，例如厚度、亮度、倾斜、颜色等等。

![沿深度最大池化有助于CNN学习任意不变性](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\沿深度最大池化.png)

Keras不包括沿深度最大池化层，但是TensorFlow的低层深度学习API包括，只需要使用`tf.nn.max_pool`函数，并用大小为4的元组指定核大小与步长。其中，每个元组的前3个值应为1，它们指示核大小以及沿批、高度、宽度维度的步长应为1，最后一个值表示核大小与沿深度维度的步长（两者必须相等），它们必须是输入深度的因数（a divisor）（例如，如果前一层输出20个特征图，则最后一个值不能为3）。如果想要将其作为Keras模型的一层，则用`Lambda`层包装它，或创建一个自定义的Keras层。

```python
# 创建一个自定义的Keras层。

class DepthMaxPool(keras.layers.Layer):
    def __init__(self, pool_size, strides=None, padding="VALID", **kwargs):
        super().__init__(**kwargs)
        if strides is None:
            strides = pool_size
        self.pool_size = pool_size
        self.strides = strides
        self.padding = padding
    def call(self, inputs):
        return tf.nn.max_pool(inputs,
                              ksize=(1, 1, 1, self.pool_size),
                              strides=(1, 1, 1, self.pool_size),
                              padding=self.padding)
```

```python
depth_pool = DepthMaxPool(3)
with tf.device("/cpu:0"): # there is no GPU-kernel yet
    depth_output = depth_pool(cropped_images)
depth_output.shape
```

```python
# 用Lambda层包装。

depth_pool = keras.layers.Lambda(lambda X: tf.nn.max_pool(
    X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding="VALID"))
with tf.device("/cpu:0"): # there is no GPU-kernel yet
    depth_output = depth_pool(cropped_images)
depth_output.shape
```

```python
plt.figure(figsize=(12, 8))
plt.subplot(1, 2, 1)
plt.title("Input", fontsize=14)
plot_color_image(cropped_images[0])  # plot the 1st image
plt.subplot(1, 2, 2)
plt.title("Output", fontsize=14)
plot_image(depth_output[0, ..., 0])  # plot the output for the 1st image
plt.axis("off")
plt.show()
```

在现代CNN体系结构中，还有一种常见的池化层类型：**全局平均池化层（global average pooling layer）**，它计算每个特征图的平均（它就像一个平均池化层，但使用的池化核的空间维度与输入的空间维度相同）。这意味着它只为每个实例的每个特征图输出单个数字。虽然这极具破坏性（特征图中的大多数信息被丢失），但是它可以用作输出层。要创建这样的层，只需要使用`keras.layers.GlobalAvgPool2D`类：

```python
global_avg_pool = keras.layers.GlobalAvgPool2D()
global_avg_pool(cropped_images)
```

它等价于如下的`Lambda`层：

```python
output_global_avg2 = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))
output_global_avg2(cropped_images)
```



### CNN结构

典型的CNN结构堆叠（stack）了一些卷积层（每层后面通常有一个ReLU层），然后是一个池化层，然后是另一些卷积层（+ReLU），然后是另一个池化层，如此下去。随着网络的处理，图像会变得越来越小，但是由于卷积层，它通常会越来越深（即有更多的特征图）。栈（stack）的顶部添加了一个普通的前向神经网络（feedforward neural network），它由一些全连接层（+ReLU）组成，最后一层输出预测值（例如，一个softmax层输出估计的类别概率）。典型的CNN结构如图所示：

![典型的CNN结构](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\典型的CNN结构.png)

一个常见的错误是使用过大的卷积核。流入，与其使用一个有$5\times 5$核的卷积层，不如堆叠两个有$3\times 3$核的层，它将使用更少的参数，需要更少的计算量，并且通常性能更好。一个例外是第一个卷积层通常可以有大的核（例如$5\times 5$），且通常步长为2或更大），这将减少图像的空间维度（spatial dimension）且不会丢失太多信息，并且因为输入图像通常只有三个通道，这也不会花费太多代价。

下面实现一个简单的CNN来处理Fashion MNIST数据集：

```python
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]

X_mean = X_train.mean(axis=0, keepdims=True)
X_std = X_train.std(axis=0, keepdims=True) + 1e-7
X_train = (X_train - X_mean) / X_std
X_valid = (X_valid - X_mean) / X_std
X_test = (X_test - X_mean) / X_std

X_train = X_train[..., np.newaxis]
X_valid = X_valid[..., np.newaxis]
X_test = X_test[..., np.newaxis]
```

```python
from functools import partial

DefaultConv2D = partial(keras.layers.Conv2D,
                        kernel_size=3, activation='relu', padding="SAME")

model = keras.models.Sequential([
    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),
    keras.layers.MaxPooling2D(pool_size=2),
    DefaultConv2D(filters=128),
    DefaultConv2D(filters=128),
    keras.layers.MaxPooling2D(pool_size=2),
    DefaultConv2D(filters=256),
    DefaultConv2D(filters=256),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Flatten(),
    keras.layers.Dense(units=128, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(units=64, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(units=10, activation='softmax'),
])
```

```python
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
score = model.evaluate(X_test, y_test)
X_new = X_test[:10] # pretend we have new images
y_pred = model.predict(X_new)
```

- 第一层使用了64个相当大的过滤器（$7\times 7$）但是没有步长，因为输入图像不是很大。它同时设置`input_shape=[28, 28, 1]`，因为图像是$28\times 28$像素，有一个颜色通道（即灰度（grayscale））。
- 接下来创建了一个最大池化层，使用的池大小（pool size）为2，所以它将每个空间维度（spatial dimension）除以2。
- 然后将相同的结构重复两次：两个卷积层，后面是一个最大池化层。对于更大的图像，可将该结构重复更多次（重复次数是可以调整的超参数）。
- 注意随着接近CNN的输出层，过滤器的数量增加（开始时数量为64，然后128，然后256），这么做是有意义的，因为低层特征常常相当少（例如，小圆圈、水平线），但是有很多不同的方法可以将它们组合为更高层的特征。在每个池化层后将过滤器数量增加一倍是常见的做法，因为池化层将每个空间维度除以2，所以我们可以在下一层中将特征图的数量增加一倍，而不必担心参数数量、内存使用或计算负载爆炸。
- 接下来是一个全连接层，由两个隐藏密集层与一个密集输出层组成。注意必须展平（flatten）它的输入，因为密集网络期望得到每个实例的1维数组。这里还增加了两个dropout层，每个的dropout率都是50%，以防止过拟合。

该CNN在测试集上的准确率超过92%。它不是最先进的，但是已经相当好，并且明显比在[模型构建](#(模型构建))中使用密集网络得到的效果好。

## 网络实例

多年来，以上基本结构的各种变体被开发出来，它们使得该领域取得了巨大进步。度量这一进展的一个好的指标是在ILSVRC [ImageNet challenge](http://image-net.org/)等竞赛中的错误率。在该竞赛中，图像分类的前五名错误率（the top-five error rate)在短短六年间从26%下降到2.3%。前五名错误率指的是系统前五个预测值不包含正确答案的测试图像数量。这些图像很大（高度为256像素），并且有1000个类别，它们中的一些非常微妙（subtle）（尝试区分120种狗）。查看获胜者（the winning entries）的演变是理解CNN如何工作的一个好方法。

### LeNet-5

[LeNet-5结构](https://homl.info/lenet5)也许是最广为人知的CNN结构。它由Yann LeCun创建于1998年，并被广泛用于手写数字识别（MNIST），它由以下层组成：

<table>
    <tr>
        <th>层</th>
        <th>类型</th>
        <th>图（maps）</th>
        <th>大小</th>
        <th>核大小</th>
        <th>步长</th>
        <th>激活（函数）</th>
    </tr>
    <tr>
        <td>Out</td>
        <td>全连接</td>
        <td>-</td>
        <td>10</td>
        <td>-</td>
        <td>-</td>
        <td>RBF</td>
    </tr>
    <tr>
        <td>F6</td>
        <td>全连接</td>
        <td>-</td>
        <td>84</td>
        <td>-</td>
        <td>-</td>
        <td>tanh</td>
    </tr>
    <tr>
        <td>C5</td>
        <td>卷积</td>
        <td>120</td>
        <td>1 × 1</td>
        <td>5 × 5</td>
        <td>1</td>
        <td>tanh</td>
    </tr>
    <tr>
        <td>S4</td>
        <td>平均池化</td>
        <td>16</td>
        <td>5 × 5</td>
        <td>2 × 2</td>
        <td>2</td>
        <td>tanh</td>
    </tr>
    <tr>
        <td>C3</td>
        <td>卷积</td>
        <td>16</td>
        <td>10 × 10</td>
        <td>5 × 5</td>
        <td>1</td>
        <td>tanh</td>
    </tr>
    <tr>
        <td>S2</td>
        <td>平均池化</td>
        <td>6</td>
        <td>14 × 14</td>
        <td>2 × 2</td>
        <td>2</td>
        <td>tanh</td>
    </tr>
    <tr>
        <td>C1</td>
        <td>卷积</td>
        <td>6</td>
        <td>28 × 28</td>
        <td>5 × 5</td>
        <td>1</td>
        <td>tanh</td>
    </tr>
    <tr>
        <td>In</td>
        <td>输入</td>
        <td>1</td>
        <td>32 × 32</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
    </tr>
</table>


一些细节如下：

- MNIST图像为$28\times 28$像素，但是它们被零填充到$32\times 32$像素，并且在提供给网络前被归一化（normalized）。网络的剩余部分不使用任何填充，这就是图像在网络中前进时大小不断收缩的原因。

- 平均池化层比平常稍微复杂些：每个神经元计算输入的均值，然后将结果乘以一个可学习系数（a learnable coefficient）（每个图一个），并加上一个可学习偏置项（同样地，每个图一个），最后应用激活函数。

- C3图（maps）中的大多数神经元仅与三个或四个S2图中的神经元相连（而不是全部6个S2图）。详见[原始论文](https://homl.info/lenet5)表1（第8页）。

- 输出层有一点特殊：每个神经元输出它的输入向量与它的权重向量之间的欧几里得距离的平方，而不是计算输入与权重向量的矩阵乘法。每个输出度量每张图像属于特定数字类别的程度。这里最好使用交叉熵损失函数，因为它对不好的预测惩罚更多，使得梯度更大且收敛更快。。

  

> Most neurons in C3 maps are connected to neurons in only three or four S2 maps (instead of all six S2 maps).

> The cross-entropy cost function is now preferred, as it penalizes bad predictions much more, producing larger gradients and converging faster.

Yann LeCun的[网站](http://yann.lecun.com/exdb/lenet/index.html)提供了LeNet-5分类数字的大量演示。

### AlexNet

[AlexNet CNN结构](https://homl.info/80)以巨大优势赢得了2012 ImageNet ILSVRC挑战：它的前五名错误率达到了17%，而第二名只达到了26%。它由Alex Krizhevsky（由此得名）、Ilya Sutskever与Geoffrey Hinton开发。它类似于LeNet-5，只是更大、更深，并且它是第一个将卷积层直接堆叠到其他卷积层上面的方法（而不是对每个卷积层堆叠一个池化层）。下表展示了它的结构：

<table>
    <tr>
        <th>层</th>
        <th>类型</th>
        <th>图（Maps）</th>
        <th>大小</th>
        <th>核大小</th>
        <th>步长</th>
        <th>填充</th>
        <th>激活（函数）</th>
    </tr>
    <tr>
        <td>Out</td>
        <td>全连接</td>
        <td>-</td>
        <td>4096</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>Softmax</td>
    </tr>
    <tr>
        <td>F10</td>
        <td>全连接</td>
        <td>-</td>
        <td>4096</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>ReLU</td>
    </tr>
    <tr>
        <td>F9</td>
        <td>全连接</td>
        <td>-</td>
        <td>4096</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>ReLU</td>
    </tr>
    <tr>
        <td>S8</td>
        <td>最大池化</td>
        <td>256</td>
        <td>6×6</td>
        <td>3×3</td>
        <td>2</td>
        <td>valid</td>
        <td>-</td>
    </tr>
    <tr>
        <td>C7</td>
        <td>卷积</td>
        <td>256</td>
        <td>13×13</td>
        <td>3×3</td>
        <td>1</td>
        <td>same</td>
        <td>ReLU</td>
    </tr>
    <tr>
        <td>C6</td>
        <td>卷积</td>
        <td>384</td>
        <td>13×13</td>
        <td>3×3</td>
        <td>1</td>
        <td>same</td>
        <td>ReLU</td>
    </tr>
    <tr>
        <td>C5</td>
        <td>卷积</td>
        <td>384</td>
        <td>13×13</td>
        <td>3×3</td>
        <td>1</td>
        <td>same</td>
        <td>ReLU</td>
    </tr>
    <tr>
        <td>S4</td>
        <td>最大池化</td>
        <td>256</td>
        <td>13×13</td>
        <td>3×3</td>
        <td>2</td>
        <td>valid</td>
        <td>-</td>
    </tr>
    <tr>
        <td>C3</td>
        <td>卷积</td>
        <td>256</td>
        <td>27×27</td>
        <td>5×5</td>
        <td>1</td>
        <td>same</td>
        <td>ReLU</td>
    </tr>
    <tr>
        <td>S2</td>
        <td>最大池化</td>
        <td>96</td>
        <td>27×27</td>
        <td>3×3</td>
        <td>2</td>
        <td>valid</td>
        <td>-</td>
    </tr>
    <tr>
        <td>C1</td>
        <td>卷积</td>
        <td>96</td>
        <td>55×55</td>
        <td>11×11</td>
        <td>4</td>
        <td>valid</td>
        <td>ReLU</td>
    </tr>
    <tr>
        <td>In</td>
        <td>输入</td>
        <td>3（RGB）</td>
        <td>227×227</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
    </tr>
</table>



为了降低过拟合风险，作者使用了两个正则化技术：在训练期间将使用了50%dropout率的dropout应用到F9与F1的输出；通过使用不同的偏移量随机移动训练图像、水平翻转图像以及改变图片照明条件来执行**数据增强（data augmentation）**。

AlexNet还在C1层与C3层的ReLU步后使用了一个被称为**局部响应归一化（local response normalization，LRN）**的竞争归一化步（competitive normalization step）：最活跃（the most strongly activated）的神经元会抑制相邻特征图中位于相同位置的其他神经元。这激励不同特征图专一化，使得它们分开，并迫使它们探索更广泛的特征，最终改善泛化能力。下式展示了如何应用LRN：

$$
b_i=a_i(k+\alpha\sum_{j=j_{low}}^{j_{high}}a_j^2)^{-\beta}
$$

$$
\left\{
\begin{aligned}
&j_{high}=\min(i+\frac{r}{2},f_n-1)\\
&j_{low}=\max(0,i-\frac{r}{2})
\end{aligned}
\right.
$$

其中：

- $b_i$为位于特征图$i$的第$u$行、第$v$列的神经元的归一化（normalized）输出（因为该式只考虑位于该行、该列的神经元，所以$u$与$v$不需要显示）。
- $a_i$为ReLU步后、归一化（normalization）步前的神经元的活性（activation）。
- $k$、$\alpha$、$\beta$与$r$为超参数。$k$被称为**偏置（bias）**，$r$被称为**深度半径（depth radius）**。
- $f_n$为特征图数量。

在AlexNet中，超参数设置如下：$r=2$、$\alpha=0.00002$、$\beta=0.75$、$k=1$。该步可以通过`tf.nn.local_response_normalization`函数实现（如果想在Keras模型中使用它，可以用`Lambda`层包装它）。

Matthew Zeiler与Rob Fergus开发了AlexNet的一个变体，被称为[ZF Net](https://homl.info/zfnet)，它赢得了2013 ILSVRC挑战。它本质上是AlexNet，只是一些超参数（特征图的数量、核大小、步长等）被微调。

### GoogLeNet

[GoogLeNet结构](https://homl.info/81)是由来自Google Research的Christian Szegedy等人开发的，它将前五名错误率推至7%以下，从而赢得了ILSVRC 2014挑战。如此出色的性能很大程度上是因为该网络比以前的CNN深得多（如[图1](#(卷积神经网络)(网络实例)(GoogLeNet)(1))所示）。这是由被称为**初始模块（inception modules）**的子网络实现的，它使得GoogLeNet远比以前的结构更能有效地使用参数：GoogLeNet的参数实际上比AlexNet少10倍（大约$6\times10^6$而不是$6\times10^7$）。

下图展示了初始模块的结构。符号（notation）$3\times3+1(S)$表示该层使用$3\times3$核，1的步长与`same`填充。输入信号首先被拷贝并被输送到4个不同的层。所有的卷积层使用ReLU激活函数。注意，第二组卷积层使用不同的核大小（$1\times1$、$3\times3$与$5\times5$），这使得它们能够捕获不同尺度（scales）的模式。同时注意每层使用1的步长与`same`填充最大池化层也是如此），因此，它们的输出都具有与输入相同的高度和宽度，这样就可以在最终的**深度拼接层（depth concatenation layer）**（即将4个顶部的卷积层的特征图堆叠起来）中将所有的输出都沿着深度维度拼接。该拼接层可以使用TensorFlow的`tf.concat`操作（指定`axis=3`，该axis为深度）实现。

![初始模块](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\初始模块.png)

看上去，初始模块使用$1\times1$核的卷积层不能捕获任何特征，因为它们每次只查看一个像素。实际上，这样的层有三个作用：

- 虽然它们不能捕获空间（spatial）模式，但是可以捕获沿深度维度的模式。
- 通过配置，它们输出比输入具有更少的特征图，所以它们充当**瓶颈层（bottleneck layers）**，这意味着它们实现了降维。这会降低计算代价与参数数量，加速训练并改善泛化能力。
- 每对卷积层（$[1\times1,3\times3]$与$[1\times1,5\times5]$）像一个强大的卷积层，它可以捕获更复杂的模式。实际上，这对卷积层不是像单个卷积层那样在图像上扫描一个简单的线性分类器，而是在图像上扫描一个两层神经网络。

简言之，可以将整个初始模块看做一个卷积层，它输出的特征图可以捕获不同尺度下的复杂模式。

每个卷积层的卷积核数量是一个超参数，这意味着对于每个添加的初始层（inception layer），有6个额外的超参数需要微调。

下图是GoogLeNet CNN的结构。每个卷积层与每个池化层输出的特征图数量显示在核大小前。GoogLeNet 实际上是一个高的堆栈（one tall stack），包括9个初始模块（带旋转陀螺的长方形）。初始模块中的6个数字表示该模块中每个卷积层输出的特征图的数量（按照与上图相同的顺序）。注意所有的卷积层都使用ReLU激活函数。


<a name="(卷积神经网络)(网络实例)(GoogLeNet)(1)">![GoogLeNet结构](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\GoogLeNet结构.png)</a>

- 前两个层将输入图片的高度与宽度除以4（所以它的面积除以16），以减少计算负载。第一层使用一个大的核大小，以便保留大部分信息。
- 然后一个局部响应归一化层确保前一层学习到多种多样的特征（如前所述）。
- 随后是两个卷积层，其中第一个层充当一个瓶颈层，如前所述，可以认为这对层为单个更智能（smarter）的卷积层。
- 再次，一个局部响应归一化层确保前一层学习到多种多样的特征。
- 接下来，一个最大池化层将图片高度与宽度除以2，再次加速计算。
- 然后是9个初始模块的高堆栈，它们与两个最大池化层交织，以降维并加速网络。
- 接下来，一个全局平均池化层输出每个特征图的均值：它丢弃了任何剩余的空间（spatial）信息，这很好，因为此时已经没有很多的空间信息了。实际上，GoogLeNet的输入图像通常期望为$224\times 224$像素，因为在5个最大池化层后，特征图已降为$7\times 7$。另外，这是一个分类任务，而不是定位（localization），因此并不关心对象（object）的位置。由于该层带来的降维，CNN的顶层不需要若干全连接层（像AlexNet那样），这显著降低了网络中的参数数量，并限制了过拟合的风险。
- 最后几层是用于正则化的dropout、有1000个单元（units）（因为有1000个类别）的全连接层以及一个softmax激活函数去输出估计的类别概率。

上图稍微简化了一些：原始的GooLeNet结构还包括两个辅助分类器，它们分别插在（plugged）第三个与第六个初始模块的顶部。它们都由一个平均池化层、一个卷积层、两个全连接层以及一个softmax激活层组成。在训练过程中，它们的损失（缩减70%）加到整个损失上。目标是解决梯度消失问题并正则化网络，但是后来表明它们的作用相当小。

Google研究人员后来提出一些GoogLeNet结构的变体，包括Inception-v3与Inception-v4，它们使用一些稍微不同的初始模块，并达到更好的性能。

### VGGNet

ILSVRC 2014挑战的亚军是[VGGNet](https://homl.info/83)，它由来自Oxford大学的Visual Geometry Group（VGG）研究实验室的Karen Simonyan与Andrew Zisserman开发。它的结构十分简单经典，有2或3个卷积层，然后是一个池化层，然后又是2或3个卷积层加一个池化层，如此下去（总的卷积层数量只有16或19，取决于VGG种类），加上一个最终的密集网络，包含2个隐藏层与1个输出层。它只使用$3\times3$过滤器，但是过滤器很多。

### ResNet

Kaiming He等人使用一个[**残差网络（Residual Network）**（或被称为**ResNet**）](https://homl.info/82)赢得了ILSVRC 2015挑战。该网络的前五名错误率低于3.6%。获胜的变体使用了一个非常深的CNN，包含152层（其他变体有34、50与101层）。它证实了总的趋势：模型越来越深，参数越来越少。能够训练这样的深度网络的关键点是使用**跳跃连接（skip connections）**（也被称为**shortcut connections**）：输送到一层中的信号也被加到栈中更高的层的输出。

当训练一个神经网络时，目标是使其对目标函数$h(\pmb{x})$建模。如果将输入$\pmb{x}$加到网络输出（即添加一个跳跃连接），则网络将被迫对$f(\pmb{x})=h(\pmb{x})-\pmb{x}$，而不是$h(\pmb{x})$建模。这被称为**残差学习（residual learning）**（如图）。

![残差学习](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\残差学习.png)

当初始化一个普通的（regular）神经网络，它的权重接近0，所以网络只输出接近0的值。如果增加一个跳跃连接，得到的网络只输出输入的一个拷贝，也就是说，它初始时对恒等函数（identity function）建模。如果目标函数与该恒等函数相当接近（常常会出现这种情况），则这将显著加速训练。

另外，如果添加很多跳跃连接，则网络可以在一些层没有开始学习前就取得进展（如图）。由于跳跃连接的存在，信号可以很容易地通过整个网络。深度残差网络可以看做**残差单元（residual units，RUs）**的堆叠，其中每个残差单元都是一个带有跳跃连接的小的神经网络。

![普通的深度神经网络（左）与深度残差网络（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\普通的深度神经网络（左）与深度残差网络（右）.png)

下图是ResNet的结构，它简单地令人惊讶。它的开始与结束与GoogLeNet一样（只是没有dropout层），中间是简单残差单元的非常深的堆叠。每个残差单元由两个卷积层组成（没有池化层），使用$3\times3$核并保留空间（spatial）维度（步长为1，`same`填充），进行批标准化并使用ReLU激活。

![ResNet结构](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\ResNet结构.png)

注意，每经过几个（every few）残差单元，特征图的数量都加倍，同时它们的高度与宽度都减半（使用步长为2的卷积层）。在这种情况下，输入不能直接加到残差单元的输出，因为它们的形状不同（例如，该问题影响上图中虚线箭头表示的跳跃连接）。为了解决这个问题，输入通过一个$1\times1$的卷积层，它的步长为2，具有正确（right）数量的输出特征图（如图）。

<a name="(卷积神经网络)(网络实例)(ResNet)(1)">![更改特征图大小与深度时的跳跃连接](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\更改特征图大小与深度时的跳跃连接.png)</a>

ResNet-34为有34层（只考虑卷积层与全连接层）的ResNet，它包含3个输出64个特征图的残差单元、4个输出128个特征图的残差单元、6个输出256个特征图的残差单元，以及3个输出512个特征图的残差单元。

比ResNet-34深的ResNet，例如ResNet-152，使用稍微不同的残差单元。它们使用3个卷积层，而不是两个$3\times3$的卷积层（假设有256个特征图）：第一个是有64个特征图的$1\times1$的卷积层，它充当一个瓶颈层；然后是一个有64个特征图的$3\times3$的层；最后是另一个有256个特征图的$1\times1$的卷积层，它恢复原始深度。ResNet-152包含3个这样的残差单元，它们输出256个特征图；然后是8个残差单元，输出512个特征图，以及36个残差单元输出1024个特征图，最后是3个残差单元输出2048个特征图。

Google的[Inception-v4](https://homl.info/84)结构融合了GoogLeNet与ResNet的思想，并在ImageNet分类中取得了接近3%的前五名错误率。

### Xception

另一个GoogLeNet的变体[Xception](https://homl.info/xception)（代表**Extreme Inception**）由François Chollet（Keras的作者）发表于2016年，它在一个大的视觉任务（a huge vision task）（3亿5000万张图片，17000个类别）上的表现显著超过 Inception-v3。与Inception-v4一样，它融合了GoogLeNet与ResNet的思想，但是它用一个被称作**深度可分离卷积层（depthwise separable convolution layer）**（也被简称为**可分离卷积层（separable convolution layer）**）的特殊类型的层代替了初始模块。这些层以前在某些CNN结构中使用过，但是它们不如在Xception结构中那么重要。一个普通的卷积层使用过滤器去同时捕获空间（spatial）信息（例如一个椭圆）与跨通道模式（例如嘴巴+鼻子+眼睛=脸），一个可分离卷积层作出一个更强的假设：空间模式与跨通道模式可被分别建模（如图）。这样，它由两个部分组成：部分一对每个输入特征图应用一个空间过滤器，然后部分二专门查找跨通道模式——它只是一个带有$1\times1$过滤器的普通卷积层。

![深度可分离卷积层](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\深度可分离卷积层.png)

一些可分离卷积层的每个输入通道只有一个空间（spatial）过滤器，应该避免在通道数过少的层后使用它们，例如输入层（上图就是这样的情况，但是它仅作演示目的）。出于该原因，Xception结构起始于2个普通的卷积层，但结构的其余部分只使用可分离卷积（separable convolutions）（总共34个），再加上一些最大池化层与常规的最终层（the usual final layers）（一个全局平均池化层与一个密集输出层）。

Xception不包含任何初始模块，但是它被认为是GoogLeNet的变体的原因在于：一个初始模块包含一个具有$1\times1$过滤器的卷积层，它们专门查找跨通道模式；但是一个位于它们上面的卷积层是普通的卷积层，它们同时查找空间与跨通道模式。所以可以认为一个初始模块为一个普通卷积层（同时考虑空间模式与跨通道模式）与一个可分离卷积层（分别考虑这两个模式）的中介。在实践中，看起来可分离卷积层通常表现更好。

可分离卷积层比普通的卷积层使用更少的参数、内存与计算量，通常它们甚至表现得更好，所以在默认情况下，应该考虑使用它们（除了在通道数很少的层后）。


ILSVRC 2016挑战的获胜者是来自香港中文大学的GUImage团队。它们使用了许多不同技术的集成，包括一个被称作[GBD-Net](https://homl.info/gbdnet)的复杂的（sophisticated）目标检测系统，使得前五名错误率低于3%。尽管这一结果无疑令人印象深刻，但是这一解决方案的复杂性（complexity）与ResNet的简洁性（simplicity）形成鲜明对比。另外，一年后另一个相当简单的结构表现得更好，如下。

### SENet

ILSVRC 2017挑战的获胜结构是[Squeeze-and-Excitation Network (SENet)](https://homl.info/senet)。该结构扩展了已有的结构，例如初始网络（inception networks）与ResNets，并且提升了它们的性能。这使得SENet以令人震惊的2.25%的前五名错误率赢得了该竞赛。初始网络与ResNets的扩展版本分别被称为**SE-Inception**与**SE-ResNet**。该提升来自以下事实：一个SENet给原始结构中的每个单元（即每个初始模块或每个残差单元）添加一个被称为**SE块（SE block）**的小型神经网络，如图：

![SE-Inception模块（左）与SE-ResNet单元（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\SE-Inception模块（左）与SE-ResNet单元（右）.png)

一个SE块分析它所依附的单元的输出，它只关注深度维度（它不查找任何模式），并学习哪些特征通常活性最大。然后它使用这一信息去重新校准特征图，如图所示。例如，一个SE块可能学习到嘴巴、鼻子与眼睛通常在图片中同时出现，如果看到一个嘴巴与一个鼻子，则应该期望看到眼睛。所以如果块看到嘴巴与鼻子特征图中有一个强活性（strong activation），但是眼睛特征图中只有轻度活性（wild activation），它将增强眼睛特征图（更准确地说，它会减少不相关的特征图）。如果眼睛与其他东西有点混淆，则特征图重新校准有助于解决歧义。

> if you see a mouth and a nose, you should expect to see eyes as well.

![一个SE块执行特征图重新校准](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个SE块执行特征图重新校准.png)

一个SE块仅由三层组成：一个全局平均池化层、一个使用ReLU激活函数的隐藏密集层与一个使用sigmoid激活函数的密集输出层（如图）。

![SE块结构](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\SE块结构.png)

如前，全局平均池化层计算每个特征图的活性平均值（the mean activation）。例如，如果输入包含256个特征图，则它将输出256个数字去代表每个过滤器的总体响应水平（the overall level of response）。下一层就是“squeeze”发生的地方：该层的神经元数量明显少于256个，通常比特征图数量少16倍（例如16个神经元），所以这256个数字被压缩为一个小的向量（例如，16维）。这是一个特征响应分布的低维向量表示（即一个嵌入（embedding））。这个瓶颈步迫使SE块学习特征组合的一般表示（a general representation）。最后，输出层接受这个嵌入，并输出一个重新校准向量，其中每个特征图包含一个介于0\~1之间的数字。特征图入随后乘以该重新校准向量，所以不相关的特征（有一个低重新校准得分）被缩小而相关特征（有一个高重新校准得分）保持不变。

#### 网络实现

下面使用Keras从头实现一个ResNet-34。首先创建一个`ResidualUnit`：

```python
DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,
                        padding="SAME", use_bias=False)

class ResidualUnit(keras.layers.Layer):
    def __init__(self, filters, strides=1, activation="relu", **kwargs):
        super().__init__(**kwargs)
        self.activation = keras.activations.get(activation)
        self.main_layers = [
            DefaultConv2D(filters, strides=strides),
            keras.layers.BatchNormalization(),
            self.activation,
            DefaultConv2D(filters),
            keras.layers.BatchNormalization()]
        self.skip_layers = []
        if strides > 1:
            self.skip_layers = [
                DefaultConv2D(filters, kernel_size=1, strides=strides),
                keras.layers.BatchNormalization()]

    def call(self, inputs):
        Z = inputs
        for layer in self.main_layers:
            Z = layer(Z)
        skip_Z = inputs
        for layer in self.skip_layers:
            skip_Z = layer(skip_Z)
        return self.activation(Z + skip_Z)
```

以上代码与[图1](#(卷积神经网络)(网络实例)(ResNet)(1))相当匹配。在构造器中，我们创建了所有所需的层：主层（main layers）位于图右边，跳跃层位于图左边（只有当步长大约1时才需要）。然后在`call`方法中，我们使输入经过主层与跳跃层（如果有的话），然后将两个输出加起来并应用激活函数。

下面可以使用`Sequential`模型构建ResNet-34，因为它只是层的一个长序列（可以将残差单元看做一个层，因为已经有了`ResidualUnit`类）：

```python
model = keras.models.Sequential()
model.add(DefaultConv2D(64, kernel_size=7, strides=2,
                        input_shape=[224, 224, 3]))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Activation("relu"))
model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding="SAME"))
prev_filters = 64
for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
    strides = 1 if filters == prev_filters else 2
    model.add(ResidualUnit(filters, strides=strides))
    prev_filters = filters
model.add(keras.layers.GlobalAvgPool2D())
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(10, activation="softmax"))
```

```python
model.summary()
```

代码中稍微棘手的部分是像模型中添加`ResidualUnit`层的循环：其中，前三个RUs有64个过滤器，然后是4个有128个过滤器的RUs，等等。当过滤器数量与前一RU层中的过滤器的数量相同时，设置步长为1，否则设置它为2。然后增加`ResidualUnit`，最后更新`prev_filters`。

这样只用少于40行的代码，我们就构建了赢得ILSVRC 2015挑战的模型。这展示了ResNet的优雅性（elegance）与Keras API的表达性（expressiveness）。

### 使用来自Keras的预训练模型

通常不必去手动实现诸如GoogLeNet或ResNet这样的标准模型，因为只需要`keras.applications`包中的一行代码就能获得预训练网络。例如，可以使用如下代码行加载在ImageNet上预训练的ResNet-50模型：

```python
model = keras.applications.resnet50.ResNet50(weights="imagenet")
```

它创建了一个ResNet-50模型并下载在ImageNet数据集上预训练的权重。为了使用它，首先需要确保图片有正确的尺寸。ResNet-50模型期望$224\times224$像素的图像（其他模型可能期望其他尺寸，例如$299\times299$），所以这里使用TensorFlow的`tf.image.resize`函数去调整之前加载的图像的尺寸：

```python
images_resized = tf.image.resize(images, [224, 224])
plot_color_image(images_resized[0])
plt.show()
```

`tf.image.resize`不会保留纵横比。如果这会造成问题，尝试在调整图像尺寸前将图像裁剪为合适的纵横比。可以使用`tf.crop_and_resize`一次性完成这两个操作。

```python
images_resized = tf.image.resize_with_pad(images, 224, 224, antialias=True)
plot_color_image(images_resized[0])
```

```python
images_resized = tf.image.resize_with_crop_or_pad(images, 224, 224)
plot_color_image(images_resized[0])
plt.show()
```

```python
china_box = [0, 0.03, 1, 0.68]
flower_box = [0.19, 0.26, 0.86, 0.7]
images_resized = tf.image.crop_and_resize(images, [china_box, flower_box], [0, 1], [224, 224])
plot_color_image(images_resized[0])
plt.show()
plot_color_image(images_resized[1])
plt.show()
```

预训练模型假定图像用特定方式预处理过。在某些情况下，它们可能期望输入被放缩到0\~1，或-1\~1，等等。每个模型都提供了一个`preprocess_input`函数用来预处理图像。这些函数假定像素值介于0\~255之间，因此必须将它们乘以255（因为之前它们被缩放到0\~1范围间）。

```python
inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)
```

接下来就可以使用预训练的模型进行预测了：

```python
Y_proba = model.predict(inputs)
```

```python
Y_proba.shape
```

与往常一样，输出`Y_proba`为一个矩阵，其中每个图像一行，每个类别一类。如果要展示前`K`个预测，包括类别名与每个预测类别（predicted class）的估计概率，使用`decode_predictions`函数。对于每张图像，它返回一个数组，包含前`K`个预测，其中每个预测表示为一个数组，它包含类别标识符（identifier）（在ImageNet数据集中，每张图片关联[WordNet dataset](https://wordnet.princeton.edu/)中的一个单词，类别ID就是WordNet ID）、名称与对应的置信得分：

```python
top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)
for image_index in range(len(images)):
    print("Image #{}".format(image_index))
    for class_id, name, y_proba in top_K[image_index]:
        print("  {} - {:12s} {:.2f}%".format(class_id, name, y_proba * 100))
    print()
```

两张图像的正确类别（monastery与daisy）都出现在前三个结果中。考虑到模型需要从1000个类别中进行选择，这样的结果相当不错。

可以看到，使用预训练模型创建一个相当不错的图像分类器非常简单。`keras.applications`提供了其他视觉模型（vision models），包括一些ResNet的变体、GoogLeNet的变体（例如Inception-v3与Xception）、VGGNet的变体、MobileNet与MobileNetV2（用于移动应用中的轻量级模型）。

### 用于迁移学习的预训练模型

如果要将图像分类器用于不属于ImageNet的图像类别，则仍然可以通过执行迁移学习而从预训练模型中受益。

如果想要构建一个图像分类器但是没有足够的训练集，则重用预训练模型的低层常常是一个好主意。下面通过重用预训练的Xception模型，训练一个模型去分类花的图片（pictures of flowers）。首先使用TensorFlow Datasets加载数据集：

```python
import tensorflow_datasets as tfds

dataset, info = tfds.load("tf_flowers", as_supervised=True, with_info=True)  # 通过设置with_info=True来获得数据集的信息。
```

```python
info.splits
```

```python
info.splits["train"]
```

```python
class_names = info.features["label"].names
class_names
```

```python
n_classes = info.features["label"].num_classes
```

```python
dataset_size = info.splits["train"].num_examples
dataset_size
```

这里只有一个`train`数据集，没有测试集与验证集，因此需要划分训练集。TF Datasets项目为此提供了一个API。下面将数据集的前10%用于训练，接下来的15%用于验证，其余的75%用于训练。

```python
test_set_raw, valid_set_raw, train_set_raw = tfds.load(
    "tf_flowers",
    split=["train[:10%]", "train[10%:25%]", "train[25%:]"],
    as_supervised=True)
```

```python
plt.figure(figsize=(12, 10))
index = 0
for image, label in train_set_raw.take(9):
    index += 1
    plt.subplot(3, 3, index)
    plt.imshow(image)
    plt.title("Class: {}".format(class_names[label]))
    plt.axis("off")

plt.show()
```

下面必须预处理这些图像。CNN期望$224\times224$的图像，因此需要调整它们的尺寸。同时需要使用Xception的`preprocess_input`函数处理图像。

```python
def preprocess(image, label):
    resized_image = tf.image.resize(image, [224, 224])
    final_image = keras.applications.xception.preprocess_input(resized_image)
    return final_image, label
```

如果要执行数据增强（data augmentation），则改变用于训练集的预处理函数，并向训练图像上添加一些随机转换（transformation）。例如，使用`tf.image.random_crop`去随机裁剪图形，使用`tf.image.random_flip_left_right`去随机水平翻转（flip）图像，等等：

```python
def central_crop(image):
    shape = tf.shape(image)
    min_dim = tf.reduce_min([shape[0], shape[1]])
    top_crop = (shape[0] - min_dim) // 4
    bottom_crop = shape[0] - top_crop
    left_crop = (shape[1] - min_dim) // 4
    right_crop = shape[1] - left_crop
    return image[top_crop:bottom_crop, left_crop:right_crop]

def random_crop(image):
    shape = tf.shape(image)
    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100
    return tf.image.random_crop(image, [min_dim, min_dim, 3])

def preprocess(image, label, randomize=False):
    if randomize:
        cropped_image = random_crop(image)
        cropped_image = tf.image.random_flip_left_right(cropped_image)
    else:
        cropped_image = central_crop(image)
    resized_image = tf.image.resize(cropped_image, [224, 224])
    final_image = keras.applications.xception.preprocess_input(resized_image)
    return final_image, label
```

下面将该预处理函数应用到所有三个数据集上，混洗（shuffle）训练集并将批与预取添加到所有数据集上（使用一个稍微复杂的预处理步骤）：

```python
batch_size = 32
train_set = train_set_raw.shuffle(1000).repeat()
train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)
valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)
```

```python
plt.figure(figsize=(12, 12))
for X_batch, y_batch in train_set.take(1):
    for index in range(9):
        plt.subplot(3, 3, index + 1)
        plt.imshow(X_batch[index] / 2 + 0.5)
        plt.title("Class: {}".format(class_names[y_batch[index]]))
        plt.axis("off")

plt.show()
```

```python
plt.figure(figsize=(12, 12))
for X_batch, y_batch in test_set.take(1):
    for index in range(9):
        plt.subplot(3, 3, index + 1)
        plt.imshow(X_batch[index] / 2 + 0.5)
        plt.title("Class: {}".format(class_names[y_batch[index]]))
        plt.axis("off")

plt.show()
```

下面加载一个在ImageNet上预训练的Xception模型。这里通过设置`include_top=False`来移除网络顶层，这会移除全局平均池化层与密集输出层。然后添加自定义的全局平均池化层（基于基模型的输出），随后是一个密集层，其中每个类别一个输出，并使用softmax激活函数。最后，创建Keras `Model`：

```python
base_model = keras.applications.xception.Xception(weights="imagenet",
                                                  include_top=False)
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
output = keras.layers.Dense(n_classes, activation="softmax")(avg)
model = keras.models.Model(inputs=base_model.input, outputs=output)
```

```python
for index, layer in enumerate(base_model.layers):
    print(index, layer.name)
```

如前所述，冻结预训练层的权重通常是个好主意（至少在训练开始时）：

```python
for layer in base_model.layers:
    layer.trainable = False
```

因为模型直接使用基模型的层，而不是`base_model`对象本身，所以设置`base_model.trainable=False`不会有效果。

最后，编译模型并开始训练。

```python
optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(train_set,
                    steps_per_epoch=int(0.75 * dataset_size / batch_size),
                    validation_data=valid_set,
                    validation_steps=int(0.15 * dataset_size / batch_size),
                    epochs=5)
```

当模型训练几代后，它的验证准确率应该达到大约75%到80%并且不再取得很大进展。这意味着顶层（the top layers）现在已得到很好的训练，因此现在已准备好去解冻（unfreeze）所有层（或者也可以尝试只解冻一些顶层）并继续训练（冻结或解冻层时，不要忘记编译模型）。这次使用一个低得多的学习率以避免破坏预训练权重。

> This means that the top layers are now pretty well trained,

```python
for layer in base_model.layers:
    layer.trainable = True

optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,
                                 nesterov=True, decay=0.001)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(train_set,
                    steps_per_epoch=int(0.75 * dataset_size / batch_size),
                    validation_data=valid_set,
                    validation_steps=int(0.15 * dataset_size / batch_size),
                    epochs=40)
```

这将花费一些时间，但是模型应该能够在测试集上达到大约95%的准确率。

## 分类与定位

定位图像中的一个对象可以看做分类任务：预测对象周围的一个边界框，常用的方法是预测物体中心的水平与垂直坐标，以及它的高度与宽度。这意味着要预测4个数字。这不需要改变以上模型，只需要添加第二个密集层，它包含4个单元（通常在全局平均池化层上），模型可用MSE损失训练：

```python
base_model = keras.applications.xception.Xception(weights="imagenet",
                                                  include_top=False)
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
class_output = keras.layers.Dense(n_classes, activation="softmax")(avg)
loc_output = keras.layers.Dense(4)(avg)
model = keras.models.Model(inputs=base_model.input,
                           outputs=[class_output, loc_output])
model.compile(loss=["sparse_categorical_crossentropy", "mse"],
              loss_weights=[0.8, 0.2], # depends on what you care most about
              optimizer=optimizer, metrics=["accuracy"])
```

但是问题在于：花数据集在花的周围没有边界框，因此需要手动添加它们。这常常是机器学习项目最难也是代价最高的部分之一：获取标签。花费时间寻找合适的（right）工具是一个好主意。为了标注图像的边界框，可以使用开源图像标记工具（image labeling tool），例如VGG Image Annotator、LabelImg、OpenLabeler、ImgLab，或者像LabelBox或Supervisely这样的商业工具（commercial tool）。如果有巨量的图像需要标注，还可以考虑众包（crowdsourcing）平台，例如Amazon Mechanical Turk。但是，建立一个众包平台、准备发送给工人（worker）的表格（form）、监督他们并确保他们生成的边界框的质量良好，需要相当大的工作量，所以确保这是值得努力的。如果只有几千张图片去标记（label），且不打算经常这样做，则自己去做可能更好。

假设已经得到了花数据集中每张图像的边界框（假设每张图像只有一个边界框）。然后需要创建建一个数据集，它的每项为预处理后的图像的批、类别标签与边界框。每项应该为一个元组，形式为`(images, (class_labels, bounding_boxes))`。然后就准备好去训练模型了。

下面是一段示例代码：

```python
def add_random_bounding_boxes(images, labels):
    fake_bboxes = tf.random.uniform([tf.shape(images)[0], 4])
    return images, (labels, fake_bboxes)

fake_train_set = train_set.take(5).repeat(2).map(add_random_bounding_boxes)
```

```python
model.fit(fake_train_set, steps_per_epoch=5, epochs=2)
```

注意边界框需要被归一化（normalized），以使得水平与垂直坐标，以及高度与宽度的范围都为0\~1。同时，常见的做法是预测高度与宽度的平方根而不是直接预测高度与宽度，这样，一个大边界框的10像素误差的惩罚就不会像一个小边界框的10像素误差的惩罚那样大。

MSE常常是训练该模型的一个相当好的损失函数，但是它不是评估模型能够多好地预测边界框的一个很好的指标。对此，最常见的指标为**交并比（Intersection over Union，IoU）**，它等于预测边界框与目标边界框的重叠部分的面积除以它们的并集部分的面积（如图）。在tf.keras中，它由`tf.keras.metrics.MeanIoU`类实现。

![用于编辑框的交并比（IoU）指标](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\用于编辑框的交并比（IoU）指标.png)

### 目标检测

分类并定位一张图像中的多个对象（objects）的任务被称为**目标检测（object detection）**。直到几年前，一种常见的方法是使用CNN去分类并定位单个对象，然后在图像上滑动它，如图所示。在该例中，图像被切割成一个$6\times8$的网格，这里展示了一个CNN（厚黑色矩形）在所有的$3\times3$区域上滑动。当CNN查看左上角的图像时，它检测到最左边的玫瑰的一部分，然后当它第一次向右边移动一步时，它再次检测到该玫瑰。下一步，它开始检测到最上边的玫瑰的一部分，然后当它再次向右移动一步时，它再次检测到该玫瑰。可以继续在整张图像上滑动该CNN，查看所有的$3\times3$区域。另外，因为对象的大小各异，因此需要在不同大小的区域上滑动CNN。例如，完成$3\times3$区域的工作后，还可能需要在所有$4\times4$区域上滑动CNN。

<a name="(卷积神经网络)(目标检测)(1)">![通过在图像上滑动一个CNN来检测多个对象](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\通过在图像上滑动一个CNN来检测多个对象.png)</a>

该技术相当简单（straightforward），但是它会多次检测相同对象，只是位置稍微不同。然后需要一些后处理（post-processing）以去除所有不必要的边界框。为此，一种常见的方法被称为**非极大值抑制（non-max suppression）**，如下：

- 首先需要添给CNN加一个额外的**objectness**输出，以估计图像中确实有一个对象（例如花）的概率（也可以添加一个“没有花”类别（“no-flower” class），但是它通常工作得不好）。它必须使用sigmoid激活函数，可以使用二元交叉熵损失训练它。然后去除所有objectness得分小于某个阈值的边界框，这会丢弃所有实际上不包含花的边界框。
- 寻找objectness得分最高的边界框，并去除所有与其大量重叠的其他边界框（例如，IoU大于60%的边界框）。例如，在[图1](#(卷积神经网络)(目标检测)(1))中，具有最大的objectness得分的边界框是外语最顶部玫瑰上的厚边界框。另一个位于该玫瑰上的边界框与该边界框大量重叠，因此需要去除它。
- 重复以上两步直到没有更多的需要去除的边界框。

这个简单的目标检测方法工作得相当好，但是它需要多次运行CNN，所以它相当慢。

### 全卷积网络

Jonathan Long等人在[2015年的一篇论文](https://homl.info/fcn)中首次提出了**全卷积网络（Fully Convolutional Networks，FCNs）**的概念，并用于语义分割。作者指出可以用卷积层（layers）替代CNN顶部的密集层（layers）。假设一个密集层有200个神经元，位于一个卷积层的上面，该卷积层输出100个特征图，每个大小为$7\times7$（这是特征图大小，而不是核大小）。每个神经元会计算来自卷积层的所有$100\times7\times7$个活性（activations）的一个加权和（加上一个偏置项）。如果将密集层替换为一个卷积层，它有200个过滤器，每个大小为$7\times7$，使用`valid`填充。该层将输出200个特征图，每个都是$1\times1$（因为核大小等于输入特征图的大小，且使用`valid`填充）。换言之，它将输出200个数字，就像密集层所做的那样。可以发现这些数字与密集层产生的数字相同，唯一的区别是密集层输出形状为$[batch\ size,200]$的张量，而卷积层输出形状为$[batch\ size,1,1,200]$的张量。

为了将一个密集层转换为一个卷积层，卷积层的过滤器的数量必须等于密集层中单元的数量，过滤器大小必须等于输入特征图的大小，且必须使用`valid`填充，而步长可以为1或更大。

因为一个密集层期望一个特定输入大小（因为对于每个输入特征，它只有一个权重），而一个卷积层可以处理任意大小的图像（一个例外是：如果一个卷积层使用`valid`填充，且输入大小小于核大小，则报错。且它期望它的输入有特定数量的通道，因为每个核为每个输入通道包含一组不同的权重）。因为一个FCN只包含卷积层（以及池化层，它拥有相同的属性（property）），它可以在任意大小的图像上训练并执行。

例如，假设已经训练了一个CNN用于花的分类与定位。它在$224\times224$的图像上训练，并输出10个数字：输出0\~4通过softmax激活函数并产生类别概率（每个类别一个）；输出5通过logistic激活函数并产生objectness得分；输出6\~9没有使用任何激活函数，它们达标边界框的中心坐标以及它的高度与宽度。接下来就可以将它的密集层转换为卷积层。实际上，我们甚至不需要重新训练它，只需要将来自密集层（layers）的权重拷贝给卷积层即可。当然，也可以在训练前将该CNN转换为FCN。

现在假设当将$224\times224$的图像输入网络（如左图）时，位于输出层前的最后一个卷积层（也被称为瓶颈层）输出$7\times7$的特征图。如果将$448\times448$的图像输入该FCN（如右图），则瓶颈层现在将输出$14\times14$特征图（假设网络中只使用`same`填充：实际上，`valid`填充会减小特征图的大小。另外，448可以被2整除若干次，直到到达7，并且不会有任何舍入误差。如果任何一层使用了不等于1或2的步长，则将会有舍入误差，因此可能会再次导致特征图变小）。因为密集输出层被一个使用10个$7\times7$特征图的卷积层代替，该卷积层使用`valid`填充，步长为1，输出将由10个特征图组成，每个大小都是$8\times8$。换言之，FCN会一次性处理整张图像，并且将输出一个$8\times8$的网格，其中每个单元都包含10个数字（5个类别概率，1个objectness得分以及4个边界框坐标）。这与使用原始的CNN并在图像上滑动它，其中每行8步、每列8步。想象一下，原始图像被切割为一个$14\times14$的网格，然后在网格上滑动一个$7\times7$的窗口，窗口有$8\times8=64$个可能的位置，因此有$8\times8$个预测。然而，FCN方法高效得多，因为网络只查看图像一次。

![相同的全卷积网络处理小图像（左）与大图像（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\相同的全卷积网络处理小图像（左）与大图像（右）.png)

### You Only Look Once

**You Only Look Once（YOLO）**是Joseph Redmon等人在[2015年的一篇论文](https://homl.info/yolo)中提出的一种非常流行的，且非常快速、准确的目标检测结构，随后[在2016年](https://homl.info/yolo2)（YOLOv2）与[2018年](https://homl.info/yolo3)（YOLOv3）被改进。它如此之快，可以在视频上实时运行，见Redmon的[演示](https://homl.info/yolodemo)。

YOLOv3的结构与上述结构相当相似，但是有一些重要区别：

- 对于每个网格单元，它输出5个边界框（而不是一个），每个边界框都有一个objectness得分。对于每个网格单元，它同时输出20个类别得分，因为它在PASCAL VOC数据集上被训练，该数据集包含20个类别。这样每个网格单元输出45个数字：5个边界框，每个都有4个坐标，加上5个objectness得分，加上20个类别概率。
- YOLOv3预测相对于网格单元的一个偏移，其中$(0,0)$表示该单元的左上角，$(1,1)$表示该单元的右下角，而不是预测边界框中心的绝对坐标。对于每个网格单元，YOLOv3经过训练，只预测中心位于该单元的边界框（但是边界框本身通常延伸到该网格单元外）。YOLOv3对边界框坐标应该logistic激活函数以确保它们位于0\~1之间。
- 在训练神经网络前，YOLOv3找到5个具有代表性的边界框维度，它们被称为**锚框（anchor boxes）**（或**先验框（bounding box priors）**）。它通过将K-Means算法应用到训练集边界框的高度与宽度上来实现这点。例如，如果训练图像包含许多行人，则其中一个锚框可能具有一个典型行人的维度（dimensions）。然后当神经网络为每个网格单元预测5个边界框时，它实际上预测了每个边界框的重缩放（rescale）量。例如，假设一个锚框100个像素高、50个像素宽，假设网络预测一个垂直重缩放因子（rescaling factor）为1.5、一个水平重缩放因子为0.9（对于其中一个网格单元）。这将产生一个大小为$150\times45$像素的边界框。更准确地说，对于每个网格单元与每个锚框，网络预测水平与垂直缩放因子的对数。有了这些先验知识，网络就更有可能预测到适当维度的边界框，并且它能加速训练，因为它更快地学习到合理的边界框是什么样的。
- 网络使用不同尺度（scales）的图像进行训练：训练过程中，每过几批，网络随机选择一个新的图像维度（从$330\times330$到$608\times608$像素）。这使得网络从不同尺度检测对象。另外，这使得在不同尺度使用YOLOv3成为可能：相比于大尺度，小尺度的准确率较低但是速度更快，因此需要根据情况作出权衡。

其他的一些创新点包括：使用跳跃连接去恢复在CNN中丢失的一些空间分辨率（spatial resolution）（在[语义分割](#(卷积神经网络)(语义分割))中将提到）等。在2016年的论文中，作者提出了YOLO9000模型，它使用层次分类（hierarchical classification）：该模型为一个被称为**WordTree**的视觉层次中的每个结点预测一个概率。这使得网络以高置信度（high confidence）预测图像所代表的对象（例如一条狗）成为可能，即使它不确定狗的特定类型。

## <a name="(卷积神经网络)">语义分割</a>

在**语义分割（semantic segmentation）**中，每个像素根据其所属的对象类别被分类，如图所示。注意属于同一类别的不同对象不被区分。例如，所有位于被分割图像右侧的自行车最终成为一个大的像素块。该任务的主要难点在于当图像经过一个常规的CNN后，它们逐渐丢失它们的空间分辨率（因为步长大于1的层），所以，一个常规的CNN可能最终直知道图像左下角有一个人，但是不知道更详细的信息。

![语义分割](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\语义分割.png)

就像目标检测一样，有许多不同的方法处理该问题，一些方法相当复杂。然而，Jonathan Long等人在2015年的论文（前面提到过）中提出了一个相当简单的解决方案。作者首先采用了一个预训练过的CNN并将其转换为一个FCN。该CNN对输出图像应用一个32的总步长（即，将所有大于1的步长相加的结果），这意味着最后一层输出一个比输入图像小32倍的特征图。显然，这太粗糙，因此它们加了一个**上采样层（upsampling layer）**将分辨率乘以32。

> However, a fairly simple solution was proposed in the 2015 paper by Jonathan Long et al. we discussed earlier.

> The CNN applies an overall stride of 32 to the input image (i.e., if you add up all the strides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than the input image.

有几种解决方案可用于上采样（增加一张图像的大小），例如双线性插值（bilinear interpolation），但是通常最终只能$\times4$或$\times8$。相反，他们使用了一个**反卷积层（transposed convolutional layer）**：它相当于首先通过插入空行与空列（全部为0）拉伸图像，然后执行一个常规的卷积（如图）。或者，有些人更喜欢将其看做一个使用分数（fractional）步长（例如，图中的$1/2$）的常规的卷积层。反卷积层可被初始化以执行类似于线性插值的操作，但是因为它是可训练的层，因此它可以在训练中学习去做得更好。在tf.keras中，可以使用`Conv2DTranspose`做到这点。

![使用反卷积层上采样](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用反卷积层上采样.png)

注意，在反卷积层中，步长定义了输入被拉伸的程度，而不是过滤器的步长，所以步长越大，输出越大（与卷积层与池化层不同）。

这个解决方法可行，但是还是太不精确。为了做到更好，作者从较低层加了一些跳跃连接。例如，它们将输出图像上采样2倍（而不是32），并与有两倍分辨率的较低层的输出相加。然后它们将结果上采样16倍，导致总上采样了32倍（如图）。这恢复了一些之前池化层丢失的空间分辨率。在他们最好的结构中，他们使用了第二个类似的跳跃连接从更低层恢复更精细的细节。简言之，原始CNN的输出经过以下额外步骤：尺寸$\times2$（upscale $\times2$），加上一个（尺寸（scale）合适的）较低层的输出，尺寸$\times2$，加上一个更低层的输出，最后尺寸$\times8$。甚至可以将图像放大到超过原始图像大小，这可以用来增加原始图像的分辨率，该技术被称为**超分辨率（super-resolution）**。

![使用跳跃连接从较低层恢复一些空间分辨率](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用跳跃连接从较低层恢复一些空间分辨率.png)

# 循环神经网络

## 网络结构

### 循环神经元与层

循环神经网络非常像一个前向神经网络（feedforward neural network）（激活函数只从输入层向输出层流动，当然，有一些例外情况），只是它也有指向往回指的连接。最简单的RNN仅由一个接受输入的神经元组成，它产生一个输出，并将其发送给自己，如左图所示。在每个**时间步（time step）**（也被称为**帧（frame）**）$t$，该**循环神经元（recurrent neuron）**接受输入$\pmb{x}_{(t)}$与前一时间步$y_{t-1}$自身的输出。因为在第一个时间步没有以前输出，所以此时它通常设置为0。可以用时间轴表示这个微小的网络，如右图所示。这被称为**通过时间展开网络（unrolling the network through time）**。

<a name="(循环神经网络)(网络结构)(循环神经元与层)(1)">![一个循环神经元（左）通过时间展开（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个循环神经元（左）通过时间展开（右）.png)</a>

可以轻松创建一个循环神经元层（a layer of recurrent neurons）。在每个时间步$t$，每个神经元同时接受输入向量$\pmb{x}_{(t)}$与前一时间步$\pmb{y}_{(t-1)}$的输出向量，如图所示。注意输入与输出现在都是向量（当只有单个神经元时，输出为标量）。

![一个循环神经元层（左）通过时间展开（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个循环神经元层（左）通过时间展开（右）.png)

每个循环神经元有两组权重，一个用于输入$\pmb{x}_{(t)}$，另一个用于前一个时间步$\pmb{y}_{(t-1)}$的输出。这里将这些权重向量称为$\pmb{w}_x$与$\pmb{w}_y$。如果要考虑整个循环层而不是单个神经元，可以将所有权重向量放在两个权重矩阵$\pmb{W}_x$与$\pmb{W}_y$中。整个循环层的输出向量为：
$$
\pmb{y}_{(t)}=\phi(\pmb{W}_x^T\pmb{x}_{(t)}+\pmb{W}_y^T\pmb{y}_{(t-1)}+\pmb{b})
$$
其中，$\pmb{b}$为偏置向量，$\phi(·)$为激活函数。注意许多研究人员倾向于在RNN中使用双曲正切激活函数而不是ReLU激活函数，例如Vu Pham等人2013年的论文[Dropout Improves Recurrent Neural Networks for Handwriting Recognition](https://homl.info/91)；也有基于ReLU的RNN，例如Quoc V. Le等人2015年的论文[A Simple Way to Initialize Recurrent Networks of Rectified Linear Units](https://homl.info/92)。

与前向神经网络一样，可以通过将时间步$t$的所有输入放置在一个输入矩阵$\pmb{X}_{(t)}$中，一次性计算整个小批的一个循环层的输出：
$$
\pmb{Y}_{(t)}=\phi(\pmb{X}_{(t)}\pmb{W}_x+\pmb{Y}_{(t-1)}\pmb{W}_y+\pmb{b})\\
=\phi([\pmb{X}_{(t)}\pmb{Y}_{(t-1)}]\pmb{W}+\pmb{b})\ with\ \pmb{W}=\left[
\begin{matrix}
\pmb{W}_x\\
\pmb{W}_y
\end{matrix}
\right]
$$
其中：

- $\pmb{Y}_{(t)}$为一个$m\times n_{neurons}$的矩阵，它包含小批中每个实例在每个时间步$t$的层的输出（$m$为小批中的实例数量，$n_{neurons}$为神经元数量）。
- $\pmb{X}_{t}$为一个$m\times n_{inputs}$的矩阵，它包含所有实例的输入（$n_{inputs}$为输入特征数量）。
- $\pmb{W}_x$为一个$n_{inputs}\times n_{neurons}$的矩阵，它包含当前时间步的输入的连接权重。
- $\pmb{W}_y$为一个$n_{neurons}\times n_{neurons}$的矩阵，它包含前一个时间步的输出的连接权重。
- $\pmb{b}$为一个大小为$n_{neurons}$的向量，它包含每个神经元的偏置项。
- 权重矩阵$\pmb{W}_x$与$\pmb{W}_y$常常垂直拼接为单个形状为$(n_{inputs}+n_{neurons})\times n_{neurons}$的权重矩阵（如上所示）。
- $[\pmb{X}_{(t)},\pmb{Y}_{(t-1)}]$表示矩阵$\pmb{X}_{(t)}$与$\pmb{Y}_{(t-1)}$的水平拼接。

注意$\pmb{Y}_{(t)}$为$\pmb{X}_{(t)}$与$\pmb{Y}_{(t-1)}$的函数，而$\pmb{Y}_{(t-1)}$又是$\pmb{X}_{(t-1)}$与$\pmb{Y}_{(t-2)}$的函数，而$\pmb{Y}_{(t-2)}$又是$\pmb{X}_{(t-2)}$与$\pmb{Y}_{(t-3)}$的函数，以此类推。这使得$\pmb{Y}_{(t)}$为所有输入$\pmb{X}_{(i)},0\le i\le t$的函数。在第一个时间步，$t=0$，没有以前输出，所以它们通过假定为全0。

### 记忆单元

既然在时间步$t$的循环神经元的输出是所有之前时间步的输入的函数，我们可以说它有一种**记忆（memory）**形式。神经网络中可以保持某种状态的部分被称为**记忆单元（memory cell）**，或简单称为**单元（cell）**。单个循环神经元，或一个循环神经元层，是非常基本的单元，它们只能学习短期模式（short pattern）（典型大约10步长，但这取决于具体任务）。

一般来说，一个单元在时间步$t$的状态，记为$\pmb{h}_{t}$（“h”代表“hidden”），是在这个时间步的一些输入及其在前一时间步的状态的函数：$\pmb{h}_{(t)}=f(\pmb{h}_{(t-1)},\pmb{x}_{(t)})$。它在时间步$t$的输出，记为$\pmb{y}_{(t)}$，也是前一个状态与当前输入的函数。在已经讨论过的基本单元中，输出等于状态，但在更复杂的单元中，情况并非总是如此，如图：

![一个单元的隐藏层与它的输出可能不同](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个单元的隐藏层与它的输出可能不同.png)

### 输入与输出序列

RNN可以同时获取一系列输入并产生一系列输出（如图左上角的网络），这种类型的**sequence-to-sequence网络（sequence-to-sequence network）**可用于预测时间序列（time series）例如股票价格：给它提供最后$N$天的价格，它输出未来一天的价格（从$N-1$天前到明天）。

也可以给网络提供一系列输入并忽略除最后一个输出外的所有输出（如图右上角的网络）。这是一个**sequence-to-vector网络（sequence-to-vector network）**。例如，可以给网络提供与电影评论相对应的一系列单词，网络将输出情感得分（sentiment score）（例如，从-1（厌恶）到+1（喜爱））。

还可以在每个时间步反复向网络提供相同的输入向量，并让它输出一个序列（如图右下角的网络）。这是一个**vector-to-sequence 网络（vector-to-sequence network）**。例如，输入可以是一张图像（或CNN的输出），输出可以是该图像的一个标题。

最后，可以有一个sequence-to-vector网络称为**编码器（encoder）**，它紧接着一个vector-to-sequence网络，称为**解码器（decoder）**。该网络可用于将一个语言的句子翻译成另一个语言的句子。此时可以给网络提供一个语言的句子，编码器将该句子转换为一个向量表示，然后解码器将该向量解码为另一个语言的句子。这种两阶段（two-step）模型被称为**编码器-解码器（Encoder–Decoder）**，它比直接使用单个sequence-to-sequence RNN进行翻译的效果好得多：句子中的最后一个词会影响译文的第一个词，因此需要等到看到整个句子后再翻译它。

> We will see how to implement an Encoder–Decoder in Chapter 16 (as we will see, it is a bit more complex than in Figure 15-4 suggests).

![seq-to-seq（左上）、seq-to-vector（右上）、vector-to-seq（左下角）与Encoder-Decoder（右下角）网络](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\seq-to-seq（左上）、seq-to-vector（右上）、vector-to-seq（左下角）与Encoder-Decoder（右下角）网络.png)

## 训练RNN

训练一个RNN的诀窍在于通过时间展开它，然后简单地使用普通的反向传播（如图）。这被称为**通过时间反向传播（backpropagation through time，BPTT）**。

与普通的反向传播类似，首先一个前向传递通过整个展开的网络（由虚线箭头表示），然后输出序列使用损失函数$C(\pmb{Y}_{(0)},\pmb{Y}_{(1)},...\pmb{Y}_{(T)})$进行评估（$T$为最大时间步）。注意该损失函数可能忽略一些输出，如图所示（例如，在一个sequence-to-vector RNN中，最后一个输出外的所有输出被忽略）。随后，损失函数的梯度通过展开的网络向后传播（propagated backward）（由实线箭头表示）。最后，使用BPTT过程中计算得到的梯度更新模型参数。注意，梯度向后流动，通过损失函数使用的所有输出，而不仅仅只是通过最后一个输出（例如，在下图中，损失函数使用网络的最后三个输出$\pmb{Y}_{(2)}$、$\pmb{Y}_{(3)}$与$\pmb{Y}_{(4)}$计算得到，所以梯度通过这三个输出，但是不通过$\pmb{Y}_{(0)}$与$\pmb{Y}_{(1)}$）。另外，因为每个时间步使用相同的$\pmb{W}$与$\pmb{b}$，所以反向传播会做正确的事情并在所有时间步上求和。

![通过时间反向传播](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\通过时间反向传播.png)

## 简单RNN

下面用RNN来预测一个时间序列。

假设现在要研究某个网站的每小时的活跃用户数，或某个城市的每日气温（daily temperature），或公司的财务健康（使用多个指标每季度一次）。在所有这些情况下，数据将是一个序列，每个时间步有一个或多个值。这被称为**时间序列（time series）**。在前两个示例中，每个时间步只有单个值，因此它们是**单变量时间序列（univariate time series）**；而在最后一个例子中，每个时间步有多个值（例如公司的收入、债务等等），所以它是一个**多变量时间序列（multivariate time series）**。一个典型的任务是预测未来的值，这被称为**预报（forecasting）**；另一个常见的任务是填空：预测（或“postdict”）过去的缺失值，这被称为**插补（imputation）**。例如，下面展示了3个单变量时间序列，每个50步长，目的是为它们中的每个预报下一个时间步的值（由“X”表示）：

```python
import numpy as np

def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise
    return series[..., np.newaxis].astype(np.float32)
```

```python
np.random.seed(42)

n_steps = 50
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]
```

```python
X_train.shape, y_train.shape
```

```python
import matplotlib.pyplot as plt

def plot_series(series, y=None, y_pred=None, x_label="$t$", y_label="$x(t)$"):
    plt.plot(series, ".-")
    if y is not None:
        plt.plot(n_steps, y, "bx", markersize=10)
    if y_pred is not None:
        plt.plot(n_steps, y_pred, "ro")
    plt.grid(True)
    if x_label:
        plt.xlabel(x_label, fontsize=16)
    if y_label:
        plt.ylabel(y_label, fontsize=16, rotation=0)
    plt.hlines(0, 0, 100, linewidth=1)
    plt.axis([0, n_steps + 1, -1, 1])

fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))
for col in range(3):
    plt.sca(axes[col])
    plot_series(X_valid[col, :, 0], y_valid[col, 0],
                y_label=("$x(t)$" if col==0 else None))
plt.show()
```

为了简便，这里使用`generate_time_series`生成时间序列。该函数可根据需要（通过`batch_size`实参）创建任意多个时间序列，每个长度为`n_steps`，每个序列的每个时间步只有一个值（即所有序列都是单变量的）。该函数返回一个NumPy数组，形状为`[batch size, time steps, 1]`，每个序列是两个振幅固定但频率和相位随机的正弦波之和，并加上了一点噪音。

当处理时间序列以及其他类型序列，例如句子时，输入特征通常表示为3维数组，它的形状为`[batch size, time steps, dimensionality]`。对于单变量时间序列，`dimensionality`为1；对于多变量时间序列，`dimensionality`大于1。

以上代码还创建了一个训练集、一个验证集以及一个测试集。其中`X_train`包含7000个时间序列（即它的形状为$[7000,50,1]$），`X_valid`包含2000个时间序列，`X_test`包含1000个时间序列。因为要为每个序列预测一个值，所以目标为列向量（例如，`y_train`的形状为$[7000,1]$）。

在使用RNN前，可以先设置一些基线指标（baseline metrics），否则我们可能认为我们的模型表现得很好但实际上它比基本模型（basic models）表现更差。例如，最简单的方法是将每个序列的最后一个值作为预测值。这被称为**天真预报（naive forecasting）**，有时它的表现出奇地难以超越。在这种情况下，它的均方误差大约为0.020：

```python
from tensorflow import keras

y_pred = X_valid[:, -1]
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
```

```python
plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])
plt.show()
```

另一个简单的方法是使用全连接网络。因为它期望每个输入为一个特征的列表（flat list），所以需要加一个`Flatten`层。下面使用一个简单的线性回归模型，这样每个预测值都是时间序列中的值的线性组合：

```python
import tensorflow as tf

np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(1)
])

model.compile(loss="mse", optimizer="adam")
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid))
```

```python
model.evaluate(X_valid, y_valid)
```

```python
import matplotlib as mpl

def plot_learning_curves(loss, val_loss):
    plt.plot(np.arange(len(loss)) + 0.5, loss, "b.-", label="Training loss")
    plt.plot(np.arange(len(val_loss)) + 1, val_loss, "r.-", label="Validation loss")
    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))
    plt.axis([1, 20, 0, 0.05])
    plt.legend(fontsize=14)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.grid(True)

plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()
```

```python
y_pred = model.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])
plt.show()
```

最终得到的MSE大约为0.004，比天真（naive）方法好得多。

下面看看一个简单的RNN能否打败它：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.SimpleRNN(1, input_shape=[None, 1])
])

optimizer = keras.optimizers.Adam(learning_rate=0.005)
model.compile(loss="mse", optimizer=optimizer)
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid))xxxxxxxxxx Let’s see if we can beat that with a simple RNN:np.random.seed(42)tf.random.set_seed(42)model = keras.models.Sequential([    keras.layers.SimpleRNN(1, input_shape=[None, 1])])optimizer = keras.optimizers.Adam(learning_rate=0.005)model.compile(loss="mse", optimizer=optimizer)history = model.fit(X_train, y_train, epochs=20,                    validation_data=(X_valid, y_valid))
```

```python
model.evaluate(X_valid, y_valid)
```

```python
plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()
```

```python
y_pred = model.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])
plt.show()
```

这是可以构建的最简单的RNN，它只包含一层，该层只有一个神经元，如[图1](#(循环神经网络)(网络结构)(循环神经元与层)(1))所示。这里不需要指定输入序列的长度，因为循环神经网络可以处理任意数量的时间步（这是设置第一个输入维度为`None`的原因）。默认情况下，该`SimpleRNN`使用双曲正切激活函数。它的工作方式如前：初始状态$h_{init}$被设置为0，它与第一个时间步的值$x_{0}$一起被传递单个循环神经元，该神经元计算这些值的加权和，然后对结果应用双曲正切激活函数，这产生了第一个输出$y_0$。在简单RNN中，该输出也是新的状态$h_0$。这个新状态与下一个输入值$x_{1}$一起传递给这个相同的循环神经元。这个过程一直重复，直到到达最后一个时间步，然后该层输出最后一个值$y_{49}$。所有这些都是针对每个时间序列同时执行的。


默认情况下，Keras的循环层只返回最后一个输出，如果要让它们在每个时间步返回一个输出，必须设置`return_sequences=True`。

最终的MSE仅达到0.014，所有它比天真方法好但是没有打败简单的线性模型。注意对于每个神经元，线性模型每个输入的每个时间步都有一个参数，加上一个偏置项，因此该简单的线性模型总共有51个参数。而对于简单RNN中的循环神经元，每个输出与每个隐藏状态维度都只有一个参数，再加上一个偏置项。该简单RNN总共只有3个参数。

可见这个简单RNN太简单了，以致于它无法获得良好性能。下面尝试添加更多的循环层。

## 深度RNN

可以将多个单元层堆叠起来，如图所示。这被称为**深度RNN（deep RNN）**。

<a name="(循环神经网络)(深度RNN)(1)">![深度RNN（左）通过时间展开（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\深度RNN（左）通过时间展开（右）.png)</a>

使用tf.keras实现一个深度RNN很简单，只需要堆叠循环层。下面示例使用了三个`SimpleRNN`层（当然也可以添加任意其他类型的循环层，例如一个`LSTM`层或一个`GRU`层）：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20, return_sequences=True),
    keras.layers.SimpleRNN(1)
])

model.compile(loss="mse", optimizer="adam")
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid))
```

```python
model.evaluate(X_valid, y_valid)
```

```python
plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()
```

```python
y_pred = model.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])
plt.show()
```

注意，要对所有的循环层设置`return_sequences=True`（如果只关心最后一个输出的话，最后一个循环层不用设置）。否则它们将输出一个3维数组（只包含最后一个时间步的输出）而不是一个3维数组（包含所有时间步的输出），这样下一个循环层会因为接收不到期望的3维格式而抱怨。

模型的MSE达到了0.003，打败了线性模型。

注意，模型的最后一层并不理想：它必须有一个单元，因为我们要预测一个单变量时间序列，这意味着每个时间步必须有单个的输出值。但是，拥有单个单元意味着隐藏状态只是一个数字。这真的不多，而且可能没有那么有用。RNN主要使用其他循环层的隐藏状态在时间步间携带所有所需信息，并且它不会太多地使用最后一层的隐藏状态。此外，由于`SimpleRN`层默认使用tanh激活函数，因此预测值必须在-1\~1之间，但是有时我们想要使用其他激活函数。出于这两个原因，也许最好使用`Dense`层替换输出层，它运行速度稍快，且准确率大致相同，并且允许我们选择任何想要的输出激活函数。如果作出这样的改变，还要确保从第二个（现在是最后一个）循环层中移除` return_sequences=True`。

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Dense(1)
])

model.compile(loss="mse", optimizer="adam")
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid))
```

```python
model.evaluate(X_valid, y_valid)
```

```python
plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()
```

```python
y_pred = model.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])
plt.show()
```

可以看到，模型收敛得更快，且性能一样好。另外，如果需要的话，还可以改变输出激活函数。

### 预报多个时间步

目前为止我们只预测了下一个时间步的值，只要合理地改变目标，就可以预测若干步后的值（例如，要预测10步后的值，只需要将目标改为10步后的值而不是1步后的值）。

如果要预测接下来的若干个值，则至少有两种方法：

- 使用如前训练过的模型，预测下一个值，并将该值加到输入中（就好像这个预测值真的出现了一样），然后再次使用该模型预测接下来的值，如此下去。
- 训练一个RNN去一次性预测若干个值。

下面使用第一种方法预测接下来的10个值：

```python
np.random.seed(43) # not 42, as it would give the first series in the train set

series = generate_time_series(1, n_steps + 10)
X_new, Y_new = series[:, :n_steps], series[:, n_steps:]
X = X_new
for step_ahead in range(10):
    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]
    X = np.concatenate([X, y_pred_one], axis=1)

Y_pred = X[:, n_steps:]
```

```
Y_pred.shape
```

```python
def plot_multiple_forecasts(X, Y, Y_pred):
    n_steps = X.shape[1]
    ahead = Y.shape[1]
    plot_series(X[0, :, 0])
    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], "ro-", label="Actual")
    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], "bx-", label="Forecast", markersize=10)
    plt.axis([0, n_steps + ahead, -1, 1])
    plt.legend(fontsize=14)

plot_multiple_forecasts(X_new, Y_new, Y_pred)
plt.show()
```

可以预料到，下一个时间步的预测而通常比之后时间步的预测要更准确，因为误差可能会积累（可以在上图中看到）。

下面在验证集上评估该方法（首先需要重新生成序列，并包含额外的9个时间步）：

```python
np.random.seed(42)

n_steps = 50
series = generate_time_series(10000, n_steps + 10)
X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]
X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]
X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]
```

```python
X = X_valid
for step_ahead in range(10):
    y_pred_one = model.predict(X)[:, np.newaxis, :]
    X = np.concatenate([X, y_pred_one], axis=1)

Y_pred = X[:, n_steps:, 0]
```

```python
Y_pred.shape
```

```python
np.mean(keras.metrics.mean_squared_error(Y_valid, Y_pred))
```

可以看到MSE大约为0.027，这比之前模型（models）的MSE大得多。但是这本身是更困难的任务，因此这种比较并不意味着什么。将该性能与天真预测（naive predictions）（单纯地预报时间序列在10个时间步内保持不变）或与简单的线性模型比较更有意义：

```python
Y_naive_pred = np.tile(X_valid[:, -1], 10) # take the last time step value, and repeat it 10 times
np.mean(keras.metrics.mean_squared_error(Y_valid, Y_naive_pred))
```

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(10)
])

model.compile(loss="mse", optimizer="adam")
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

天真方法相当糟糕（它的MSE大约为0.257），但是线性模型的MSE大约为0.0188，比使用该RNN每次预测未来一步要好得多，并且训练与运行要快得多。尽管如此，如果只想在更复杂的任务上预测几个时间步，则该方法可能工作得很好。

第二种方法直接训练一个RNN一次性预测接下来的10个值。这里仍然可以使用一个sequence-to-vector模型，但是它将输出10个值，而不是1个。首先需要将目标改变为包含接下来10个值的向量（上述步骤已实现），然后只需要令输出层包含10个单元，而不是1个：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Dense(10)
])

model.compile(loss="mse", optimizer="adam")
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

训练完成后，可以很轻松地一次性预测接下来10个值：

```python
np.random.seed(43)

series = generate_time_series(1, 50 + 10)
X_new, Y_new = series[:, :50, :], series[:, -10:, :]
Y_pred = model.predict(X_new)[..., np.newaxis]
```

```python
plot_multiple_forecasts(X_new, Y_new, Y_pred)
plt.show()
```

该模型工作得很好：接下来10个时间步的MSE大约为0.008，比线性模型好得多。但是仍然有改进空间：可以训练模型在每个时间步去预报接下来的10个值，而不是训练模型只在最后一个时间步预测接下来的10个值。也就是说，可以将该sequence-to-vector RNN转换为一个sequence-to-sequence RNN。该技术的优点在于损失将包含每个时间步的RNN输出项，而不只是最后一个时间步的输出项。这意味着有更多的误差梯度流过模型，它们不必只流过时间，而且也从每个时间步的输出流出。这将稳定与加速训练。

更清楚地说，在时间步0，模型将输出一个向量，它包含时间步1\~10的预报；然后在时间步1，模型将预报时间步2\~11，等等。因此目标必须是一个与输入序列等长的序列，其中每个时间步包含一个10维向量。下面准备这些目标序列：

```python
np.random.seed(42)

n_steps = 50
series = generate_time_series(10000, n_steps + 10)
X_train = series[:7000, :n_steps]
X_valid = series[7000:9000, :n_steps]
X_test = series[9000:, :n_steps]
Y = np.empty((10000, n_steps, 10))
for step_ahead in range(1, 10 + 1):
    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]
Y_train = Y[:7000]
Y_valid = Y[7000:9000]
Y_test = Y[9000:]
```

```python
X_train.shape, Y_train.shape
```

注意目标包含出现在输入中的值（`X_train`与`Y_train`间有很多重叠），但这不是作弊。在每个时间步，模型只知道过去时间步的信息，因此它不能往前看。这被称为一个**因果（causal）模型**。

为了将该模型转换为一个sequence-to-vector模型，必须在所有循环层（包括最后一层）设置`return_sequences=True`，并且必须在每个时间步应用输出`Dense`层。为此，Keras提供了`TimeDistributed`层：它包装（wraps）任意层并将其应用到它的输入序列的每个时间步。它通过调整输入形状以便使每个时间步被视为一个单独实例，从而高效地实现该功能（即：它将输入的形状从$[batch\ size,time\ steps,input\ dimensions]$调整为$[batch\ size\times time\ steps,input\ dimensions]$，在该例中，输入维度为20，因为前一个`SimpleRNN`有20个单元）。然后它运行`Dense`层，最终它将输出形状调回为序列（即：它将输出形状从$[batch\ size\times time\ steps,input\ dimensions]$调整为$[batch\ size,time\ steps,input\ dimensions]$，在该例中，输出维度为10，因为`Dense`层有10个单元）。以下是更新后的模型：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])
```

实际上，`Dense`层支持序列作为输入（甚至是更高维的输入）：它就像`TimeDistributed(Dense(...))`一样处理它们，意味着它只被应用到最后一个输入维度（与所有时间步独立）。因此可以将最后一层替换为`Dense(10)`。但是为了清楚起见，这里使用`TimeDistributed(Dense(10))`，因为它清楚地表明该`Dense`层在每个时间步被独立应用，且模型将输出一个序列，而不是单个向量。

> independently across all time steps

注意：`TimeDistributed(Dense(n))`层与`Conv1D(n, filter_size=1)`层等价。

训练过程中需要所有输出，但是只有最后一个时间步的输出对预测与评估有用。因此我们虽然训练依赖所有输出的MSE，但是使用一个自定义指标去评估，它计算最后一个时间步的输出的MSE：

```python
def last_time_step_mse(Y_true, Y_pred):
    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])

model.compile(loss="mse", optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

```python
np.random.seed(43)

series = generate_time_series(1, 50 + 10)
X_new, Y_new = series[:, :50, :], series[:, 50:, :]
Y_pred = model.predict(X_new)[:, -1][..., np.newaxis]
```

```python
plot_multiple_forecasts(X_new, Y_new, Y_pred)
plt.show()
```

验证MSE大约为0.06，比之前的模型好25%。可以将该方法与第一个方法结合：只使用该RNN预测接下来的10个值，然后将这些值拼接到输入时间序列（series），并再次使用该模型预测接下来的10个值，然后重复该过程直到所需次数。使用该方法，可以生成任意长的序列。对于长期预测（long-term predictions），这可能不太准确，但是如果目标是生成原始音乐或文本，它可能已足够了。

简单RNN相当擅长于预报时间序列或处理其他类型的序列，但它们在长时间序列（series or sequences）上的表现要差。下面讨论原因以及解决方法。

## 处理长序列

为了在长序列上训练一个RNN，必须在许多时间步上运行上，这使得展开的RNN成为一个非常深的网络。与任何深度神经网络一样，它也会遭受不稳定梯度问题：它可能需要很长时间训练，或训练可能不稳定。另外，当RNN处理长序列时，它会逐渐忘记序列中前面的输入。

### 对抗不稳定梯度问题

许多在深度网络中用到的缓解不稳定梯度问题的技巧都可用于RNN：好的参数初始化、更快速的优化器、dropout等等。但是，非饱和激活函数（例如，ReLU）在这里帮助可能不大，实际上，它们甚至可能导致RNN在训练过程中更不稳定。假设梯度下降更新权重后将第一个时间步的输入增加一点，因为同样的权重用于每个时间步，第二个时间步的输出可能也增加一点，第三个时间步同理，如此下去，直到输出爆炸。非饱和激活函数不能阻止这种情况。可以通过使用更小的学习率来降低该风险，但是也可以单纯地使用饱和激活函数例如双曲正切（这解释了为什么它是默认设置）。类似地，梯度本身也会爆炸。如果注意到训练不稳定，则可以监测梯度大小（例如，使用TensorBoard），并可能使用梯度裁剪。

另外，批标准化不能像深度前馈网络那样有效地用于RNN。事实上，不能将其用于时间步之间，只能将其用于循环层之间。更精确地说，技术上可以将一个BN层加到一个记忆单元上，以便将它应用到每个时间步（同时应用到该时间步的输入和前一步的隐藏状态）。但是，同样的BN层会被用在每个时间步，它们参数相同，而不管输入与隐藏状态的实际尺度与偏移如何。在实践中，这不会产生好的结果，正如César Laurent等人在[2015年的一篇论文](https://homl.info/rnnbn)中所证明的那样：作者发现BN仅在应用到输入而不是隐藏状态时有一点益处。也就是说，当它应用到循环层间、而不是循环层内（[图1](#(循环神经网络)(深度RNN)(1))的水平方向）时的效果仅比没有应用它要好一点（[图1](#(循环神经网络)(深度RNN)(1))的垂直方向）。在Keras中，只需简单地在每个循环层前添加一个`BatchNormalization`做到这一点，但不要抱有太大期望。 

另一种形式的标准化（normalization）在RNN中工作得更好：**层标准化（Layer Normalization）**。这个思想由Jimmy Lei Ba等人在[2016年的一篇论文中](https://homl.info/layernorm)提出：它与批标准化非常相似，但是它不是沿着批维度，而是沿着特征维度标准化。一个优点是，它可以直接在每个时间步为每个实例独立地计算所需统计数据。这意味着它在训练与测试过程中的行为一样（与BN相反），并且它不需要使用移动平均去估计训练集中所有实例的的特征统计数据。与BN类似，层标准化为每个输入学习一个缩放（scale）与偏移参数。在RNN中，它通常用于输入的线性组合与隐藏状态后。


下面使用BN训练一个RNN：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(20, return_sequences=True),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

下面使用tf.keras实现在一个简单的记忆单元中实现层标准化。为了做到这一点，需要定义一个自定义记忆单元，它就像一个常规的层一样，只是它的`call`方法接受两个实参：在当前时间步的`inputs`与来自前一时间步的隐藏`states`。注意`states`实参为一个包含一个或多个张量的列表。在简单CNN中，它包含一个等于前一个时间步的输出的张量，但是其他的单元可能有多个状态张量（例如，一个`LSTMCell`有一个长期状态与一个短期状态）。一个单元还必须有一个`state_size`属性与一个`output_size`属性。在简单CNN中，两者都等于单元数量。以下代码实现了一个自定义记忆单元，它的表现类似于一个`SimpleRNNCell`，只是它还在每个时间步应用层标准化：

```python
from tensorflow.keras.layers import LayerNormalization
```

```python
class LNSimpleRNNCell(keras.layers.Layer):
    def __init__(self, units, activation="tanh", **kwargs):
        super().__init__(**kwargs)
        self.state_size = units
        self.output_size = units
        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,
                                                          activation=None)
        self.layer_norm = LayerNormalization()
        self.activation = keras.activations.get(activation)
    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
        if inputs is not None:
            batch_size = tf.shape(inputs)[0]
            dtype = inputs.dtype
        return [tf.zeros([batch_size, self.state_size], dtype=dtype)]
    def call(self, inputs, states):
        outputs, new_states = self.simple_rnn_cell(inputs, states)
        norm_outputs = self.activation(self.layer_norm(outputs))
        return norm_outputs, [norm_outputs]
```

这段代码相当直接（继承自`SimpleRNNCell`会更简单，这样就不需要创建一个内部`SimpleRNNCell`或处理`state_size`或`output_size`属性了，但是这里的目的是展示如何从头创建一个自定义单元）。`LNSimpleRNNCell`类继承自`keras.layers.Layer`类，就像其他任何自定义层一样。构造器接受单元数量与所需的激活函数，并设置`state_size`与`output_size`属性，然后创建一个`SimpleRNNCell`，它没有任何激活函数（因为要在线性操作后、激活函数前执行层标准化）。然后构造器创建了一个`LayerNormalization`层，最后它获取所需的激活函数。`call`方法首先应用该简单的RNN单元，它计算当前输入与之前隐藏状态（states）的线性组合，并两次返回结果（在`SimpleRNNCell`中，输出等于隐藏状态，换言之，`new_states[0]`等于`outputs`，因此可以安全地在`call`方法的剩余部分中忽略`new_states`）。接下来，`call`方法应用层标准化，然后是激活函数。最后，它两次返回输出（一次作为输出，一次作为新的隐藏状态）。为了使用这个自定义单元，只需要创建一个`keras.layers.RNN`层，向其传递一个单元实例：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,
                     input_shape=[None, 1]),
    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

类似地，可以创建一个自定义单元并在每个时间步之间应用dropout。但是有一个更简单的方法：Keras提供的所有的循环层（除了`keras.layers.RNN`）与所有单元都有一个`dropout`超参数与一个`recurrent_dropout`超参数：前者定义了应用到输入的dropout率（在每个时间步），后者定义了隐藏状态的dropout率（同样在每个时间步）。不需要创建一个自定义单元去在RNN的每个时间步应用dropout。

通过这些技术，可以缓解不稳定梯度问题并更高效地训练RNN。

## 处理短期记忆问题

由于遍历RNN时数据经过的转换，在每个时间步有一些数据被丢失。一段时间后，RNN的状态几乎不包含第一个输入（inputs）的任何痕迹（trace）。为了解决这个问题，各种类型的具有长期记忆的单元被引入。它们被证明是如此成功以至于基本单元已不再被大量使用。

### LSTM单元

**长短期记忆（Long Short-Term Memory，LSTM）**单元由Sepp Hochreiter与Jürgen Schmidhuber[在1997年提出](https://homl.info/93)，并在多年间由一些研究人员逐渐改进，例如[Alex Graves](https://homl.info/graves)、[Haşim Sak](https://homl.info/94)与[Wojciech Zaremba](https://homl.info/95)。如果将LSTM单元看一个黑盒，则它的使用方式非常类似于一个基本单元，只是它的表现要好得多：训练收敛得更快，并且它会检测数据中的长期依赖。在Keras中，可以简单地使用LSTM层而不是`SimpleRNN`层：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.LSTM(20, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

可选地，也可以使用通用的`keras.layers.RNN`层，并传递给它一个`LSTMCell`层：

```python
model = keras.models.Sequential([
    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True,
                     input_shape=[None, 1]),
    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])
```

但是，在GPU上运行时，`LSTM`层会使用优化的实现，因此通常最好使用它（`RNN`层在定义自定义单元时最有用）。

训练完成后评估：

```python
model.evaluate(X_valid, Y_valid)
```

```python
plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()
```

```python
np.random.seed(43)

series = generate_time_series(1, 50 + 10)
X_new, Y_new = series[:, :50, :], series[:, 50:, :]
Y_pred = model.predict(X_new)[:, -1][..., np.newaxis]
```

```python
plot_multiple_forecasts(X_new, Y_new, Y_pred)
plt.show()
```

下图展示了LSTM单元的结构：

![LST单元](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\LST单元.png)

如果不看盒的内部结构，LSTM单元看起来就像一个常规的单元，只是它的状态被分为两个向量：$\pmb{h}_{(t)}$与$\pmb{c}_{(t)}$（“c”代表“cell”）。可以认为$\pmb{h}_{(t)}$为短期状态而$\pmb{c}_{(t)}$为长期状态。

下面打开这个盒。关键思想在于，该网络可以学习在长期状态中存储什么，需要丢弃什么，以及从中读取什么。当长期状态$\pmb{c}_{(t-1)}$从左到右穿过网络时，可以看到它首先通过**遗忘门（forget gate）**，丢弃一些记忆，然后通过额外操作添加一些新的记忆（额外操作添加由**输入门（input gate）**选择的记忆）。得到的$\pmb{c}_{(t)}$被直接发送出去，不经过任何进一步的变换。因此，在每个时间步，一些记忆被丢弃，一些记忆被添加。另外，在额外操作后，长期状态被复制并传递给tanh函数，得到的结果被**输出门（output gate）**过滤。这产生短期状态$\pmb{h}_{(t)}$（它等于该时间步$\pmb{y}_{(t)}$的单元的输出）。

下面查看新记忆从何而来以及门是如何工作的。首先，当前输入向量$\pmb{x}_{(t)}$以及前面的短期状态$\pmb{h}_{(t-1)}$被输送到四个不同的全连接网络。它们都有不同的用途：

- 主层是输出$\pmb{g}_{(t)}$的层。它有着通常的分析当前输入$\pmb{x}_{(t)}$与前一个（短期）状态$\pmb{h}_{(t-1)}$的作用。在基本单元中，只有这一层，且它的输出直接输出到$\pmb{y}_{(t)}$与$\pmb{h}_{(t)}$。相反，在一个LSTM单元中，该层的输出不是直接输出，它的最重要的部分被存储在长期状态中（其余部分被丢弃）。
- 其余三层为**门控制器（gate controllers）**。因为它们使用logistic函数，它们的输出范围为0\~1。它们的输出被被提供给逐位相乘操作，因此如果它们输出0，则它们关闭门；如果它们输出1，则它们打开门。具体来说：
  - 遗忘门（由$\pmb{f}_{(t)}$控制）控制长期状态的哪些部分应该被清除。
  - 输入门（由$\pmb{i}_{(t)}$控制）控制应该将$\pmb{g}_{(t)}$的哪些部分加到长期状态中。
  - 最后，输出门（由$\pmb{o}_{(t)}$控制）控制在该时间步，长期状态的哪些部分应该被读取并输出（同时到$\pmb{h}_{(t)}与$$\pmb{y}_{(t)}$）。


> In a basic cell, there is nothing other than this layer, and its output goes straight out to **y**(*t*) and **h**(*t*) . In contrast, in an LSTM cell this layer’s output does not go straight out, but instead its most important parts are stored in the long-term state (and the rest is dropped).

总之，一个LSTM单元可以学习去识别重要输入（这是输入门的作用），将它存储到长期状态中（这是遗忘门的作用），并在需要的时候提取它。这解释了为什么这些单元在捕捉时间序列（series）、长文本、录音等中的长期模式方面取得了惊人的成功。

以下公式总结了如何计算每个实例在每个时间步的单元的长期状态、短期状态与输出：

$$
\pmb{i}_{(t)}=\sigma(\pmb{W}_{xi}^T\pmb{x}_{(t)}+\pmb{W}_{hi}^T\pmb{h}_{(t-1)}+\pmb{b}_i)\\
\pmb{f}_{(t)}=\sigma(\pmb{W}_{xf}^T\pmb{x}_{(t)}+\pmb{W}_{hf}^T\pmb{h}_{t-1}+\pmb{b}_f)\\
\pmb{o}_{(t)}=\sigma(\pmb{W}_{xo}^T\pmb{x}_{(t)}+\pmb{W}_{ho}^T\pmb{h}_{(t-1)}+\pmb{b}_o)\\
\pmb{g}_{(t)}=\tanh(\pmb{W}_{xg}^T\pmb{x}_{(t)}+\pmb{W}_{hg}^T\pmb{h}_{(t-1)}+\pmb{b}_g)\\
\pmb{c}_{(t)}=\pmb{f}_{(t)}\otimes\pmb{c}_{(t-1)}+\pmb{i}_{(t)}\otimes\pmb{g}_{(t)}\\
\pmb{y}_{(t)}=\pmb{h}_{(t)}=\pmb{o}_{(t)}\otimes\tanh(\pmb{c}_{(t)})
$$

其中：

- $\pmb{W}_{xi}$、$\pmb{W}_{xf}$、$\pmb{W}_{xo}$与$\pmb{W}_{xg}$为四层中每一层与输入向量$\pmb{x}_{(t)}$的连接权重矩阵。
- $\pmb{W}_{hi}$、$\pmb{W}_{hf}$、$\pmb{W}_{ho}$与$\pmb{W}_{hg}$为四层中每一层与前一短期状态$\pmb{h}_{(t-1)}$的连接权重矩阵。
- $\pmb{b}_{i}$、$\pmb{b}_f$、$\pmb{b}_{o}$与$\pmb{b}_g$为四层中每一层偏置项。注意TensorFlow将$\pmb{b}_{f}$初始化为全1而不是全0的向量。这防止了在训练开始时遗忘一切。

#### peephole连接

在常规的LSTM单元中，门控制器只能查看输入$\pmb{x}_{(t)}$与前一个短期状态$\pmb{h}_{(t)}$。通过让它们同时能查看长期状态，给它们提供一点更多的上下文可能是一个好主意。该思想由[Felix Gers与Jürgen Schmidhuber在2000年提出](https://homl.info/96)。它们提出了一种LSTM变体，它包含被称为**peephole连接（peephole connections）**的额外连接：前一长期状态$\pmb{c}_{(t-1)}$作为输入被加到遗忘门与输入门的控制器上，当前的长期状态$\pmb{c}_{(t-1)}$作为输入被加到输出门的控制器上。这常常能改善性能，但并不总是如此，并且没有明确的模式表明哪些任务在有或没有它们的情况下会更好，需要在具体任务上尝试它并看它是否有帮助。

在Kera中，`LSTM`层基于`keras.layers.LSTMCell`单元，该单元不支持peephole。然而实验性的`tf.keras.experimental.PeepholeLSTMCell`支持。因此可以创建一个`keras.layers.RNN`层并将一个`PeepholeLSTM`传递给它的构造器。

#### GRU单元

**门控循环单元（Gated Recurrent Unit，GRU）**（如图）由Kyunghyun Cho等人在[2014年的一篇论文](https://homl.info/97)中提出，该论文同时引入了前述的编码器-解码器网络。

![GRU单元](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\GRU单元.png)

GRU单元是LSTM单元的简化版本，并且它看起来表现得一样好（这是它越来越受欢迎的原因）（ Klaus Greff等人在2015年的一篇论文[LSTM: A Search Space Odyssey](https://homl.info/98)中似乎表明所有的LSTM变体的表现大致相同）。主要简化如下：

- 两个状态向量合并为一个向量$\pmb{h}_{(t)}$。
- 一个门控制器$\pmb{z}_{(t)}$同时控制遗忘门与输入门。如果门控制器输出1，则遗忘门被打开（$=1$），输入门被关闭（$1-1=0$）；如果它输出0，则结果相反。换言之，一旦一个记忆被存储，则首先擦除它存储的位置。这实际上是LSTM单元本身的一个常见变体。
- 没有输出门，每个时间步输出完整的状态向量。然后，有一个新的门控制器$\pmb{r}_{(t)}$控制着前一状态的哪部分将显示给主层（$\pmb{g}_{(t)}$）。

以下公式总结了如何计算每个实例在每个时间步的单元状态：

$$
\pmb{z}_{(t)}=\sigma(\pmb{W}_{xz}^T\pmb{x}_{(t)}+\pmb{W}_{hz}^T\pmb{h}_{(t-1)}+\pmb{b}_z)\\
\pmb{r}_{(t)}=\sigma(\pmb{W}_{xr}^T\pmb{x}_{(t)}+\pmb{W}_{hr}^T\pmb{h}_{(t-1)}+\pmb{b}_r)\\
\pmb{g}_{(t)}=\tanh(\pmb{W}_{xg}^T\pmb{x}_{(t)}+\pmb{W}_{hg}^T(\pmb{r}_{(t)}\otimes\pmb{h}_{(t-1)})+\pmb{b}_g)\\
\pmb{h}_{(t)}=\pmb{z}_{(t)}\otimes\pmb{h}_{(t-1)}+(1-\pmb{z}_{(t)})\otimes\pmb{g}_{(t)}
$$

Keras提供一个`Keras.layers.GRU`层（基于`keras.layers.GRUCell`记忆单元），要使用它，只需将`SimpleRNN`或`LSTM`替换为`GRU`。

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.GRU(20, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

```python
model.evaluate(X_valid, Y_valid)
```

```python
plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()
```

```python
np.random.seed(43)

series = generate_time_series(1, 50 + 10)
X_new, Y_new = series[:, :50, :], series[:, 50:, :]
Y_pred = model.predict(X_new)[:, -1][..., np.newaxis]
```

```python
plot_multiple_forecasts(X_new, Y_new, Y_pred)
plt.show()
```

LSTM与GRU单元是RNN成功的主要原因之一。然而尽管它们可以处理比简单RNN长得多的序列，它们仍然有相当有限的短期记忆，并且它们很难学习100个或更多的时间步（例如音频样本、长时间序列或长句子）中的长期模式。解决该问题的一种方法是缩短输入序列，例如使用1维卷积层。

#### 使用1维卷积层去处理序列

一个2维卷积层在一张图像上滑动若干相当小的核并产生多个2维特征图（每个核一个）。类似地，一个1维卷积层在一个序列上滑动若干核并为每个核产生1维特征图。每个核学习去检测一个非常小的序列模式（不超过核大小）。如果使用10个核，则该层的输出由10个1维序列组成（每个序列长度相同），或者，可以将该输出看作一个10维序列。这意味着可以构建一个神经网络，它由循环层与1维卷积层（甚至1维池化层）组成。如果使用一个步长为1、使用`same`填充的1维卷积层，则输出序列将与输入序列等长。但是如果使用`valid`填充或步长大于1，则输出序列将比输入序列短，所以确保要相应地调整目标。例如，以下模型与之前的模型一样，只是它首先通过一个步长为2的1维卷积层以2的因子下采样输入序列。核大小大于步长，因此每个输入都被用来计算层的输出，从而模型能够学习去保留有用信息，只丢弃不重要的细节。通过缩短序列，卷积层可以帮助GRU层检测更长的模式。注意必须同时裁剪目标中的前3个时间步（因为核大小为4，卷积层的第一个输出将基于输入时间步0\~3），并以2的因子下采样目标。

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding="valid",
                        input_shape=[None, 1]),
    keras.layers.GRU(20, return_sequences=True),
    keras.layers.GRU(20, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train[:, 3::2], epochs=20,
                    validation_data=(X_valid, Y_valid[:, 3::2]))
```

如果训练并评估该模型，会发现它是目前为止最好的模型。卷积层确实有帮助。实际上，可以只使用1维卷积层并完全丢弃循环层。

### WaveNet

在[2016年的一篇论文中](https://homl.info/wavenet)，Aaron van den Oord以及其他的DeepMind研究人员提出了一个被称作**WaveNet**的结构。它们堆叠1维卷积层，在每层加倍**扩张率（dilation rate）**（每个神经元的输入是如何分散的）：第一个卷积层一次只能看到两个时间步，接下来的卷积层看到4个时间步（它的感受域有4个时间步长），接下来的卷积层看到8个时间步，如此下去（如图）。这样，较低的层学习短期模式，而较高的层学习长期模式。由于加倍的扩张率，该网络可以非常高效地处理很长的序列。

![WaveNet结构](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\WaveNet结构.png)

在该WaveNet论文中，作者实际上堆叠了10个卷积层，扩张率为1、2、4、

8、……、256、512，然后他们堆叠了另一组10个相同的层，然后又是另一组相同的10层。他们通过指出一个由10个具有这些扩张率的卷积层堆像一个有1024大小的核的卷积层超高效卷积层来证明该结构合理性（只是它更快、更强大并且使用更少的参数），这是他们堆叠3个这样的层的原因。他们还在每层前，使用数量等于扩张率的0来左填充输入序列，以在整个网络中保持序列长度相同。下面实现了一个简化的WaveNet来处理前述序列：

```python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential()
model.add(keras.layers.InputLayer(input_shape=[None, 1]))
for rate in (1, 2, 4, 8) * 2:
    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding="causal",
                                  activation="relu", dilation_rate=rate))
model.add(keras.layers.Conv1D(filters=10, kernel_size=1))
model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
                    validation_data=(X_valid, Y_valid))
```

该`Sequential`模型开始于一个显式的（explicit）输入层（这比仅在第一层设置`input_shape`更简单），然后是一个1维卷积层，它使用`causal`填充：这保证了该卷积层在进行预测时不会窥探未来（它等价于在输入的左边填充适量的0，并使用`valid`填充）。然后使用不断增长的扩张率：1、2、4、8，然后又一个1、2、4、8，来添加相似层对。最后，添加输出层：一个卷积层，它包含10个大小为1的过滤器且没有任何激活函数。由于这些填充层的存在，每个卷积层输出一个与输入等长的序列，所以训练过程中使用的目标可以是整个序列：不需要裁剪它们或下采样它们。

下面是论文中定义的原始WaveNet，它使用了更多的技巧，例如像在ResNet中的跳跃连接，以及类似于在GRU单元中的**门激活单元（Gated Activation Units）**。

```python
class GatedActivationUnit(keras.layers.Layer):
    def __init__(self, activation="tanh", **kwargs):
        super().__init__(**kwargs)
        self.activation = keras.activations.get(activation)
    def call(self, inputs):
        n_filters = inputs.shape[-1] // 2
        linear_output = self.activation(inputs[..., :n_filters])
        gate = keras.activations.sigmoid(inputs[..., n_filters:])
        return self.activation(linear_output) * gate
```

```python
def wavenet_residual_block(inputs, n_filters, dilation_rate):
    z = keras.layers.Conv1D(2 * n_filters, kernel_size=2, padding="causal",
                            dilation_rate=dilation_rate)(inputs)
    z = GatedActivationUnit()(z)
    z = keras.layers.Conv1D(n_filters, kernel_size=1)(z)
    return keras.layers.Add()([z, inputs]), z
```

```python
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

n_layers_per_block = 3 # 10 in the paper
n_blocks = 1 # 3 in the paper
n_filters = 32 # 128 in the paper
n_outputs = 10 # 256 in the paper

inputs = keras.layers.Input(shape=[None, 1])
z = keras.layers.Conv1D(n_filters, kernel_size=2, padding="causal")(inputs)
skip_to_last = []
for dilation_rate in [2**i for i in range(n_layers_per_block)] * n_blocks:
    z, skip = wavenet_residual_block(z, n_filters, dilation_rate)
    skip_to_last.append(skip)
z = keras.activations.relu(keras.layers.Add()(skip_to_last))
z = keras.layers.Conv1D(n_filters, kernel_size=1, activation="relu")(z)
Y_proba = keras.layers.Conv1D(n_outputs, kernel_size=1, activation="softmax")(z)

model = keras.models.Model(inputs=[inputs], outputs=[Y_proba])
```

```python
model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=2,
                    validation_data=(X_valid, Y_valid))
```

最后两个模型在预测时间序列（series）方面提供了目前最好的性能。在WaveNet论文中，作者在各种音频任务上实现了最先进的性能（该结构名字的由来），这些任务包括文本到语音（text-tospeech）任务、跨多种语言产生难以置信的真实嗓音。它们还使用该模型去生成音乐（每次一个音频样本）。考虑到一秒钟的音频可以包含数万个时间步，即使LSTM与GRU也无法处理如此长的序列，这个壮举就更加令人印象深刻了。

# 表示学习

## 自编码器

**自编码器（autoencoder）**是能够在无监督的情况下学习输入数据的密集表示（dense representations）的人工神经网络。这种密集表示被称为**潜在表示（latent representations）**或**编码（codings）**。这些编码通常比输入数据的维度少得多，这使得自编码对降维有帮助，尤其当用于可视化时。自编码器还充当特征检测器，并且它们可以用于深度神经网络的无监督预训练。最后，一些自编码器是生成模型，它们生成能够随机与训练数据非常相似的新数据。例如，可以在人脸图片上训练一个自编码器，它可以生成新的人脸。然而，生成的图像通常模糊而且不完全真实。

> Autoencoders are artificial neural networks capable of learning dense representations of the input data, called *latent representations* or *codings*, without any supervision (i.e., the training set is unlabeled).

相反，**生成对抗网络（generative adversarial networks，GANs）**生成的人脸目前很令人信服了，以至于很难相似它们表示的人不存在，参见[https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)，这是一个展示人脸的网站，其中人脸由被称为*StyleGAN*的GAN结构生成。还可以参见[https://thisrentaldoesnotexist.com/](https://thisrentaldoesnotexist.com/)查看一些生成的Airbnb卧室。GAN目前广泛用于超分辨率（增加图像分辨率）、[着色（colorization）](https://github.com/jantic/DeOldify)、强大的图像编辑（例如用真实背景替代照片炸弹）、将简单草图转换为逼真图像、预测视频的下一帧、扩充数据集（以训练其他模型）、生成其他类型数据（例如文本、音频与时间序列）、识别其他模型中的弱点并加强它们，等等。

自编码器与GANs都是无监督的，它们都学习密集表示，他们都可以用作生成模型，并且它们有许多相似应用。然而，它们的工作方式非常不同：

- 自编码器只是学习将它们的输入拷贝到输出。这看起来微不足道，但是使用各种方式约束网络使得它相当困难。例如，可以限制潜在表示的大小，或像输入中添加噪音并训练网络以恢复原始输入。这些约束防止自编码器简单地将输入直接拷贝到输出，迫使它学习用高效方法去表示数据。简言之，编码是自编码器在某些约束条件下学习等价函数（identity function）的副产品。
- GANs由两个神经网络组成：**生成器（generator）**尝试生成与训练集相似的数据，**判别器（discriminator）**尝试区分真实数据与伪造数据。这个结构在深度学习中非常新颖，因为生成器与判别器在训练过程中相互竞争：生成器常常被比作尝试制造逼真假币（realistic counterfeit money）的罪犯，而判别器就像尝试区分真钱与假钱的警方调查员。**对抗性训练（adversarial training）**（训练竞争神经网络（competing neural networks））被广泛认为是近年来最重要的思想之一。在2016年，Yann LeCun甚至说它是“the most interesting idea in the last 10 years in Machine Learning”。

## 高效的数据表示

给定以下数字序列：

- 40, 27, 25, 36, 81, 57, 10, 73, 19, 68
- 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14

乍一看，因为第一个序列短得多，因此它应该更容易记忆。然而，仔细看第二个序列会发现它只是一个从50到14的偶数列表。一旦注意到这个模式，第一个序列就变得比第一个序列容易记忆得多，因为只需要记住该模式（即递减的偶数）以及开始与结束数字（即50与14）即可。如果可以快速且容易地记忆非常长的序列，则不需要关心第二个序列是否存在什么模式，只需要记住每个数字即可。由于很难记住长序列，因此识别模式很有用。因此在训练过程中约束自编码器会迫使它发现并利用数据中的模式。

20世纪70年代初，William Chase与Herbert Simon对记忆（memory）、感知（perception）与模式匹配（pattern matching）之间的关系进行了著名的[研究](https://homl.info/111)。它们观察到专业的国际象棋棋手只需看棋盘5秒就能记住比赛中所有棋子的位置，这是大部分人认为不可能的任务。然而，只有当棋子被放置在（现实比赛中的）现实位置，而不是被随机放置时，才会这样。这是因为国际象棋专家的游戏经验使得更容易看到国际象棋模式。注意，模式帮助它们高效地存储信息。

就像这个记忆实验中的棋手一样，自编码器查看输入，将它们转换为高效地潜在表示，然后吐出一些（有希望）非常接近输入的东西。一个自编码器总是由两部分组成：一个**编码器（encoder）**（或**识别网络（recognition network）**）将输入转换为潜在表示，随后是一个**解码器（decoder）**（或**生成网络（generative network）**）将内部表示转换为输出，如图：

![国际象棋记忆实验（左）与一个简单的自编码器（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\国际象棋记忆实验（左）与一个简单的自编码器（右）.png)

可以看到，典型情况下，自编码器与多层感知器（Multi-Layer Perceptron）有着相同的结构，只是输出层的神经元数量必须与输出数量相同。在这个例子中，只有一个隐藏层，它由两个神经元组成（编码器），以及一个输出层，它由三个神经元组成（解码器）。输出常常被称为**重建（reconstructions）**，因为自编码器尝试重建输入，并且代价函数包含一个**重建损失（reconstruction loss）**，当重建与输入不同时，它会惩罚模型。

因为内部表示的维度比输入数据的小（它是2维而不是3维的），因此说这个自编码器是**欠完备的（undercomplete）**。一个欠完备的自编码器不能简单地将它的输入拷贝到编码，它必须找到一个方法来输出输入的一个拷贝。它被迫学习输入数据中最重要的特征（并丢弃不重要的特征）。

## 使用欠完备的线性自编码器执行PCA

如果自编码器只使用线性活性（activations），并且损失函数为均方误差，则它最终执行主成分分析。

以下代码构建一个简单的线性自编码器，它在一个3维数据集上执行PCA，将其投影到2维：

```python
import numpy as np

np.random.seed(4)

def generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1):
    angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5
    data = np.empty((m, 3))
    data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2
    data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2
    data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)
    return data

X_train = generate_3d_data(60)
X_train = X_train - X_train.mean(axis=0, keepdims=0)
```

```python
import tensorflow as tf
from tensorflow import keras

np.random.seed(42)
tf.random.set_seed(42)

encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])
decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])
autoencoder = keras.models.Sequential([encoder, decoder])

autoencoder.compile(loss="mse", optimizer=keras.optimizers.SGD(lr=1.5))
```

该代码与之前构建的所有MLPs没有太大区别，但是有一些需要注意的地方：

- 这里将自编码器组织成两个子构件（subcomponents）：编码器与解码器。两者都是常规的`Sequential`模型，分别有一个`Dense`层。自编码器是一个`Sequential`模型，包含一个编码器，随后是一个解码器。
- 自编码器的输出数量输入数量。
- 为了执行简单PCA，这里不使用任何激活函数（即所有的神经元都是线性的），并且损失函数是MSE。

下面在简单生成的3维数据集上训练模型并使用它去编码这个数据集（即将它投影到2维）：

```python
history = autoencoder.fit(X_train, X_train, epochs=20)
```

```python
codings = encoder.predict(X_train)
```

注意相同的数据集`X_train`同时用作输入与目标。下图展示了原始的3维数据集（左）与自编码器的隐藏层的输出（即编码层（the coding layer），右）。可以看到自编码器找到了最佳的2维平面来投影数据，它尽可能多地保留数据中的方差。

![由欠完备的线性自编码器执行的PCA](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\由欠完备的线性自编码器执行的PCA.png)

```python
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(4,3))
plt.plot(codings[:,0], codings[:, 1], "b.")
plt.xlabel("$z_1$", fontsize=18)
plt.ylabel("$z_2$", fontsize=18, rotation=0)
plt.grid(True)
plt.show()
```

可以认为自编码器是一种自监督学习（self-supervised learning）的一种形式（即使用自动生成标签的监督学习技术，在这个例子中标签就是输入）。

## 堆叠的自编码器

自编码器也可以有多个隐藏层，在这种情况下它们被称为**堆叠的自编码器（stacked autoencoders）**（或**深度自编码器（deep autoencoders）**）。增加更多的层有助于自编码器学习更复杂的编码。也就是说，必须小心，不要让自编码器太强大。设想一个编码器它强大，以至于它只学习将每个输入映射到一个任意数字（并且编码器学习反向映射）。显然这样的自编码器会完美地重建训练集，但是在该过程中，它没有学习到任何有用的数据表示（并且它不太可能很好地泛化到新实例上）。

典型情况下，一个堆叠的自编码器的结构关于中间隐藏层（编码层）对称。简单来说，它就像一个三明治。例如，用于MNIST的自动编码器可能有784个输入，然后是一个隐藏层，有100个神经元，然后是一个中间隐藏层，有30个神经元，然后是另一个隐藏层，有100个神经元，以及一个输出层，有784个神经元。如图：

![堆叠的自编码器](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\堆叠的自编码器.png)

### 使用Keras实现堆叠的自编码器

可以实现一个堆叠的自编码器，这与实现一个常规的深度MLP非常相似。特别地，训练深度网络的技术同样可用于训练自编码器。例如，以下代码使用SELU激活函数，为Fashion MNIST构建了一个堆叠的自编码器：

```python
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full.astype(np.float32) / 255
X_test = X_test.astype(np.float32) / 255
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]
```

```python
def rounded_accuracy(y_true, y_pred):
    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))
```

```python
tf.random.set_seed(42)
np.random.seed(42)

stacked_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(30, activation="selu"),
])
stacked_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[30]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])
stacked_ae.compile(loss="binary_crossentropy",
                   optimizer=keras.optimizers.SGD(lr=1.5), metrics=[rounded_accuracy])
history = stacked_ae.fit(X_train, X_train, epochs=20,
                         validation_data=(X_valid, X_valid))
```

- 一如往常，这里将自编码器划分为两个子模型：编码器与解码器。
- 编码器接受$28\times28$像素的灰度图像，将它们扁平化（flatten），使得每张图像表示为一个大小为784的向量，然后通过两个逐渐减小的`Dense`层处理这些向量，两者都使用SELU激活函数（也可以添加LeCun正态初始化，但是这个网络不是很深，因此结果不会有很大不同）。对于每个输入图像，编码器输出大小为30的向量。
- 解码器接受大小为30的编码（由编码器输出）并通过两个逐渐增大的`Dense`层处理它们，并且它将最终向量的形状转换为$28\times28$的数组，使得解码器的输出与编码器的输入的形状相同。
- 当编码这个堆叠的自编码器时，使用二元交叉熵损失函数，而不是均方误差。这里将重建任务看作一个多标签二元分类问题：每个像素灰度（intensity）表示一个像素是黑色的概率。以这种方式（而不是一个回归问题）构建往往使得模型更快收敛（因为指标期望每个像素的标签为0或1，因此使用准确率指标不能正确工作。这里创建一个自定义指标，它将目标与预测舍入到0或1，来计算准确率）。
- 最后，将`X_train`同时作为输入与目标来训练模型（类似地，将`X_valid`同时作为验证输入与目标）。

### 可视化重建

确保自编码器被正确（properly）训练的一种方法是比较输入与输出：差别应该不会很明显。这里绘制一些验证集的图像，以及它们的重建：

>Let’s plot a few images from the validation set, as well as their reconstructions:

```python
def plot_image(image):
    plt.imshow(image, cmap="binary")
    plt.axis("off")
```

```python
def show_reconstructions(model, images=X_valid, n_images=5):
    reconstructions = model.predict(images[:n_images])
    fig = plt.figure(figsize=(n_images * 1.5, 3))
    for image_index in range(n_images):
        plt.subplot(2, n_images, 1 + image_index)
        plot_image(images[image_index])
        plt.subplot(2, n_images, 1 + n_images + image_index)
        plot_image(reconstructions[image_index])
```

```python
show_reconstructions(stacked_ae)
```

重建可辨认，但是损失太大。可能需要训练模型更长时间，或使编码器解码器更深，或使编码更大。但是如果网络太强大，它会在没有学习到数据中的任何有用模式的情况下作出完美的重建。现在就使用这个模型。

### 可视化Fashion MNIST数据集

既然已经训练了一个堆叠的自编码器，就可以使用它去降低数据集维度。对于可视化，与其他降维算法（例如[降维](#(降维))章节讨论的降维算法）相比，这通常不会给出很好的结果，但是自编码的一大优点是它可以处理包含许多实例与许多特征的大型数据集。因此一种策略是使用一个自编码器将维度降到合理水平，然后使用另一种降维算法进行可视化。下面使用该策略来可视化Fashion MNIST。首先，使用堆叠的自编码器将维度降到30，然后使用t-SNE算法的Scikit-Learn实现将维度降到2以可视化：

```python
np.random.seed(42)

from sklearn.manifold import TSNE

X_valid_compressed = stacked_encoder.predict(X_valid)
tsne = TSNE()
X_valid_2D = tsne.fit_transform(X_valid_compressed)
X_valid_2D = (X_valid_2D - X_valid_2D.min()) / (X_valid_2D.max() - X_valid_2D.min())
```

下面绘制数据集：

```python
plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap="tab10")
plt.axis("off")
plt.show()
```

美化一下散点图：

```python
import matplotlib as mpl

# adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
plt.figure(figsize=(10, 8))
cmap = plt.cm.tab10
plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=cmap)
image_positions = np.array([[1., 1.]])
for index, position in enumerate(X_valid_2D):
    dist = np.sum((position - image_positions) ** 2, axis=1)
    if np.min(dist) > 0.02: # if far enough from other images
        image_positions = np.r_[image_positions, [position]]
        imagebox = mpl.offsetbox.AnnotationBbox(
            mpl.offsetbox.OffsetImage(X_valid[index], cmap="binary"),
            position, bboxprops={"edgecolor": cmap(y_valid[index]), "lw": 2})
        plt.gca().add_artist(imagebox)
plt.axis("off")
plt.show()
```

t-SNE算法识别了一些簇，它们与类别相当匹配（每个类别使用不同颜色表示）。

## 使用堆叠的自编码器的无监督预训练

前面提到，如果要处理一个复杂的有监督任务，但是没有很多有标签训练数据，一种解决方法是找一个执行相似任务的神经网络并重用它的低层。这使得使用很少的训练数据训练一个高性能的（high-performance）模型成为可能，因为神经网络不需要学习所有的低层特征，它只需要重用已有网络学到的特征检测器（ feature detectors）。

类似地，如果有很大的数据集，但是大部分数据都是无标签的，则可以首先使用所有数据训练一个堆叠的自编码器，然后重用低层，为实际任务去创建一个神经网络，并使用有标签数据训练它。例如，图中展示了如何使用一个堆叠的自编码器为一个分类神经网络执行无监督预训练。当训练该分类器时，如果没有太多的有标签训练数据，则可以冻结预训练层（至少是低层）。

实现没有什么特别之处：只需要使用所有训练数据训练自编码器（有标签的加上无标签的），然后重用它的编码层去创建一个新的神经网络。

![使用自编码器的无监督预训练](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\使用自编码器的无监督预训练.png)



无标签数据很多但有标签数据很少是很普遍的。构建一个大型无标签数据集常常很便宜（例如，一个简单的脚本可以从网上下载数百万张图片），但是为这些图片打标签（例如，将它们分类为“可爱”或“不可爱”）通常只能由人可靠地完成。标记实例既耗时又昂贵，因此只有几千个人工标记的实例是很正常的。

## 联系权重

当一个自编码器完全对称时（就像前面构建的那个），一种常见的技术是将解码器层的权重与编码器层的权重联系（tie）起来。这将模型中的权重数量减半，加速训练并限制过拟合的风险。具体来说，如果自编码器一共有$N$层（不算输入层），$\pmb{W}_L$表示第$L$层的连接权重（例如，第$1$层是第一个隐藏层，第$N/2$层是编码层，第$N$层表示是输出层），则解码器的权重可以被简单定义为：$\pmb{W}_{N-L+1}=W_L^T$（其中$L=1,2,...,L/2)$。

为了使用Keras联系层间的权重，先定义一个自定义层：

```python
class DenseTranspose(keras.layers.Layer):
    def __init__(self, dense, activation=None, **kwargs):
        self.dense = dense
        self.activation = keras.activations.get(activation)
        super().__init__(**kwargs)
    def build(self, batch_input_shape):
        self.biases = self.add_weight(name="bias",
                                      shape=[self.dense.input_shape[-1]],
                                      initializer="zeros")
        super().build(batch_input_shape)
    def call(self, inputs):
        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)
        return self.activation(z + self.biases)
```

这个自定义层充当一个常规的`Dense`层，但是它使用另一个`Dense`层的权重，并转置它（设置`transpose_b=True`等价于转置第一个实参，但是它更高效，因为它直接在`matmul`操作中执行转置）。然而，它使用自己的偏置向量。接下啦就可以构建一个新的堆叠的自编码器，与前一个非常相似，只是解码器的`Dense`层联系到编码器的`Dense`层：

```python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

dense_1 = keras.layers.Dense(100, activation="selu")
dense_2 = keras.layers.Dense(30, activation="selu")

tied_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    dense_1,
    dense_2
])

tied_decoder = keras.models.Sequential([
    DenseTranspose(dense_2, activation="selu"),
    DenseTranspose(dense_1, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])

tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])

tied_ae.compile(loss="binary_crossentropy",
                optimizer=keras.optimizers.SGD(lr=1.5), metrics=[rounded_accuracy])
history = tied_ae.fit(X_train, X_train, epochs=10,
                      validation_data=(X_valid, X_valid))
```

```python
show_reconstructions(tied_ae)
plt.show()
```

与前一个模型相比，该模型使用的参数数量几乎减半，重建误差略小。

## 一次训练一个自编码器

可以一次训练一个浅层自编码器，然后将它们全部堆叠为单个堆叠的自编码器（名字的由来），如图。该技术现在用得不多，但是仍然可能有一些讨论“贪婪分层训练”的论文。

![一次训练一个自编码器](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一次训练一个自编码器.png)

在训练的第一阶段，第一个自编码器学习重建输入，然后使用第一个自编码器编码整个训练集，这会得到一个新的（压缩的）训练集。然后再这个新的数据集上训练第二个自编码器。这是训练的第二阶段。最后，使用所有这些自编码器构建一个大的三明治，如上图（即首先堆叠每个自编码器的隐藏层，然后反向堆叠每个自编码器的输出层）。这会得到最终的堆叠的自编码器。使用这种方式很容易训练更多的自编码器，构建一个非常深的堆叠的自编码器。

```python
def train_autoencoder(n_neurons, X_train, X_valid, loss, optimizer,
                      n_epochs=10, output_activation=None, metrics=None):
    n_inputs = X_train.shape[-1]
    encoder = keras.models.Sequential([
        keras.layers.Dense(n_neurons, activation="selu", input_shape=[n_inputs])
    ])
    decoder = keras.models.Sequential([
        keras.layers.Dense(n_inputs, activation=output_activation),
    ])
    autoencoder = keras.models.Sequential([encoder, decoder])
    autoencoder.compile(optimizer, loss, metrics=metrics)
    autoencoder.fit(X_train, X_train, epochs=n_epochs,
                    validation_data=(X_valid, X_valid))
    return encoder, decoder, encoder(X_train), encoder(X_valid)
```

```python
tf.random.set_seed(42)
np.random.seed(42)

K = keras.backend
X_train_flat = K.batch_flatten(X_train) # equivalent to .reshape(-1, 28 * 28)
X_valid_flat = K.batch_flatten(X_valid)
enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(
    100, X_train_flat, X_valid_flat, "binary_crossentropy",
    keras.optimizers.SGD(lr=1.5), output_activation="sigmoid",
    metrics=[rounded_accuracy])
enc2, dec2, _, _ = train_autoencoder(
    30, X_train_enc1, X_valid_enc1, "mse", keras.optimizers.SGD(lr=0.05),
    output_activation="selu")
```

```python
stacked_ae_1_by_1 = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    enc1, enc2, dec2, dec1,
    keras.layers.Reshape([28, 28])
])
```

```python
show_reconstructions(stacked_ae_1_by_1)
plt.show()
```

```python
stacked_ae_1_by_1.compile(loss="binary_crossentropy",
                          optimizer=keras.optimizers.SGD(lr=0.1), metrics=[rounded_accuracy])
history = stacked_ae_1_by_1.fit(X_train, X_train, epochs=10,
                                validation_data=(X_valid, X_valid))
```

```python
show_reconstructions(stacked_ae_1_by_1)
plt.show()
```

前面提到，当前对深度学习的高度兴趣的一个触发因素是[Geoffrey Hinton等人](https://homl.info/136)于2006年的发现：深度神经网络可以使用这种贪婪逐层预训练方式按照无监督方式预训练。为此，它们使用玻尔兹曼机。但是在2007年，[Yoshua Bengio](https://homl.info/112)等人证明（show）自编码器也能工作得一样好。一些（several）年来，这是唯一的高效训练深度网络的方式，直到许多技术的引入使得一次性训练一个深度网络成为可能。

> until many of the techniques introduced in Chapter 11 made it possible to just train a deep net in one shot.

## 卷积自编码器

如果要处理图像，则目前为止看到的自编码器不能很好地工作（除非图像非常小）：对于图像，卷积网络比密集网络适合得多。因此如果要为图像构建一个自编码器（例如用于无监督预训练或降维），则需要构建一个[卷积自编码器](https://homl.info/convae)。编码器是常规的CNN，由卷积层与池化层组成。典型情况下，它减小输入的空间维度（即高度与宽度）并增加深度（即特征图数量）。解码器必须执行相反操作（放大（upscale）图像并将深度缩小到原始维度），为此，可以使用反卷积层（或者可以结合上采样层与卷积层）。以下是用于Fashion MNIST的简单卷积自编码器：

> convolutional neural networks are far better suited than dense networks to work with images.

> The encoder is a regular CNN composed of convolutional layers and pooling layers.

```python
tf.random.set_seed(42)
np.random.seed(42)

conv_encoder = keras.models.Sequential([
    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),
    keras.layers.Conv2D(16, kernel_size=3, padding="SAME", activation="selu"),
    keras.layers.MaxPool2D(pool_size=2),
    keras.layers.Conv2D(32, kernel_size=3, padding="SAME", activation="selu"),
    keras.layers.MaxPool2D(pool_size=2),
    keras.layers.Conv2D(64, kernel_size=3, padding="SAME", activation="selu"),
    keras.layers.MaxPool2D(pool_size=2)
])
conv_decoder = keras.models.Sequential([
    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding="VALID", activation="selu",
                                 input_shape=[3, 3, 64]),
    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding="SAME", activation="selu"),
    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding="SAME", activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])

conv_ae.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(lr=1.0),
                metrics=[rounded_accuracy])
history = conv_ae.fit(X_train, X_train, epochs=5,
                      validation_data=(X_valid, X_valid))
```

```python
conv_encoder.summary()
conv_decoder.summary()
```

```python
show_reconstructions(conv_ae)
plt.show()
```

## 循环自编码器

如果要为序列（例如时间序列（time series）或文本）构建自编码器（例如用于无监督学习或降维），则循环神经网络可能比密集网络更适合。构建一个**循环自编码器（recurrent autoencoder）**很直接：典型情况下，编码器为sequence-to-vector RNN，它将输入序列压缩为一个向量，解码器是一个vector-to-sequence RNN，执行相反操作：

```python
recurrent_encoder = keras.models.Sequential([
    keras.layers.LSTM(100, return_sequences=True, input_shape=[28, 28]),
    keras.layers.LSTM(30)
])
recurrent_decoder = keras.models.Sequential([
    keras.layers.RepeatVector(28, input_shape=[30]),
    keras.layers.LSTM(100, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(28, activation="sigmoid"))
])
recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])
recurrent_ae.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(0.1),
                     metrics=[rounded_accuracy])
```

```python
history = recurrent_ae.fit(X_train, X_train, epochs=10, validation_data=(X_valid, X_valid))
```

```python
show_reconstructions(recurrent_ae)
plt.show()
```

该循环自编码器可以处理任意长度的序列，每个时间步28维。这意味着它可以将每张图片当做一个行序列来处理Fashion MNIST：在每个时间步，RNN处理一行28像素。显然，可以为任意类型序列使用循环自编码器。注意这里将`RepeatVector`作为解码器的第一层，以确保输入向量在每个时间步被馈送到解码器。

## 过完备的自编码器

到目前为止，为了迫使自编码器学习到关心的（interesting）特征，我们限制了其编码层的大小，使其欠完备。实际上，可以使用许多其他类型的约束，包括允许编码层与输入层一样大，甚至更大的约束，这会产生**过完备的自编码器**。下面是其中的一些方法。

## 去噪自编码器

一种迫使自编码器学习有用特征的方式是给输入添加噪音，训练它去恢复原始的、没有噪音的输入。该思想从20世纪80年代就开始了（例如，它在Yann LeCun 1987年的硕士论文中被提到）。在[2008年的一篇论文](https://homl.info/113)中，Pascal Vincent等人证明自编码器也可以用于特征提取。在[2010年的一片论文](https://homl.info/114)中，Vincent等人引入了**堆叠的去噪自编码器（stacked denoising autoencoders）**。

噪音可以是加到输入上的高斯噪音，或随机关闭数据，就像dropout一样。如图展示了两种选择：

![去噪自编码器，使用高斯噪音（左）或dropout（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\去噪自编码器.png)

实现很直接：它是一个常规的堆叠的自编码器，有一个额外的`Dropout`层应用于编码器的输入（或者可以使用`GaussianNoise`层代替之）。回想以下，`Dropout`层只在训练过程中被激活（`GaussianNoise`也是如此）。

使用高斯噪音的实现如下：

```python
tf.random.set_seed(42)
np.random.seed(42)

denoising_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.GaussianNoise(0.2),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(30, activation="selu")
])
denoising_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[30]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
denoising_ae = keras.models.Sequential([denoising_encoder, denoising_decoder])
denoising_ae.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(lr=1.0),
                     metrics=[rounded_accuracy])
history = denoising_ae.fit(X_train, X_train, epochs=10,
                           validation_data=(X_valid, X_valid))
```

```python
tf.random.set_seed(42)
np.random.seed(42)

noise = keras.layers.GaussianNoise(0.2)
show_reconstructions(denoising_ae, noise(X_valid, training=True))
plt.show()
```

使用`Dropout`的实现如下：

```python
tf.random.set_seed(42)
np.random.seed(42)

dropout_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(30, activation="selu")
])
dropout_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[30]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])
dropout_ae.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(lr=1.0),
                   metrics=[rounded_accuracy])
history = dropout_ae.fit(X_train, X_train, epochs=10,
                         validation_data=(X_valid, X_valid))
```

```python
tf.random.set_seed(42)
np.random.seed(42)

dropout = keras.layers.Dropout(0.5)
show_reconstructions(dropout_ae, dropout(X_valid, training=True))
```

下面展示了一些噪音图像（一半像素被关闭），以及由基于dropout的去噪自编码器重建的图像。可以看到，噪音自编码器不仅可以用于数据可视化或无监督预训练（就像目前为止讨论的其他自编码器），还可以很容易且有效地用于去除图像中的噪音。

![噪音图像（顶部）与它们的重建（底部）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\噪音图像（顶部）与它们的重建（底部）.png)

## 稀疏自编码器

另一种常常产生好的特征提取的约束是**稀疏度（sparsity）**：通过将合适的项添加到损失函数，自编码器被迫减少编码层的活跃（active）神经元的数量。例如，它的编码层可能被迫平均只有5%显著活跃的神经元。这迫使自编码器将每个输入表示为少量激活的组合。典型情况下，最终编码层的每个神经元表示一个有用的特征。

一个简单的方法是在编码层使用sigmoid激活函数（以将编码约束为0\~1之间的值），使用一个大的编码层（例如，有300个神经元），并将$\ell_1$正则化添加到编码层的激活函数上（解码器就是一个常规的解码器）。

下面是一个常规的自编码器：

```python
tf.random.set_seed(42)
np.random.seed(42)

simple_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(30, activation="sigmoid"),
])
simple_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[30]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
simple_ae = keras.models.Sequential([simple_encoder, simple_decoder])
simple_ae.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(lr=1.),
                  metrics=[rounded_accuracy])
history = simple_ae.fit(X_train, X_train, epochs=10,
                        validation_data=(X_valid, X_valid))
```

```python
show_reconstructions(simple_ae)
plt.show()
```

创建两个函数打印活性（activation）直方图：

```python
def plot_percent_hist(ax, data, bins):
    counts, _ = np.histogram(data, bins=bins)
    widths = bins[1:] - bins[:-1]
    x = bins[:-1] + widths / 2
    ax.bar(x, counts / len(data), width=widths*0.8)
    ax.xaxis.set_ticks(bins)
    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(
        lambda y, position: "{}%".format(int(np.round(100 * y)))))
    ax.grid(True)
```

```python
def plot_activations_histogram(encoder, height=1, n_bins=10):
    X_valid_codings = encoder(X_valid).numpy()
    activation_means = X_valid_codings.mean(axis=0)
    mean = activation_means.mean()
    bins = np.linspace(0, 1, n_bins + 1)

    fig, [ax1, ax2] = plt.subplots(figsize=(10, 3), nrows=1, ncols=2, sharey=True)
    plot_percent_hist(ax1, X_valid_codings.ravel(), bins)
    ax1.plot([mean, mean], [0, height], "k--", label="Overall Mean = {:.2f}".format(mean))
    ax1.legend(loc="upper center", fontsize=14)
    ax1.set_xlabel("Activation")
    ax1.set_ylabel("% Activations")
    ax1.axis([0, 1, 0, height])
    plot_percent_hist(ax2, activation_means, bins)
    ax2.plot([mean, mean], [0, height], "k--")
    ax2.set_xlabel("Neuron Mean Activation")
    ax2.set_ylabel("% Neurons")
    ax2.axis([0, 1, 0, height])
```

下面使用这些函数绘制编码（encoding）层的活性。左边的直方图展示了所有活性的分布，可以看到接近0或1的值总体上更频繁，这与sigmoid函数的饱和性质一致。右边的直方图展示了神经元平均活性的分布，可以看到大部分神经元的平均活性接近0.5。两个直方图说明每个神经元倾向于接近0或1，它们的概率都大约为50%。然而，一些神经元几乎一直都有活性（右边的直方图的右边）。

```python
plot_activations_histogram(simple_encoder, height=0.35)
plt.show()
```

下面将$\ell_1$正则化添加到编码层上：

```python
tf.random.set_seed(42)
np.random.seed(42)

sparse_l1_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(300, activation="sigmoid"),
    keras.layers.ActivityRegularization(l1=1e-3)  # Alternatively, you could add
                                                  # activity_regularizer=keras.regularizers.l1(1e-3)
                                                  # to the previous layer.
])
sparse_l1_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[300]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])
sparse_l1_ae.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(lr=1.0),
                     metrics=[rounded_accuracy])
history = sparse_l1_ae.fit(X_train, X_train, epochs=10,
                           validation_data=(X_valid, X_valid))
```

```python
show_reconstructions(sparse_l1_ae)
```

```python
plot_activations_histogram(sparse_l1_encoder, height=1.)
plt.show()
```

`ActivityRegularization`层只是返回它的输入，但是一个副作用是它会添加一个等于输入绝对值之和的损失（该层只在训练过程中起作用）。等价地，可以移除`ActivityRegularization`层并在前一层中设置`activity_regularizer=keras.regularizers.l1(1e-3)`。该惩罚将激励神经网络产生接近0的编码，但是因为当神经网络不能正确重建它也会收到惩罚，因此它至少必须输出一些非零值。使用$\ell_1$范数而不是$\ell_2$范数将迫使神经网络保留最重要的编码并消除对输入图像不需要的编码（而不是单纯地减小所有编码）。

另一种常常能产生更好结果的方法是在每次训练迭代时度量编码层的实际稀疏度，当度量的稀疏度与目标稀疏度不同时惩罚模型。通过计算整个训练批中编码层每个神经元的平均活性来做到这点。批大小不能太小，否则均值不准确。

一旦得到每个神经元的平均活性，可以将一个**稀疏度损失（sparsity loss）**添加到损失函数上，来惩罚太活跃或不够活跃的神经元。例如，如果度量到一个神经元的平均活性为0.3，而目标稀疏度为0.1，必须对其惩罚以减小其活性。一种方法是简单将均方误差$(0.3-0.1)^2$添加到损失函数上，但是在实践中，一个更好的方法是使用Kullback–Leibler（KL）散度，它比均方误差有更强的梯度，如图：

```python
p = 0.1
q = np.linspace(0.001, 0.999, 500)
kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))
mse = (p - q)**2
mae = np.abs(p - q)
plt.plot([p, p], [0, 0.3], "k:")
plt.text(0.05, 0.32, "Target\nsparsity", fontsize=14)
plt.plot(q, kl_div, "b-", label="KL divergence")
plt.plot(q, mae, "g--", label=r"MAE ($\ell_1$)")
plt.plot(q, mse, "r--", linewidth=1, label=r"MSE ($\ell_2$)")
plt.legend(loc="upper left", fontsize=14)
plt.xlabel("Actual sparsity")
plt.ylabel("Cost", rotation=0)
plt.axis([0, 1, 0, 0.95])
```

给定两个离散的概率分布$P$与$Q$，两个分布间的KL散度，记作$D_{KL}(P\ ||\ Q)$，可以使用下式计算：

$$
D_{KL}(P\ ||\ Q)=\sum_iP(i)\log\frac{P(i)}{Q(i)}
$$

本例需要度量编码层的一个神经元激活的目标概率$p$与实际概率$q$（即所有训练批上的平均活性），因此KL散度简化为：

$$
D_{KL}(p\ ||\ q)=p\log\frac{p}{q}+(1-p)\log\frac{1-p}{1-q}
$$

一旦计算好编码层每个神经元的稀疏度损失，就可以将这些损失相加并将结果加到损失函数上。为了控制稀疏度损失与重建损失的相对重要性，可以将稀疏度损失乘以一个稀疏度权重超参数。如果这个权重过高，则模型将非常接近目标稀疏度，但是它可能无法正确重建输入，使得模型无用。反之，如果它太小，则模型基本忽略稀疏度目标，无法学习任何感兴趣的特征。

下面基于KL散度实现一个稀疏度自编码器。首先创建一个自定义正则化器以应用KL散度正则化：

```python
K = keras.backend
kl_divergence = keras.losses.kullback_leibler_divergence

class KLDivergenceRegularizer(keras.regularizers.Regularizer):
    def __init__(self, weight, target=0.1):
        self.weight = weight
        self.target = target
    def __call__(self, inputs):
        mean_activities = K.mean(inputs, axis=0)
        return self.weight * (
            kl_divergence(self.target, mean_activities) +
            kl_divergence(1. - self.target, 1. - mean_activities))
```

现在可以为编码层活性使用`KLDivergenceRegularizer`，从而构建稀疏自编码器了：

```python
tf.random.set_seed(42)
np.random.seed(42)

kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)
sparse_kl_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(300, activation="sigmoid", activity_regularizer=kld_reg)
])
sparse_kl_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[300]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])
sparse_kl_ae.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(lr=1.0),
              metrics=[rounded_accuracy])
history = sparse_kl_ae.fit(X_train, X_train, epochs=10,
                           validation_data=(X_valid, X_valid))
```

```python
show_reconstructions(sparse_kl_ae)
```

在Fashion MNIST数据集上训练好该稀疏自编码器后，编码层神经元的活性大多接近0（大约70%的活性低于0.1），并且所有神经元的平均活性都在0.1左右（大约90%的神经元的平均活性在0.1\~0.2之间），如图：

```python
plot_activations_histogram(sparse_kl_encoder)
plt.show()
```

## 变分自编码器

**变分自编码器（variational autoencoders）**由Diederik Kingma与Max Welling[在2013年引入](https://homl.info/115)并迅速称为最受欢迎的自编码器类型之一。

它们与之前讨论的所有自编码器都有很大不同，表现在以下方面：

- 它们是**概率自编码器（probabilistic autoencoders）**，意味着它们的输出部分取决于偶然性，即使在训练后（与去噪自编码器不同，它只在训练过程中使用随机性）。
- 最重要的是，它们是**生成自编码器（generative autoencoders）**，意味着它们能够生成新实例，就好像它们是从训练集上采样得到的一样。

这两种特性使得它们与RBM相当相似，但是它更容易训练，并且采样过程快得多（使用RBM需要在采样新实例前等到网络稳定到“热平衡”）。实际上，就像它们的名字所表明的那样，变分自编码器执行变分贝叶斯推断（一个有效的执行近似贝叶斯推断的方法）。

如图展示了一个变分自编码器。可以看到所有自编码器都有的基本结构，编码器后跟着一个解码器（该例中，它们都有两个隐藏层），但是有一个不同：编码器产生一个**平均编码（mean coding）**$\pmb{\mu}$与一个标准差$\pmb{\sigma}$，而不是直接为给定输入产生编码。实际的编码从一个均值为$\pmb{\mu}$、标准差为$\pmb{\sigma}$的高斯分布中随机采样得到。然后解码器正常地解码采样后的编码。右图显示了一个训练实例通过该自编码器。首先，编码器产生$\pmb{\mu}$与$\pmb{\sigma}$，然后随机采样一个编码（注意它并不正好位于$\pmb{\mu}$，最后该编码被解码，最终的输出像训练实例。

<a name="(表示学习)(自编码器)(变分自编码器)(1)">![变分自编码器（左）与一个通过它的实例（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\变分自编码器（左）与一个通过它的实例（右）.png)</a>

可以看到，虽然输入可能有非常复杂的分布，但是变分自编码器倾向于产生看起来像是从一个简单高斯分布中采样的编码（变分自编码器实际上更通用，编码不限于高斯分布）：在训练过程中，损失函数迫使编码在编码空间（也称为**潜在空间（latent space）**）中逐渐迁移，最终它看起来像一团高斯点。训练好一个变分自编码器后，可以非常容易地产生新实例：只需要从高斯分布中产生一个随机编码并解码它。

损失函数由两部分组成。第一部分是常规的重建损失，它迫使自编码器重现其输入（为此，可以使用交叉熵，如前所述）。第二部分是**潜在损失（latent loss）**，它迫使自编码器的编码看起来就像是从一个简单高斯分布中采样得到的：它是目标分布（即高斯分布）与编码的实际分布间的KL散度。在数学上它比稀疏自编码器复杂一点，尤其是因为高斯噪音的存在，它限制了可以传送到编码层的信息量（从而迫使自编码器学习有用的特征）。但是可以用下式很简单地计算潜在损失：

> For more mathematical details, check out the original paper on variational autoencoders, or Carl Doersch’s great tutorial (2016).

$$
ℒ=-\frac{1}{2}\sum_{i=1}^K1+\log(\sigma_i^2)-\sigma_i^2-\mu_i^2
$$

其中，$ℒ$是潜在损失，$n$为编码维度，$\mu_i$与$\sigma_i$为编码第$i$个分量的均值与标准差。编码器输出向量$\pmb{\mu}$与$\pmb{\sigma}$（包含所有的$\mu_i$与$\sigma_i$），[如图](#(表示学习)(自编码器)(变分自编码器)(1))左边。

对变分自编码器的结构的一个常见微调是使编码器输出$\pmb{\gamma}=\log(\pmb{\sigma}^2)$而不是$\pmb{\sigma}$。然后潜在损失可以使用下式计算，该方法在数值上更稳定，且加快训练速度：
$$
ℒ=-\frac{1}{2}\sum_{i=1}^K1+\gamma_i-\exp(\gamma_i)-\mu_i^2
$$
下面为Fashion MNIST构建一个变分自编码器（[如图](#(表示学习)(自编码器)(变分自编码器)(1))，但是使用$\pmb{\gamma}$微调）。首先需要创建一个自定义层在给定$\pmb{\mu}$与$\pmb{\gamma}$的情况下去采样编码：

> but using the **γ** tweak

```python
class Sampling(keras.layers.Layer):
    def call(self, inputs):
        mean, log_var = inputs
        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean 
```

`Sampling`层接受两个输入：`mean`（$\pmb{\mu}$）与`log_var`（$\pmb{\gamma}$）。它使用函数`K.random_normal`去从正态分布中采样一个随机向量（与$\pmb{\gamma}$形状一样），其均值为0、标准差为1。然后它将其乘以$\exp(\pmb{\gamma}/2)$（它等于$\pmb{\sigma}$），最后它加上$\pmb{\mu}$并返回结果。这从均值为$\pmb{\mu}$、标准差为$\pmb{\sigma}$的正态分布中采样一个编码向量。

接下来，可以创建编码器。这里使用函数式API，因为模型不完全是顺序的：

```python
tf.random.set_seed(42)
np.random.seed(42)

codings_size = 10

inputs = keras.layers.Input(shape=[28, 28])
z = keras.layers.Flatten()(inputs)
z = keras.layers.Dense(150, activation="selu")(z)
z = keras.layers.Dense(100, activation="selu")(z)
codings_mean = keras.layers.Dense(codings_size)(z)
codings_log_var = keras.layers.Dense(codings_size)(z)
codings = Sampling()([codings_mean, codings_log_var])
variational_encoder = keras.models.Model(
    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])

decoder_inputs = keras.layers.Input(shape=[codings_size])
x = keras.layers.Dense(100, activation="selu")(decoder_inputs)
x = keras.layers.Dense(150, activation="selu")(x)
x = keras.layers.Dense(28 * 28, activation="sigmoid")(x)
outputs = keras.layers.Reshape([28, 28])(x)
variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])

_, _, codings = variational_encoder(inputs)
reconstructions = variational_decoder(codings)
variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions])

latent_loss = -0.5 * K.sum(
    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),
    axis=-1)
variational_ae.add_loss(K.mean(latent_loss) / 784.)
variational_ae.compile(loss="binary_crossentropy", optimizer="rmsprop", metrics=[rounded_accuracy])
history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,
                             validation_data=(X_valid, X_valid))
```

```python
show_reconstructions(variational_ae)
plt.show()
```

注意输出`codings_mean`（$\pmb{\mu}$）与`codings_log_var`（$\pmb{\gamma}$）的`Dense`层输入相同（即第二个`Dense`层的输出）。然后`codings_mean`与`codings_log_var`传给`Sampling`层。最后，`variational_encoder`有三个输出，可以检查`codings_mean`与`codings_log_var`的值。这里只需要使用最后一个输出（`codings`）。

解码器可以使用顺序API而不是函数式API，因为它只是层的简单堆叠。解码器忽略了编码器的前两个输出（只需要将编码馈送给解码器）。然后，添加潜在损失与重建损失。首先计算批中每个实例的潜在损失（沿着最后一个轴求和），然后计算批中所有实例的平均损失并将结果除以784，以确保与重建损失相比它的尺度适当。实际上，该变分自编码器的重建损失应该是像素重建误差之和，但是当Keras计算`binary_crossentropy`损失时，它计算所有784个像素的均值而不是和。因此重建损失比需要的小784倍。可以定义一个自定义损失去计算和而不是均值，但是将潜在损失除以784更简单（最终的损失比本应该的损失值小784倍，但是这只意味着应该使用一个更大的学习率）。

注意这里使用了`RMSprop`优化器，在这种情况下它工作得很好。最后训练自编码器。

### 产生Fashion MNIST图像

下面使用这个变分自编码器生成看上去像时尚物品的图像。需要做的就是从高斯分布中采样随机编码并解码它们：

下面展示了12个生成的图像：

```python
def plot_multiple_images(images, n_cols=None):
    n_cols = n_cols or len(images)
    n_rows = (len(images) - 1) // n_cols + 1
    if images.shape[-1] == 1:
        images = np.squeeze(images, axis=-1)
    plt.figure(figsize=(n_cols, n_rows))
    for index, image in enumerate(images):
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(image, cmap="binary")
        plt.axis("off")
```

```python
tf.random.set_seed(42)

codings = tf.random.normal(shape=[12, codings_size])
images = variational_decoder(codings).numpy()
plot_multiple_images(images, 4)
```

大部分图像看上去相当可信，尽管有点模糊。其余的不太好，但是不要对该自编码器太苛刻，因为它只有几分钟的学习时间。更多的微调与训练时间会让图像看起来更好。

> The majority of these images look fairly convincing, if a bit too fuzzy.

变分自编码器使执行**语义插值（semantic interpolation）**成为可能：在编码级别上插值，而不是在像素级别上对两张图像插值。首先让两张图像通过编码器，然后对得到的两个编码插值，最后解码插值后的编码并得到最终的图像。它就像一个常规的Fashion MNIST图像，但是它处于原始图像的中间状态。以下代码接受12个生成的编码，将它们组织为$3\times4$的网格，然后使用TensorFlow的`tf.image.resize`函数将网格的大小调整为$5\times7$。默认情况下，`resize`函数执行双线性插值，因此每个其他行与列将包含插值后的编码。然后使用解码器去产生所有图像，并展示它们：

```python
tf.random.set_seed(42)
np.random.seed(42)

codings_grid = tf.reshape(codings, [1, 3, 4, codings_size])
larger_grid = tf.image.resize(codings_grid, size=[5, 7])
interpolated_codings = tf.reshape(larger_grid, [-1, codings_size])
images = variational_decoder(interpolated_codings).numpy()

plt.figure(figsize=(7, 5))
for index, image in enumerate(images):
    plt.subplot(5, 7, index + 1)
    if index%7%2==0 and index//7%2==0:
        plt.gca().get_xaxis().set_visible(False)
        plt.gca().get_yaxis().set_visible(False)
    else:
        plt.axis("off")
    plt.imshow(image, cmap="binary")
```

原始图像被加框，其余为相邻图像间语义插值后的结果。可以看到，第四行与第五列的鞋是位于其上方与下方的两个鞋的好的插值。

一些年来，变分自编码器很流行，但是GAN最终占据领先地位，特别是因为它们能够生成更真实、更清晰的图像。

## 生成对抗网络

生成对抗网络由Ian Goodfellow等人在[2014年的一篇论文](https://homl.info/gan)中提出。尽管这个想法几乎立刻让研究人员兴奋起来，但是用了几年时间才克服训练GAN的一些困难。事后看来，它看起来很简单：使神经网络相互竞争，希望竞争能够推断它们进步。如图，GAN由两个神经网络组成：

- 生成器：接受一个随机分布作为输入（典型情况下是高斯）并输出一些数据——典型情况下是一张图像。可以认为随机输入是要生成的图像的潜在表示（即编码（codings））。可以看到，生成器提供与变分自编码器中的解码器相同的功能，它可以以相同方式生成新图像（只需要馈送一些高斯噪音，它输出一个新的图像）。然而，它的训练方式非常不同。
- 判别器：接受来自生成器的伪造图像或来自训练集的真实图像作为输入，必须猜测输入图像是伪造的还是真实的。

> make neural networks compete against each other in the hope that this competition will push them to excel.

> the generator offers the same functionality as a decoder in a variational autoencoder,

![一个生成对抗网络](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\一个生成对抗网络.png)

在训练过程中，生成器与判别器目标相反：判别器尝试区分伪造图像与真实图像，而生成器尝试产生看起来足够真实的图像以欺骗判别器。因为GAN由两个目标不同的网络组成，它不能像常规神经网络那样训练。每个训练迭代分为两阶段：

- 第一阶段训练判别器。从训练集中采样一批真实图像，并用生成器生成相同数量的伪造图像。伪造图像的标签设置为0而真实图像的标签设置为1，判别器使用二元交叉生损失，在该打标签的批上训练一步。重要的是，反向传播只在该阶段优化判别器的权重。
- 第二阶段训练生成器。首先使用它产生另一批伪造图像，然后再次使用判别器区分图像是伪造的还是真实的。这次不在批中添加真实图像，所有的标签都设置为1（真实的）。换言之，我们想要生成器产生判别器会（错误地）相信是真实的图像。关键是，判别器的权重在此步中被冻结，因此反向传播只影响生成器的权重。

生成器从未真正看到任何真实图像，虽然它逐渐地学习去产生令人信服的伪造图像。它所得到的只是通过判别器返回的梯度。判别器越好，这些二手梯度中包含的真实图像的信息越多，因此生成器可以取得显著进步。

下面为Fashion MNIST构建一个简单的GAN。

首先需要构建生成器与判别器。生成器与自编码器的解码器很相似，判别器是一个常规的二元分类器（它接受图像作为输入，以包含单个单元、使用sigmoid激活函数的`Dense`层结束）。对于每个训练迭代的第二阶段，还需要一个包含生成器与紧随的判别器的全GAN模型：

```python
np.random.seed(42)
tf.random.set_seed(42)

codings_size = 30

generator = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[codings_size]),
    keras.layers.Dense(150, activation="selu"),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
discriminator = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(150, activation="selu"),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(1, activation="sigmoid")
])
gan = keras.models.Sequential([generator, discriminator])
```

然后，需要编译这些模型。因为判别器是一个二元分类器，可以很自然地使用二元交叉熵损失。生成器只通过`gan`模型训练，所以不需要编译它。`gan`模型也是一个二元分类器，因此它可以使用二元交叉熵函数。重要的是，判别器不应该在第二个阶段中被训练，因此在编译`gan`模型需要使它不可训练：

```python
discriminator.compile(loss="binary_crossentropy", optimizer="rmsprop")
discriminator.trainable = False
gan.compile(loss="binary_crossentropy", optimizer="rmsprop")
```

只有在编译模型时，Keras才会考虑`trainable`属性，因此当运行此代码后，当调用`discriminator`的`fit`或`train_on_batch`方法时，`discriminator`可训练，然而当在`gan`模型上调用这些方法，则它不可训练。

因为训练循环不寻常，因此不能使用常规的`fit`方法。可以编写一个自定义训练循环。为此，首先需要创建一个`Dataset`去遍历图像：

```python
batch_size = 32
dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)
dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)
```

下面将训练循环包装在`train_gan`函数中：

```python
def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50):
    generator, discriminator = gan.layers
    for epoch in range(n_epochs):
        print("Epoch {}/{}".format(epoch + 1, n_epochs))              # not shown in the book
        for X_batch in dataset:
            # phase 1 - training the discriminator
            noise = tf.random.normal(shape=[batch_size, codings_size])
            generated_images = generator(noise)
            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)
            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)
            discriminator.trainable = True
            discriminator.train_on_batch(X_fake_and_real, y1)
            # phase 2 - training the generator
            noise = tf.random.normal(shape=[batch_size, codings_size])
            y2 = tf.constant([[1.]] * batch_size)
            discriminator.trainable = False
            gan.train_on_batch(noise, y2)
        plot_multiple_images(generated_images, 8)                     # not shown
        plt.show()                                                    # not shown
```

```python
train_gan(gan, dataset, batch_size, codings_size, n_epochs=1)
```

可以看到每次迭代有两阶段：

- 在阶段一，高斯噪音被馈送到生成器以产生伪造图像，并通过连接相同数量的真实图像来完成此批。对于伪造图像，目标`y1`设置为0，对于真实图像，目标设置为1。然后在该批上训练判别器。注意这里设置`trainable`属性为`True`：只是为了消除Keras在注意到`trainable`现在为`False`，但在编译模型时为`True`时显示的警告（反之亦然）。
- 在阶段而，一些高斯噪音被馈送到GAN。它的生成器首先产生伪造的图像，然后判别器尝试猜测这些图像是伪造的还是真实的。我们希望判别器相信这些伪造的图像是真实的，因此目标`y2`设置为1。注意这里设置`trainable`属性为`False`，同样是为了避免警告。

下面展示这些生成的图像：

```python
tf.random.set_seed(42)
np.random.seed(42)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = generator(noise)
plot_multiple_images(generated_images, 8)
```

可以看到在第一代的最后，它们已经开始看起来像（非常嘈杂的）Fashion MNIST图像了。


不幸的是，图像永远不会比这好得多，甚至在某些代，GAN看起来忘记了它所学习到的东西。

```python
train_gan(gan, dataset, batch_size, codings_size)
```

### 训练GAN的困难

在训练过程中，生成器与判别器在零和游戏中不断尝试智胜（outsmart）对方。随着训练的推进，游戏最终可能进入一个被博弈论者（game theorists）称为**纳什均衡（Nash equilibrium）**的状态。它以数学家John Nash的名字命名：在这种情况下，如果其他玩家不改变自己的策略，那么没有哪个玩家会因改变自己的策略而受益。例如，当每个人都在道路左侧行驶时，任何单个司机转向另一侧对自己都不会有好处。当然，还有第二种可能的纳什均衡：当每个人都在道路右侧行驶时。不同的初始化状态与动态（dynamics）可能导致不同的均衡。在这个例子中，一旦达到均衡，就有单个最优策略（即与其他人一样在同一侧驾驶），但是纳什均衡可涉及多个竞争策略（例如，捕食者追逐猎物，猎物尝试逃跑，两者都不能通过改变它们的策略而受益）。

论文作者证明一个GAN只能达到一个纳什均衡：当生成器产生完美的真实图像，并且判别器被迫去猜测（50%真实，50%伪造）。看上去只需要训练GAN足够长的时间，它最终将达到该均衡，并返回一个完美的生成器。不幸的是，没有这么简单，没有什么能保证最终能达到均衡。

最大的一个困难被称作**模式坍塌（mode collapse）**：当生成器的输出逐渐变得不多样化。假设相比其他类别，生成器更擅长产生鞋，则它用鞋子能更多地欺骗判别器，这会鼓励它去产生更多的鞋的图像。最终，它会忘记如何产生其他物体。同时，判别器唯一能看到的图像为鞋，它也会忘记如何区分其他类别的伪造图像。最终，当判别器成功将伪造的鞋与真实的鞋区分开来，生成器将被迫移到其他类别。然后，它可能擅长（产生）衬衫，忘记鞋，然后判别器也跟随过来（即擅长区分衬衫，但是忘记如何区分鞋）。GAN可能逐渐在几个类别间循环，但是从未擅长其中任何一个。

另外，因为生成器与判别器不断督促对方（push against each other），它们的参数可能最终摇摆且变得不稳定。训练可能正确（properly）开始，然后因为这些不稳定性，突然不知何故分散。因为许多因素影响这些复杂的动态（dynamics），GAN对超参数非常敏感，因此可能需要花费很多精力去微调它们。

自2014年，这些问题让研究人员手忙脚乱：关于这个话题发表了很多的论文，一些提出新的损失函数（要比较主要的GAN损失，查看[GitHub project by Hwalsuk Lee](https://homl.info/ganloss)）（虽然Google研究人员[2018年发表的一片论文](https://homl.info/gansequal)对它们的效率提出质疑），或稳定训练或模式坍塌问题的技术。例如，一个被称为**经验回放（experience replay）**的流行技术将每次迭代生成器产生的图像存在一个回访缓冲（replay buffer）中（逐渐丢弃旧的生成的图像）并使用真实图像与从缓冲区中取到的伪造图像训练判别器（而不是单纯地使用当前生成器产生的伪造图像）。这减少了判别器过拟合最新的生成器的输出的机会。另一个常用的技术被称为**小批判别器（mini-batch discrimination）**：它度量批间图像的相似度，并将统计数据提供给判别器，因此它可以很容易地拒绝缺少多样性的一整批伪造图像。这鼓励生成器产生更多样的图像，减少模式坍塌的机会。其他论文简单地提出恰好表现良好的特定的结构。

简言之，这仍然是一个非常活跃的研究领域，且GAN的动态仍未被完全理解。但是已经有了巨大进展，且一些结果确实令人震撼。


#### 深度卷积GAN

2014年的原始的GAN论文实验了卷积层，但是只是尝试生成小图像。不久之后，许多研究人员尝试基于更深的卷积网络为更大的图像构建GAN。事实证明这很棘手，因为训练非常不稳定，但是Alec Radford等人在实验了许多不同的结构与超参数后，终于在2015年底成功。他们把他们的结构称为**[深度卷积GANs](https://homl.info/dcgan)（deep convolutional GANs，DCGANs）**。以下是他们为构建稳定的卷积GAN提出的主要指导方针：

- 使用跨步卷积（在判别器中）与反卷积（在生成器中）代替任意池化层。
- 同时在生成器与判别器中使用批规范化，除了在生成器的输出层与判别器的输入层。
- 移除全连接隐藏层，以实现更深的结构。
- 在生成器中对所有层使用ReLU激活，除了输出层使用tanh。
- 在判别器中对所有层使用leaky ReLU激活。

这些指导方针在许多情况下能奏效，但是并不总是如此，因此可能仍然需要实验不同的超参数（事实上，只是改变随机数种子并再次训练相同的模型有时能奏效）。例如，以下是一个小的DCGAN，它在Fashion MNIST上工作得相当好：

```python
tf.random.set_seed(42)
np.random.seed(42)

codings_size = 100

generator = keras.models.Sequential([
    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]),
    keras.layers.Reshape([7, 7, 128]),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding="SAME",
                                 activation="selu"),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding="SAME",
                                 activation="tanh"),
])
discriminator = keras.models.Sequential([
    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding="SAME",
                        activation=keras.layers.LeakyReLU(0.2),
                        input_shape=[28, 28, 1]),
    keras.layers.Dropout(0.4),
    keras.layers.Conv2D(128, kernel_size=5, strides=2, padding="SAME",
                        activation=keras.layers.LeakyReLU(0.2)),
    keras.layers.Dropout(0.4),
    keras.layers.Flatten(),
    keras.layers.Dense(1, activation="sigmoid")
])
gan = keras.models.Sequential([generator, discriminator])
```

```python
discriminator.compile(loss="binary_crossentropy", optimizer="rmsprop")
discriminator.trainable = False
gan.compile(loss="binary_crossentropy", optimizer="rmsprop")
```

生成器接受大小为100的编码，并将它们投影到6272维（`7 * 7 * 128`），并重塑结果得到一个$7\times7\times128$的张量。这个张量被批规范化并馈送给一个步长为2的反卷积层。它对其上采样，使其从$7\times7$成为$14\times14$并将其深度从128减少到64。结果再次被批规范化并馈送给另一个步长为2的反卷积层，它对其上采样，使其从$14\times14$成为$28\times28$并将深度从64减少到1。该层使用tanh激活函数因此输出的范围为-1\~1。因此，在训练GAN前，需要重新缩放训练集到这个相同大小。同样需要重塑（reshape）它以增加通道维度：

```python
X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale
```

判别器看起来非常像一个常规的用于二分类的CNN，只是使用跨步卷积而不是最大池化层下采样图像。同时注意这里使用leaky ReLU激活函数。

总之，这里遵循了DCGAN指导方针，除了将鉴别器中的`BatchNormalization`替换为`Dropout`层（否则在这种情况下训练不稳定），且将生成器中的ReLU替换为SELU。随意微调这个架构，可以看到它对超参数（尤其是两个网络的相对学习率）有多敏感。

最后，为了构建数据集，然后编译并训练该网络，这里使用与之前相同的代码。训练50代后，生成器产生的图像类似如图所示的图像。它仍不完美，但是其中很多图像都很令人信服。

```python
batch_size = 32
dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)
dataset = dataset.shuffle(1000)
dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)
```

```python
train_gan(gan, dataset, batch_size, codings_size)
```

```python
tf.random.set_seed(42)
np.random.seed(42)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = generator(noise)
plot_multiple_images(generated_images, 8)
```

![50代训练后DCGAN生成的图像](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\50代训练后DCGAN生成的图像.png)

如果放大该结构并在更大的人脸数据集上训练它，可以得到相当逼真的图像。事实上，DCGAN可以学习相当有意义的潜在表示，如图所示：生成许多图像，其中九张是手动选择的（左上），包括三张表示戴眼镜的男人，三个不戴眼镜的男人，以及三个不戴眼镜的女人。对于这些类别（category）中的每个，用于生成图像的编码被平均，基于该平均编码生成一张图像（左下）。简言之，三张左下图像中的每个都表示位于其上方的三个图像的均值。但是这在像素级别上计算的简单均值（这会导致三张重叠的脸），它是在潜在空间中计算的均值，因此图像看上去仍然像正常的脸。令人惊讶的是，如果计算戴眼镜的男人，减去不戴眼镜的男人，加上不戴眼镜的女人，其中每项对应一个平均编码，并生成对应这个编码的图像，将得到右边$3\times3$人脸网格中心的图像：一个戴眼镜的女人。它周围的其余8个图像是基于相同的编码加一点噪音生成的，以展示DCGAN的语义插值能力。

![用于视觉概念的向量运算](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\用于视觉概念的向量运算.png)

[^用于视觉概念的向量运算]: Reproduced with the kind authorization of the authors.

如果将每张图像的类别作为额外的输入添加到生成器与判别器中，则它们都将学习每个类别看起来是什么样的，因此这样能够控制生成器产生的每张图像的类别。这被称为**条件GAN[https://homl.info/cgan]（conditional GAN，CGAN）**。

然而DCGAN不完美。例如，当尝试使用DCGAN生成非常大的图像时，常常得到局部令人信服但是全局不一致的特征（例如一只袖子比另一只袖子长得多的衬衫）。

### GAN的渐进增长

Nvidia研究人员Tero Karras等人在[2018年的一篇论文](https://homl.info/progan)中提出一个重要技术：它们建议在训练开始生成小图像，然后将卷积层添加到生成器与判别器中以产生越来越大的图像（$4\times4$、$8\times8$、$16\times16$、……、$512\times512$、$1024\times1024$）。该方法类似堆叠的自编码器的贪婪逐层训练（greedy layer-wise training）。其余的层添加到生成器的最后与判别器的开始，之前训练的层仍然可训练。

例如，当将生成器的输出从$4\times4$增长到$8\times8$时（如图），一个上采样层（使用最近的邻居过滤器）添加到已有卷积层上，因此它输出$8\times8$的特征图，然后它被馈送给新的卷积层（使用`same`填充，步长为1，因此它的输出同样是$8\times8$）。这个新的层之后是一个新的输出卷积层：一个常规的卷积层，核大小为1，将输出投影（projects down）到所需数量的颜色通道（例如3）。当新的卷积层被添加后，为了避免破坏第一个卷积层的训练过的权重，最终的输出是原始输出层（现在输出$8\times8$的特征图）与新的输出层的加权和。新输出的权重为$\alpha$，而原始输出的权重为$1-\alpha$，$\alpha$逐渐从0增加到1。换言之，新的卷积层（图中使用虚线表示）逐渐淡入，而原始输出层逐渐淡出。当新的卷积层加到判别器中时（随后时一个用于下采样的平均池化层），使用了一个相似的淡入/淡出技术。

![渐进增长的GAN：一个输出4 × 4彩色图像的GAN生成器（左），将其扩展以输出8 × 8图像（右）](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\渐进增长的GAN.png)

论文还介绍了其他几种旨在增加输出多样性的技术：

- **小批标准差层（minibatch standard deviation layer）**
  - 添加到判别器末尾附近。对于输入中的每个位置，它计算批中沿所有通道与所有实例的标准差（`S = tf.math.reduce_std(inputs, axis=[0, -1])`）。然后对所有点的标准偏差进行平均，得到单个值（`v = tf.reduce_mean(S)`）。最后，一个额外的特征图被添加到批中每个实例中，并用计算好的值填充它（`tf.concat([inputs, tf.fill([batch_size, height, width, 1], v)], axis=-1)`）。如果生成器产生的图像多样性很小，则判别器中沿特征图的标准差将很小。由于该层的存在，判别器可以很容易获得这个统计数据，使它更不容易被产生很小多样性的生成器欺骗。这会鼓励生成器产生更多样的输出，降低模式坍塌的风险。
- **均衡学习率（equalized learning rate）**
  - 使用均值为0、标准差为1的简单高斯分布，而不是He初始化来初始化所有权重。然而，权重在运行时按与He初始化中相同的因子（即每次执行层时）缩小：它们被除以$\sqrt{2/n_{inputs}}$，其中$n_{inputs}$为该层的输入数量。论文证明当使用RMSProp、Adam或其他自适应梯度优化器时，该技术显著提升GAN的性能。这些优化器通过估计标准差来规范化梯度更新，因此有较大动能范围（dynamic range）（变量的动能范围是它可能取得最高值与最低值之间的比例）的参数需要更长时间训练，而又较小动能范围的参数可能更新过快，导致不稳定。通过将权重作为模型本身一部分重新缩放，而不是仅仅在初始化时重新缩放，该方法确保在整个训练过程中所有参数的动能范围一样，因此它们都按相同速度学习。这同时既能加速训练，又能稳定训练。
- **像素规范化层（pixelwise normalization layer）**
  - 添加到生成器的每个卷积层后。它基于同一图像、同一位置、但沿所有通道的所有激活（除以均方激活的平方根），对每个激活进行规范化。在TensorFlow代码中，这是`inputs / tf.sqrt(tf.reduce_mean(tf.square(X), axis=-1, keepdims=True) + 1e-8)`（需要平滑项`1e-8`来避免被0除）。该技术避免了由于生成器与判别器间的过度竞争导致的激活爆炸。

这些技术的组合使得作者能够生成[非常令人信服的高清晰度人脸图像](https://homl.info/progandemo)。之所以说它“令人信服（convincing）”，是因为当处理GAN时，评估是一大挑战：虽然可以自动平均生成图像的多样性，但是判断它们的质量是棘手与主观得多的任务。一种技术是使用人工评分员，但是这昂贵且耗时。因此作者提议在考虑每个尺度的情况下，度量生成图像与训练图像的局部图像结构的相似度。该思想让他们产生了另一个突破性的创新：StyleGAN。

### StyleGANs

同样的Nvidia团队[2018年的一篇论文](https://homl.info/stylegan)提出了流行的StyleGAN结构，从而再次提升了高分辨率图像生成最新技术。作者使用在生成器中使用**风格迁移（style transfer）**技术以确保生成的图像与训练图像在每个尺度上有相同的局部结构，这大大改善了生成的图像的质量。判别器与损失函数保持不变。StyleGAN由两个网络组成：

- **映射网络（mapping network）**
  - 一个八层MLP将潜在表示$\pmb{z}$（即编码）映射到向量$\pmb{w}$。该向量然后通过多个**仿射变换（affine transformations）**（即没有激活函数的`Dense`层，在图中用“A”方框表示），仿射变换产生多个向量。这些向量在不同层次（level）上控制生成的图像的风格（style），从细粒度的纹理（例如发色）到高层特征（例如成人或儿童）。简言之，该映射网络将编码映射到多个风格向量。
- **合成网络（synthesis network）**
  - 它负责生成图像。它有两个不变的（constant）已学习的输入（该输入训练后不变，但是在训练过程中不断被反向传播微调）。和之前一样，它通过多个卷积与上采样层处理该输入，但是有两个不同：首先，一些噪音被加到输入与卷积层的所有输出中（在激活函数前）；其次，每个噪音层后面是一个**自适应实例规范化（Adaptive Instance Normalization，AdaIN）**层，它独立地标准化每个特征图（通过减去特征图的均值并除以它的标准差），然后它使用风格向量决定每个特征图的尺度与偏移（每个风格向量为每个特征图包含一个尺度与一个偏置项）。

![StyleGAN生成器的结构](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\StyleGAN生成器的结构.png)

[^StyleGAN生成器的结构]: Reproduced with the kind authorization of the authors

独立于编码添加噪音的思想非常重要。图像中的某些部分相当随机，例如每个雀斑或头发的位置。在早期的GAN中，该随机性要么来自编码，要么来自生成器本身产生的某些伪随机噪音。如果它来自编码，意味着生成器必须将编码表示能力的很大一部分用于存储噪音，这相当浪费。另外，噪音必须能够流过网络，到达生成器的最后几层，这看上去像一个不必要的约束，它可能降低训练速度。最后，由于在不同层次（level）使用了相同的噪音，可能会出现一些视觉伪影。如果生成器尝试产生它自己的伪随机噪音，该噪音看起来可能不够令人信服，导致更多的视觉伪影。另外，生成器的部分权重将用于产生伪随机噪音，这看起来又很浪费。通过添加额外的噪音输入，所有这些问题都被避免，GAN能够使用提供的噪声为图像的每个部分添加适量的随机性。

每个层次添加的噪音都不同。每个噪音包括一个充满高斯噪音的特征图，在被添加前，它会广播到（给定层次的）所有特征图并使用已学习的预特征缩放因子（ per-feature scaling factors）缩放（在图中用“B”方框表示）。

最后，StyleGAN使用一个被称为**混合正则化（mixing regularization）**（或**风格混合（style mixing）**的技术，它使用两种不同的编码产生一定比例的生成的图像。具体来说，编码$\pmb{c}_1$与$\pmb{c_2}$发送给映射网络，得到两个风格风格向量$\pmb{w}_1$与$\pmb{w}_2$。然后合成网络为前几层次基于风格$\pmb{w}_1$、其余层次基于风格$\pmb{w}_2$生成一张图像。层次边界是随机选取的，这防止了网络假设相邻层次的风格是相关的，反过来又鼓励GAN中的局部性，意味着每个风格向量只影响生成图像中的有限数量的特征。

# 其他ANN结构

下面快速浏览一些在历史上重要的神经网络结构，它们在今天用得远远少于多层感知器、卷积神经网络、循环神经网络或自编码器。它们常常在文献中提及，并且一些仍然在一系列应用中被使用，因此值得去了解它们。

## Hopfield网络

**Hopfield网络（Hopfield networks）**最早由W. A. Little在1974年引入，然后由J.Hopfield在1982年推广（popularized by）。它们是**关联记忆（associative memory）**网络：首先教它们一些模式，然后当它们看到一个新的模式，它们（有希望）输出最接近的学习到的模型。这使得它们对字符识别很有用，特别是在它们被其他方法超越之前：首先通过给它展示一些字符图像（每个二进制像素映射到一个神经元）示例来训练该网络，然后当给它展示一张新的字符图像，经过一些迭代后它输出最接近的已学习的字符。

Hopfield网络时全连接图（如图）：每个神经元都连接到其他所有神经元。注意图中的图像为$6\times6$像素，因此左边的神经网络应该包含36个神经元（以及630个连接），但是为了视觉上的清晰这里展示了一个小得多的网络。

![Hopfield网络](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\Hopfield网络.png)

训练算法使用Hebb规则：对于每张训练图像，如果对应的像素都打开或都关闭，则两个神经元之间的权重增加，如果一个像素打开而另一个像素关闭，则权重减小。

为了给网络展示一张新的图像，首先只需激活与活跃像素对应的神经元，然后网络计算每个神经元的输出，这样就得到一张新的图像。然后拿着这张图像并重复整个过程。一段时间后，网络达到一个稳定状态。通常，这对应最接近输入图像的训练图像。

一个所谓的**能量函数（energy function）**与Hopfield网络关联。每次迭代，能量减少，因此网络保证最终会稳定在一个低能量状态。训练算法微调权重时会降低训练模式的能量水平，因此网络有可能会稳定在这些低能量配置中的一个。不幸的是，一些不在训练集中的模式最终也会有低能量，因此网络有时会稳定在一个未学习的配置中。这些模式被称为**虚假模式（spurious patterns）**。

Hopfield网络的另一个主要缺点是它们不能很好地扩展：它们的记忆容量大约等于神经元数量的14%。例如，为了分类$28\times28$像素的图像，需要一个有784个全连接神经元与306936个权重的Hopfield网络。这样的网络只能学习大约110个不同的字符。对于如此小的记忆，使用的参数却很多。

## 玻尔兹曼机

**玻尔兹曼机（Boltzmann Machines）**由Geoffrey Hinton与Terrence Sejnowski发明于1985年。与Hopfield网络一样，它们也是全连接ANN，但是它们基于**随机神经元（stochastic neurons）**：这些神经元以一定概率输出1，否则输出0，而不是使用一个**确定性阶跃函数（deterministic step function）**去决定输出值。这些ANN使用的概率函数基于（统计力学（statistical mechanics）中使用的）Boltzmann分布，因此而得名。<a name="(其他ANN结构)(玻尔兹曼机)(1)">下式</a>给定一个特定神经元输出1的概率：
$$
p(s_i^{(next\ step)}=1)=\sigma(\frac{\sum_{j=1}^Nw_{i,j}s_j+b_i}{T})
$$

- $s_j$为第$j$个神经元的状态（0或1）。
- $w_{i,j}$为第$i$与第$j$个神经元间的连接权重。注意$w_{i,i}=0$。
- $b_i$为第$i$个神经元的偏置项。可以通过在网络中添加一个偏置神经元来实现该项。
- $N$为网络中神经元的数量。
- $T$为一个数字，被称为该网络的**温度（temperature）**。温度越高，输出越随机（即概率越接近50%）。
- $\sigma$为logistics函数。

玻尔兹曼中的神经元分为两组：**可见神经元（visible units）**与**隐藏神经元（hidden units）**（如图）。所有神经元都以同样随机的方式工作，但是可见单元接受输入并从中读取输出。

因为玻尔兹曼机的随机特性，它永远不会稳定在一个固定配置。相反，它将在许多配置间一直切换。如果让它运行足够长的时间，则该观察到一个特定配置的概率只是连接权重与偏置项的一个函数，而不是原始配置的函数。当网络达到这样的“忘记”原始配置的状态时，就说它处于**热平衡（thermal equilibrium）**（虽然它的配置一直在变化）。通过合理地设置网络参数，让网络达到热平衡，然后观察它的状态，可以模拟各种各样的概率分布。这被称为**生成模型（generative mode）**。

![玻尔兹曼机](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\玻尔兹曼机.png)

训练一个玻尔兹曼机就是找到能够让网络近似训练集的概率分布的参数。例如，如果有3个可见神经元，且训练集包含75%的$(0,1,1)$三元组，10%的$(0,0,1)$三元组与15%的$(1,1,1)$三元组，则训练一个玻尔兹曼机后，可以用它生成概率分布大致相同的随机二元三元组。例如，大约75%的时间它会输出$(0,1,1)$三元组。

这样的生成模型可以以多种方式使用。例如，如果它在图片上被训练，并给该网络提供不完整的或有噪音的图像，则它将以合理的方式自动“修复”该图像。也可以将生成模型用于分类。只需要添加一些可见神经元去编码训练图像的类别（例如，添加10个可见神经元，当训练图像代表5时，只打开第5个神经元）。然后，当给定一张新图像时，网络会自动打开合适的可见神经元去指示图像类别（例如，如果图像代表5，则它会打开第5个可见神经元）。

不幸的是，没有有效的方法来训练玻尔兹曼机。然而，一些相当有效的方法已被开发出来去训练**受限玻尔兹曼机（restricted Boltzmann machines，RBMs）**。

## 受限玻尔兹曼机

一个RBM就是一个玻尔兹曼机，其中可见单元之间或隐藏单元之间没有连接，只有可见单元与隐藏单间之间有连接。如图表示一个RBM，它有3个可见神经单元与4个隐藏单元。

![受限玻尔兹曼机](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\受限玻尔兹曼机.png)

一个非常有效的训练算法被称作**对比散度（Contrastive Divergence）**，它[由 Miguel Á. Carreira-Perpiñán与Geoffrey Hinton在2005年引入](https://homl.info/135)。以下是它的工作原理：对于每个训练实例$\pmb{x}$，算法首先通过设置可见单元的状态为$x_1$、$x_2$、……、$x_n$来将其馈送给网络；然后通过应用[式1](#(其他ANN结构)(玻尔兹曼机)(1))描述的随机等式（stochastic equation）来计算隐藏单元的状态，这会得到一个隐藏向量$\pmb{h}$（其中$h_i$等于第$i$个单元的状态）；接下来通过应用相同的随机等式来计算可见单元的状态，这会得到一个向量$\pmb{x}'$；然后再次计算隐藏单元的状态，这会得到一个向量$\pmb{h}'$；现在可以应用下式中的规则来更新每个连接权重，其中$\eta$为学习率：



$$
w_{i,j}\leftarrow_{i,j}+\eta(\pmb{x}\pmb{h}^T-\pmb{x}'\pmb{h}'^T)
$$

这个算法的一大好处是无需等待该网络到达热平衡：它只需要前进、后退、再前进即可。这使得它比之前的算法高效的多，并且它也是基于多层RBM的深度学习第一次成功的一个关键因素。

## 深度信念网络

RBM的一些层可悲堆叠：第一层RBM的隐藏单元用作第二层RBM的可见单元，如此下去。这样的RBM栈被称为**深度信念网络（deep belief net）**。

直到2010年代早期，深度信念网络是深度学习的最新技术。它们仍然是非常活跃的研究对象，因此很可能会在未来复兴。

Geoffrey Hinton的一个学生Yee-Whye Teh观察到可以使用对比散度一次训练DBN的一层，首先是低层，然后逐渐移到顶层。由此产生了[一篇开创性的论文，它引发了2006年的深度学习海啸](https://homl.info/136)。

与RBM一样，DBN学习在没有任何监督的情况下去重现其输入的概率分布。然而，它们对此擅长得多，就像深度神经网络比浅神经网络强大得多一样：现实世界中的数据常常是按照层次模式来组织的，DBN利用了这一点。它们的低层学习输入数据中的低层（low-level）特征，而高层学习高层（high-level）特征。

与RBM一样，DBN根本上来说是无监督的，但是可以通过添加一些可见单元去表示标签，来按照有监督的方式去训练它们。另外，DBN的一大特色是它们可以按照半监督风格去训练。如图表示这样的一个用于半监督学习的DBN：

![用于半监督学习的深度信念网络](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\用于半监督学习的深度信念网络.png)

首先，无监督训练RBM 1，它学习训练数据中的低层特征。然后以RBM 1的隐藏单元作为输入对RBM 2进行训练，同样是无监督的：它学习更高层的特征（注意RBM 2的隐藏单元只包括最右的三个单元，不包括标签单元）。更多的RBM可以按照该方式堆叠。目前为止，训练全部都是无监督的。最后，以RBM 2的隐藏单元以及额外的可见单元作为输入对RBM 3进行训练，这些可见单元表示目标标签（例如，一个表示实例类别的独热编码）。RBM 3学习去将高层特征与训练标签关联。这是有监督步。

在训练的最后，如果给RBM 1一个新实例，则信号将向上传播到RBM 2，然后到RBM 3，最后回到标签单元，正确的标签有希望会显现。这就是如何使用DBN进行分类的方法。

> hopefully, the appropriate label will light up. This is how a DBN can be used for classification.

该半监督方法的一大好处是不需要太多的有标签训练数据。如果无监督RBM做得足够好的话，每个类别只需要少量有标签训练实例。类似地，婴儿在无监督的情况下学习识别物体，因此当指向一个椅子并说“椅子”，婴儿可以将“椅子”与它已经学会的物体类别联系起来。不需要指向每个椅子并说“椅子”，少量示例足以（只要足够让婴儿确信你指的是椅子，而不是它的颜色或该椅子的一个部件）。

DBN还可以反过来工作。如果激活一个标签单元，则信号将向上传播到RBM 3的隐藏层，然后向下到RBM 2，然后是RBM 1，新实例将通过RBM 1的可见单元输出。这个新实例通常看起来就像一个常规实例，它的类别对应的标签单元被激活。DBN的这种生成能力（generative capability ）相当强大。例如，它可以用来自动生成图像标题（caption），反之亦然：首先无监督训练一个DBN去学习图像中的特征，然后同样无监督训练另一个DBN去学习一组标题中的特征（例如“car”通常与“automobile”一起出现。然后将一个RBM堆叠在两个DBN的顶部，并使用一组图像及其标题训练该RBM，它学习将图像中的高层特征与标题中的高层特征联系起来。接下来，如果将一张汽车图片馈送到图像DBN中，信号将通过网络传播，向上到RBM的顶层（top-level），然后回到标题DBN的底部，并生成一个标题。由于RBM与DBN的随机性，标题会一直随机改变，但是通常适用于图像。如果产生了几百个标题，则最常见的一个可能是描述图像的好标题（更多详情以及一个演示，请参见Geoffrey Hinton的视频：[https://homl.info/137](https://homl.info/137)）。

## 自组织映射

**自组织映射（self-organizing maps，SOMs）**与目前讨论过的所有其他类型的神经网络都很不同（quite different），它们用于产生高维数据集的低维表示，一般用于可视化、聚类或分类。神经元分布在一张图（map）上（通常2维用于可视化，但它可以是任意数量的维度），如图。每个神经元对每个输入有一个加权连接（注意图中只展示了两个输入，但是典型情况下有非常大的数量，因为SOMs的全部目的为降维）。

> since the whole point of SOMs is to reduce dimensionality

![自组织映射](C:\Users\31654\Documents\Memory\Activity\机器学习\资源\自组织映射.png)

一旦网络训练完，可以将新实例馈送给它，这只会激活一个神经元（图中的一个点），它的权重向量最接近输入向量。一般地，原始输入空间中接近的实例会激活图中接近的实例。这使得SOM不仅对可视化有用（特别地，可以轻松地识别图中的簇），而且对语音识别等应用也有用。例如，如果每个实例表示一个人发元音的一个录音，则元音“a”的不同发音会激活图中同一区域的神经元，而元音“e”的实例会激活图中其他区域的神经元，且中间（intermediate）声音一般会激活图中的中间神经元。

训练算法是无监督的，它的工作原理是让所有神经元相互竞争。首先，所有权重被随机初始化，然后随机选择一个训练实例并将它馈送到网络。所有神经元计算它们的权重向量与该输入向量之间的距离（这与目前为止看到的人工神经元都非常不同）。距离最小的神经元获胜，它微调其权重向量，使其稍微更接近输入向量，这使得它更有可能在输入与之类似的未来竞争中获胜。它还动员周围的神经元，它们也会更新其权重，使其稍微更接近输入向量（但是调整的幅度不如获胜神经元）。然后算法选择其他训练实例并重复该过程，如此下去。这个算法倾向于让接近的神经元逐渐专于相似的输入。

SOM与[降维](#(降维))章中讨论的降维技术的一个重要不同在于所有实例被映射到低维空间中的离散数量的点（每个神经元一个点）。当神经元数量非常少，该技术最好被描述为聚类而不是降维。

# 参考

[^1]: [OReilly.Hands-on.Machine.Learning.with.Scikit-Learn.Keras.and.TensorFlow.2nd.2019.9.epub](https://pan.baidu.com/s/10_Z_dp-xqO5q_V-HhxY2cg#list/path=%2F)，提取码：tui4$\rightarrow$[OReilly.Hands-on.Machine.Learning.with.Scikit-Learn.Keras.and.TensorFlow.2nd.2019.9.epub](资源\OReilly.Hands-on.Machine.Learning.with.Scikit-Learn.Keras.and.TensorFlow.2nd.2019.9.epub)
[^1]: [OReilly.Hands-on.Machine.Learning.with.Scikit-Learn.Keras.and.TensorFlow.2nd.2019.9.pdf](https://pan.baidu.com/s/10_Z_dp-xqO5q_V-HhxY2cg#list/path=%2F)，提取码：tui4$\rightarrow$[OReilly.Hands-on.Machine.Learning.with.Scikit-Learn.Keras.and.TensorFlow.2nd.2019.9.pdf](资源\OReilly.Hands-on.Machine.Learning.with.Scikit-Learn.Keras.and.TensorFlow.2nd.2019.9.pdf)
[^1]: [搬书匠-2287-Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd Edition-2019-英文版.pdf](https://590m.com/file/18468592-456882362)$\rightarrow$[搬书匠-2287-Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd Edition-2019-英文版.pdf](资源\搬书匠-2287-Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd Edition-2019-英文版.pdf)
[^1]: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.pdf
[^2]: [handson-ml2-master.zip](https://github.com/ageron/handson-ml2)$\rightarrow$[handson-ml2-master.zip](资源\handson-ml2-master.zip)

# 注

集成学习中：含有`n_jobs`参数的代码与书中/notebook中有所区别；`load_iris`代码未列。

降维一章：“展开的瑞士卷的左半部分被拉伸，而右半部分被挤压不正确”，也就是说实际运行结果与书中/notebook中不一致。